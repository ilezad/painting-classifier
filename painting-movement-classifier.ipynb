{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Ismael Pérez Roldán*\n",
    "# PFG - Painting Movement Classifier\n",
    "Este notebook contiene el script encargado de clasificar los movimientos artísticos a través de Deep Learning.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import pydot\n",
    "import cv2\n",
    "import random\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, Embedding, LSTM\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras.models import load_model\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_route = os.path.join(os.getcwd(), 'dataset', 'train');\n",
    "validate_route = os.path.join(os.getcwd(), 'dataset', 'validate');\n",
    "model_route = os.path.join(os.getcwd(), 'models', '');\n",
    "\n",
    "movements = ['cubism', 'impressionism', 'symbolism',\n",
    "             'postpainterly_abstraction', 'postimpressionism']\n",
    "\n",
    "num_classes = len(movements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_img(spec, size):\n",
    "    num_slices = int(spec.shape[1] / size)\n",
    "    slices = []  \n",
    "    \n",
    "    for i in range(0, num_slices):\n",
    "        slices.append(spec[size*i : size*(i+1):, size*i : size*(i+1)])\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(route, samples_per_class, mode='train', balanced=False):\n",
    "    data = []\n",
    "    slices = []\n",
    "    X = []\n",
    "    Y = []\n",
    "    Z = []\n",
    "        \n",
    "    for m in movements:\n",
    "        files = os.listdir(os.path.join(route, m))\n",
    "        category = movements.index(m)\n",
    "        \n",
    "        pbar = tqdm(range(samples_per_class))\n",
    "        pbar.set_description(\"Processing %s\" % m)\n",
    "        for i in pbar:\n",
    "            try:\n",
    "                img = cv2.imread(os.path.join(route, m, files[i]))\n",
    "                slices = slice_img(img, slice_size)\n",
    "                for s in slices:\n",
    "                    data.append([s, category])\n",
    "                if mode == 'test':\n",
    "                    Z.append([len(slices), category])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                                    \n",
    "    if mode == 'train':\n",
    "        random.shuffle(data)\n",
    "        \n",
    "    for features, label in data:\n",
    "        X.append(features)\n",
    "        Y.append(label)\n",
    "\n",
    "    # Free up memory\n",
    "    del(data)\n",
    "    del(img)\n",
    "    del(slices)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    if balanced:\n",
    "        counter = Counter(Y)\n",
    "        min_samples = counter.most_common()[-1][1]\n",
    "        total_per_genre = [i[1] for i in sorted(counter.most_common())]\n",
    "        to_delete = [m - min_samples for m in total_per_genre]\n",
    "\n",
    "        for g in range(len(movements)):\n",
    "            indices = np.where(Y == g)[0][:to_delete[g]]\n",
    "            X = np.delete(X, indices, axis=0)\n",
    "            Y = np.delete(Y, indices, axis=0)\n",
    "\n",
    "    X = X.reshape(-1, slice_size, IMG_SIZE, 3)\n",
    "    Y = to_categorical(Y, num_classes=num_classes)    \n",
    "    print(len(X))\n",
    "    print(len(Y))\n",
    "    return X, Y, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Distribution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(x): \n",
    "    x = [np.argmax(x[i]) for i in range(len(x))]\n",
    "    y = [i for i in range(len(movements)+1)]\n",
    "    \n",
    "    n = plt.hist(x, bins=y, alpha=0.8, histtype='bar', ec='black', align='left')\n",
    "    plt.ylabel('Samples')\n",
    "    plt.xticks(y, movements)\n",
    "    plt.grid(axis='y', linestyle='-')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = 450 # 450 max, else the computer will shutdown\n",
    "test_samples = 50\n",
    "IMG_SIZE = 264\n",
    "slice_size = 127\n",
    "balanced = True\n",
    "\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training Data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4620de3b071414f9d941293a66c3346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79705d57d0ea450f852a18cdc856db97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77954f6acf24a7ca9e753a7d605dfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f7936dc9be4002b2550278cc08ee72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ecb30ae1c79414d9c9a5a1269b5b348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=450), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1885\n",
      "1885\n",
      "1885\n"
     ]
    }
   ],
   "source": [
    "print('Loading Training Data...')\n",
    "X_train, Y_train, Z_train = load_data(train_route, training_samples, mode='train', balanced=balanced)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(X):\n",
    "    X = X.reshape(-1, X.shape[1], X.shape[2], 3)\n",
    "    return X\n",
    "        \n",
    "\n",
    "def build_model(loss, optimizer, metrics, input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "        \n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "        \n",
    "    model.add(Conv2D(264, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "    model.add(Conv2D(264, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2048))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "        \n",
    "    model.add(Dense(3060))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "        \n",
    "        \n",
    "    model.add(Dense(len(movements)))\n",
    "    model.add(Activation('softmax'))\n",
    "        \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)        \n",
    "    return model\n",
    "\n",
    "def train_model(model, epochs):\n",
    "    model.summary()\n",
    "\n",
    "    mcp_save = ModelCheckpoint(os.path.join(model_route, 'model3.h5'), monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "    reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, \n",
    "              callbacks=[mcp_save, reduce_lr_loss]) \n",
    "\n",
    "    plot_model(model, to_file=model_route + 'model3.png')\n",
    "    \n",
    "    model.save(os.path.join(model_route, 'final_model.h5'))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ilezad\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ilezad\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 125, 262, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 125, 262, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 62, 131, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 129, 64)       18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 60, 129, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 62, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 60, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 26, 60, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 11, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 9, 26, 264)        304392    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 9, 26, 264)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 4, 13, 264)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 2, 11, 264)        627528    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2, 11, 264)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 1, 5, 264)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1320)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2048)              2705408   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3060)              6269940   \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 3060)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3060)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 15305     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 10,200,333\n",
      "Trainable params: 10,200,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilezad\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ilezad\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1508 samples, validate on 377 samples\n",
      "Epoch 1/100\n",
      "1508/1508 [==============================] - ETA: 6:04 - loss: 7.2829 - acc: 0.156 - ETA: 3:01 - loss: 8.1218 - acc: 0.140 - ETA: 2:00 - loss: 7.4003 - acc: 0.166 - ETA: 1:29 - loss: 6.1229 - acc: 0.187 - ETA: 1:11 - loss: 5.2647 - acc: 0.200 - ETA: 58s - loss: 4.7779 - acc: 0.187 - ETA: 49s - loss: 4.3584 - acc: 0.18 - ETA: 43s - loss: 4.0235 - acc: 0.18 - ETA: 38s - loss: 3.7632 - acc: 0.18 - ETA: 33s - loss: 3.5507 - acc: 0.19 - ETA: 30s - loss: 3.3786 - acc: 0.18 - ETA: 27s - loss: 3.2419 - acc: 0.18 - ETA: 25s - loss: 3.1189 - acc: 0.18 - ETA: 22s - loss: 3.0077 - acc: 0.19 - ETA: 20s - loss: 2.9130 - acc: 0.19 - ETA: 19s - loss: 2.8339 - acc: 0.19 - ETA: 17s - loss: 2.7615 - acc: 0.19 - ETA: 16s - loss: 2.6982 - acc: 0.20 - ETA: 15s - loss: 2.6395 - acc: 0.20 - ETA: 14s - loss: 2.5891 - acc: 0.20 - ETA: 13s - loss: 2.5401 - acc: 0.20 - ETA: 12s - loss: 2.4947 - acc: 0.20 - ETA: 11s - loss: 2.4557 - acc: 0.20 - ETA: 10s - loss: 2.4248 - acc: 0.20 - ETA: 9s - loss: 2.3954 - acc: 0.2037 - ETA: 9s - loss: 2.3646 - acc: 0.205 - ETA: 8s - loss: 2.3404 - acc: 0.202 - ETA: 7s - loss: 2.3139 - acc: 0.205 - ETA: 7s - loss: 2.2906 - acc: 0.208 - ETA: 6s - loss: 2.2683 - acc: 0.207 - ETA: 6s - loss: 2.2469 - acc: 0.210 - ETA: 5s - loss: 2.2281 - acc: 0.210 - ETA: 5s - loss: 2.2084 - acc: 0.214 - ETA: 4s - loss: 2.1910 - acc: 0.212 - ETA: 4s - loss: 2.1738 - acc: 0.213 - ETA: 3s - loss: 2.1574 - acc: 0.217 - ETA: 3s - loss: 2.1426 - acc: 0.217 - ETA: 3s - loss: 2.1281 - acc: 0.217 - ETA: 2s - loss: 2.1139 - acc: 0.215 - ETA: 2s - loss: 2.1014 - acc: 0.214 - ETA: 1s - loss: 2.0886 - acc: 0.214 - ETA: 1s - loss: 2.0792 - acc: 0.212 - ETA: 1s - loss: 2.0674 - acc: 0.211 - ETA: 0s - loss: 2.0580 - acc: 0.210 - ETA: 0s - loss: 2.0467 - acc: 0.213 - ETA: 0s - loss: 2.0377 - acc: 0.214 - ETA: 0s - loss: 2.0272 - acc: 0.216 - 16s 10ms/step - loss: 2.0266 - acc: 0.2155 - val_loss: 1.5810 - val_acc: 0.2785\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.27851, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 2/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.6023 - acc: 0.187 - ETA: 6s - loss: 1.6046 - acc: 0.171 - ETA: 5s - loss: 1.5986 - acc: 0.177 - ETA: 5s - loss: 1.5869 - acc: 0.234 - ETA: 5s - loss: 1.5985 - acc: 0.218 - ETA: 5s - loss: 1.6007 - acc: 0.213 - ETA: 5s - loss: 1.6011 - acc: 0.218 - ETA: 5s - loss: 1.5987 - acc: 0.230 - ETA: 5s - loss: 1.5975 - acc: 0.236 - ETA: 5s - loss: 1.5950 - acc: 0.250 - ETA: 4s - loss: 1.5947 - acc: 0.250 - ETA: 4s - loss: 1.5953 - acc: 0.247 - ETA: 4s - loss: 1.5934 - acc: 0.245 - ETA: 4s - loss: 1.5922 - acc: 0.247 - ETA: 4s - loss: 1.5955 - acc: 0.243 - ETA: 4s - loss: 1.5957 - acc: 0.242 - ETA: 4s - loss: 1.5959 - acc: 0.239 - ETA: 3s - loss: 1.5946 - acc: 0.241 - ETA: 3s - loss: 1.5937 - acc: 0.243 - ETA: 3s - loss: 1.5899 - acc: 0.245 - ETA: 3s - loss: 1.5916 - acc: 0.244 - ETA: 3s - loss: 1.5923 - acc: 0.242 - ETA: 3s - loss: 1.5913 - acc: 0.250 - ETA: 3s - loss: 1.5884 - acc: 0.253 - ETA: 3s - loss: 1.5882 - acc: 0.257 - ETA: 2s - loss: 1.5869 - acc: 0.260 - ETA: 2s - loss: 1.5905 - acc: 0.255 - ETA: 2s - loss: 1.5923 - acc: 0.248 - ETA: 2s - loss: 1.5912 - acc: 0.247 - ETA: 2s - loss: 1.5925 - acc: 0.246 - ETA: 2s - loss: 1.5955 - acc: 0.244 - ETA: 2s - loss: 1.5966 - acc: 0.239 - ETA: 1s - loss: 1.5962 - acc: 0.240 - ETA: 1s - loss: 1.5951 - acc: 0.239 - ETA: 1s - loss: 1.5940 - acc: 0.239 - ETA: 1s - loss: 1.5932 - acc: 0.242 - ETA: 1s - loss: 1.5915 - acc: 0.243 - ETA: 1s - loss: 1.5922 - acc: 0.240 - ETA: 1s - loss: 1.5909 - acc: 0.239 - ETA: 0s - loss: 1.5922 - acc: 0.235 - ETA: 0s - loss: 1.5924 - acc: 0.235 - ETA: 0s - loss: 1.5933 - acc: 0.235 - ETA: 0s - loss: 1.5941 - acc: 0.234 - ETA: 0s - loss: 1.5937 - acc: 0.234 - ETA: 0s - loss: 1.5929 - acc: 0.233 - ETA: 0s - loss: 1.5929 - acc: 0.233 - ETA: 0s - loss: 1.5930 - acc: 0.233 - 7s 5ms/step - loss: 1.5932 - acc: 0.2328 - val_loss: 1.5560 - val_acc: 0.2838\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.27851 to 0.28382, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 3/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.5905 - acc: 0.156 - ETA: 5s - loss: 1.5878 - acc: 0.140 - ETA: 5s - loss: 1.5764 - acc: 0.208 - ETA: 5s - loss: 1.5738 - acc: 0.242 - ETA: 5s - loss: 1.5693 - acc: 0.268 - ETA: 5s - loss: 1.5536 - acc: 0.276 - ETA: 5s - loss: 1.5538 - acc: 0.267 - ETA: 5s - loss: 1.5541 - acc: 0.281 - ETA: 5s - loss: 1.5551 - acc: 0.274 - ETA: 4s - loss: 1.5617 - acc: 0.271 - ETA: 4s - loss: 1.5665 - acc: 0.269 - ETA: 4s - loss: 1.5644 - acc: 0.270 - ETA: 4s - loss: 1.5605 - acc: 0.278 - ETA: 4s - loss: 1.5636 - acc: 0.272 - ETA: 4s - loss: 1.5675 - acc: 0.266 - ETA: 4s - loss: 1.5689 - acc: 0.261 - ETA: 4s - loss: 1.5699 - acc: 0.259 - ETA: 3s - loss: 1.5663 - acc: 0.263 - ETA: 3s - loss: 1.5681 - acc: 0.261 - ETA: 3s - loss: 1.5685 - acc: 0.267 - ETA: 3s - loss: 1.5662 - acc: 0.269 - ETA: 3s - loss: 1.5643 - acc: 0.272 - ETA: 3s - loss: 1.5666 - acc: 0.270 - ETA: 3s - loss: 1.5649 - acc: 0.270 - ETA: 2s - loss: 1.5659 - acc: 0.268 - ETA: 2s - loss: 1.5639 - acc: 0.271 - ETA: 2s - loss: 1.5656 - acc: 0.266 - ETA: 2s - loss: 1.5667 - acc: 0.263 - ETA: 2s - loss: 1.5672 - acc: 0.264 - ETA: 2s - loss: 1.5636 - acc: 0.266 - ETA: 2s - loss: 1.5625 - acc: 0.269 - ETA: 2s - loss: 1.5621 - acc: 0.267 - ETA: 1s - loss: 1.5630 - acc: 0.263 - ETA: 1s - loss: 1.5645 - acc: 0.264 - ETA: 1s - loss: 1.5627 - acc: 0.266 - ETA: 1s - loss: 1.5627 - acc: 0.266 - ETA: 1s - loss: 1.5645 - acc: 0.265 - ETA: 1s - loss: 1.5641 - acc: 0.265 - ETA: 1s - loss: 1.5630 - acc: 0.265 - ETA: 0s - loss: 1.5633 - acc: 0.266 - ETA: 0s - loss: 1.5649 - acc: 0.263 - ETA: 0s - loss: 1.5632 - acc: 0.265 - ETA: 0s - loss: 1.5611 - acc: 0.268 - ETA: 0s - loss: 1.5625 - acc: 0.267 - ETA: 0s - loss: 1.5623 - acc: 0.270 - ETA: 0s - loss: 1.5614 - acc: 0.268 - ETA: 0s - loss: 1.5648 - acc: 0.265 - 7s 5ms/step - loss: 1.5645 - acc: 0.2653 - val_loss: 1.5247 - val_acc: 0.2918\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.28382 to 0.29178, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 4/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.5469 - acc: 0.156 - ETA: 6s - loss: 1.5197 - acc: 0.281 - ETA: 5s - loss: 1.5241 - acc: 0.281 - ETA: 5s - loss: 1.5440 - acc: 0.250 - ETA: 5s - loss: 1.5424 - acc: 0.237 - ETA: 5s - loss: 1.5464 - acc: 0.239 - ETA: 5s - loss: 1.5492 - acc: 0.250 - ETA: 5s - loss: 1.5587 - acc: 0.246 - ETA: 5s - loss: 1.5573 - acc: 0.250 - ETA: 4s - loss: 1.5533 - acc: 0.265 - ETA: 4s - loss: 1.5487 - acc: 0.267 - ETA: 4s - loss: 1.5493 - acc: 0.268 - ETA: 4s - loss: 1.5540 - acc: 0.274 - ETA: 4s - loss: 1.5558 - acc: 0.263 - ETA: 4s - loss: 1.5513 - acc: 0.268 - ETA: 4s - loss: 1.5501 - acc: 0.265 - ETA: 4s - loss: 1.5512 - acc: 0.262 - ETA: 3s - loss: 1.5510 - acc: 0.267 - ETA: 3s - loss: 1.5511 - acc: 0.269 - ETA: 3s - loss: 1.5498 - acc: 0.268 - ETA: 3s - loss: 1.5455 - acc: 0.275 - ETA: 3s - loss: 1.5420 - acc: 0.282 - ETA: 3s - loss: 1.5457 - acc: 0.285 - ETA: 3s - loss: 1.5447 - acc: 0.285 - ETA: 2s - loss: 1.5474 - acc: 0.278 - ETA: 2s - loss: 1.5471 - acc: 0.281 - ETA: 2s - loss: 1.5441 - acc: 0.281 - ETA: 2s - loss: 1.5398 - acc: 0.283 - ETA: 2s - loss: 1.5373 - acc: 0.285 - ETA: 2s - loss: 1.5391 - acc: 0.285 - ETA: 2s - loss: 1.5337 - acc: 0.291 - ETA: 2s - loss: 1.5366 - acc: 0.288 - ETA: 1s - loss: 1.5374 - acc: 0.285 - ETA: 1s - loss: 1.5396 - acc: 0.282 - ETA: 1s - loss: 1.5391 - acc: 0.283 - ETA: 1s - loss: 1.5380 - acc: 0.283 - ETA: 1s - loss: 1.5407 - acc: 0.278 - ETA: 1s - loss: 1.5422 - acc: 0.278 - ETA: 1s - loss: 1.5430 - acc: 0.279 - ETA: 0s - loss: 1.5412 - acc: 0.281 - ETA: 0s - loss: 1.5421 - acc: 0.280 - ETA: 0s - loss: 1.5422 - acc: 0.279 - ETA: 0s - loss: 1.5439 - acc: 0.280 - ETA: 0s - loss: 1.5423 - acc: 0.284 - ETA: 0s - loss: 1.5411 - acc: 0.286 - ETA: 0s - loss: 1.5408 - acc: 0.289 - ETA: 0s - loss: 1.5406 - acc: 0.288 - 7s 5ms/step - loss: 1.5405 - acc: 0.2891 - val_loss: 1.4892 - val_acc: 0.3316\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.29178 to 0.33156, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 5/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.4910 - acc: 0.375 - ETA: 5s - loss: 1.5256 - acc: 0.296 - ETA: 5s - loss: 1.5157 - acc: 0.333 - ETA: 5s - loss: 1.4936 - acc: 0.351 - ETA: 5s - loss: 1.5172 - acc: 0.331 - ETA: 5s - loss: 1.5186 - acc: 0.322 - ETA: 5s - loss: 1.5024 - acc: 0.352 - ETA: 5s - loss: 1.5031 - acc: 0.351 - ETA: 5s - loss: 1.5005 - acc: 0.354 - ETA: 5s - loss: 1.5063 - acc: 0.343 - ETA: 4s - loss: 1.5109 - acc: 0.326 - ETA: 4s - loss: 1.5092 - acc: 0.325 - ETA: 4s - loss: 1.5056 - acc: 0.334 - ETA: 4s - loss: 1.5001 - acc: 0.337 - ETA: 4s - loss: 1.5072 - acc: 0.331 - ETA: 4s - loss: 1.5200 - acc: 0.318 - ETA: 4s - loss: 1.5224 - acc: 0.316 - ETA: 3s - loss: 1.5248 - acc: 0.314 - ETA: 3s - loss: 1.5237 - acc: 0.315 - ETA: 3s - loss: 1.5214 - acc: 0.312 - ETA: 3s - loss: 1.5258 - acc: 0.309 - ETA: 3s - loss: 1.5229 - acc: 0.315 - ETA: 3s - loss: 1.5232 - acc: 0.316 - ETA: 3s - loss: 1.5260 - acc: 0.312 - ETA: 2s - loss: 1.5269 - acc: 0.312 - ETA: 2s - loss: 1.5247 - acc: 0.314 - ETA: 2s - loss: 1.5264 - acc: 0.310 - ETA: 2s - loss: 1.5272 - acc: 0.308 - ETA: 2s - loss: 1.5248 - acc: 0.308 - ETA: 2s - loss: 1.5245 - acc: 0.306 - ETA: 2s - loss: 1.5224 - acc: 0.309 - ETA: 2s - loss: 1.5270 - acc: 0.305 - ETA: 1s - loss: 1.5231 - acc: 0.311 - ETA: 1s - loss: 1.5238 - acc: 0.311 - ETA: 1s - loss: 1.5232 - acc: 0.313 - ETA: 1s - loss: 1.5227 - acc: 0.313 - ETA: 1s - loss: 1.5244 - acc: 0.310 - ETA: 1s - loss: 1.5223 - acc: 0.312 - ETA: 1s - loss: 1.5226 - acc: 0.313 - ETA: 0s - loss: 1.5239 - acc: 0.312 - ETA: 0s - loss: 1.5257 - acc: 0.311 - ETA: 0s - loss: 1.5254 - acc: 0.311 - ETA: 0s - loss: 1.5259 - acc: 0.309 - ETA: 0s - loss: 1.5217 - acc: 0.313 - ETA: 0s - loss: 1.5229 - acc: 0.312 - ETA: 0s - loss: 1.5216 - acc: 0.315 - ETA: 0s - loss: 1.5200 - acc: 0.319 - 7s 5ms/step - loss: 1.5199 - acc: 0.3196 - val_loss: 1.4926 - val_acc: 0.3183\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.33156\n",
      "Epoch 6/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.4502 - acc: 0.468 - ETA: 6s - loss: 1.5163 - acc: 0.375 - ETA: 5s - loss: 1.4959 - acc: 0.375 - ETA: 5s - loss: 1.5127 - acc: 0.375 - ETA: 5s - loss: 1.5359 - acc: 0.343 - ETA: 5s - loss: 1.5081 - acc: 0.354 - ETA: 5s - loss: 1.5064 - acc: 0.352 - ETA: 5s - loss: 1.4746 - acc: 0.386 - ETA: 5s - loss: 1.4901 - acc: 0.364 - ETA: 4s - loss: 1.4884 - acc: 0.362 - ETA: 4s - loss: 1.4989 - acc: 0.343 - ETA: 4s - loss: 1.4995 - acc: 0.338 - ETA: 4s - loss: 1.4970 - acc: 0.348 - ETA: 4s - loss: 1.4930 - acc: 0.352 - ETA: 4s - loss: 1.4886 - acc: 0.358 - ETA: 4s - loss: 1.4795 - acc: 0.367 - ETA: 4s - loss: 1.4826 - acc: 0.362 - ETA: 3s - loss: 1.4809 - acc: 0.364 - ETA: 3s - loss: 1.4840 - acc: 0.361 - ETA: 3s - loss: 1.4878 - acc: 0.364 - ETA: 3s - loss: 1.4962 - acc: 0.361 - ETA: 3s - loss: 1.4870 - acc: 0.366 - ETA: 3s - loss: 1.4806 - acc: 0.364 - ETA: 3s - loss: 1.4773 - acc: 0.369 - ETA: 2s - loss: 1.4740 - acc: 0.375 - ETA: 2s - loss: 1.4760 - acc: 0.373 - ETA: 2s - loss: 1.4786 - acc: 0.366 - ETA: 2s - loss: 1.4800 - acc: 0.365 - ETA: 2s - loss: 1.4767 - acc: 0.366 - ETA: 2s - loss: 1.4794 - acc: 0.363 - ETA: 2s - loss: 1.4802 - acc: 0.362 - ETA: 2s - loss: 1.4804 - acc: 0.364 - ETA: 1s - loss: 1.4808 - acc: 0.360 - ETA: 1s - loss: 1.4787 - acc: 0.362 - ETA: 1s - loss: 1.4816 - acc: 0.358 - ETA: 1s - loss: 1.4810 - acc: 0.362 - ETA: 1s - loss: 1.4849 - acc: 0.359 - ETA: 1s - loss: 1.4862 - acc: 0.356 - ETA: 1s - loss: 1.4892 - acc: 0.352 - ETA: 0s - loss: 1.4895 - acc: 0.353 - ETA: 0s - loss: 1.4874 - acc: 0.355 - ETA: 0s - loss: 1.4867 - acc: 0.352 - ETA: 0s - loss: 1.4880 - acc: 0.351 - ETA: 0s - loss: 1.4884 - acc: 0.350 - ETA: 0s - loss: 1.4897 - acc: 0.348 - ETA: 0s - loss: 1.4883 - acc: 0.350 - ETA: 0s - loss: 1.4906 - acc: 0.349 - 7s 5ms/step - loss: 1.4897 - acc: 0.3501 - val_loss: 1.4206 - val_acc: 0.3634\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.33156 to 0.36340, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 7/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.7044 - acc: 0.187 - ETA: 5s - loss: 1.5706 - acc: 0.312 - ETA: 5s - loss: 1.5744 - acc: 0.312 - ETA: 5s - loss: 1.5640 - acc: 0.304 - ETA: 5s - loss: 1.5632 - acc: 0.312 - ETA: 5s - loss: 1.5370 - acc: 0.317 - ETA: 5s - loss: 1.5186 - acc: 0.321 - ETA: 5s - loss: 1.5094 - acc: 0.324 - ETA: 5s - loss: 1.5035 - acc: 0.322 - ETA: 4s - loss: 1.4986 - acc: 0.325 - ETA: 4s - loss: 1.4943 - acc: 0.321 - ETA: 4s - loss: 1.4882 - acc: 0.335 - ETA: 4s - loss: 1.4853 - acc: 0.336 - ETA: 4s - loss: 1.4782 - acc: 0.341 - ETA: 4s - loss: 1.4796 - acc: 0.345 - ETA: 4s - loss: 1.4757 - acc: 0.347 - ETA: 4s - loss: 1.4836 - acc: 0.345 - ETA: 3s - loss: 1.4785 - acc: 0.345 - ETA: 3s - loss: 1.4777 - acc: 0.348 - ETA: 3s - loss: 1.4796 - acc: 0.351 - ETA: 3s - loss: 1.4724 - acc: 0.361 - ETA: 3s - loss: 1.4791 - acc: 0.356 - ETA: 3s - loss: 1.4731 - acc: 0.360 - ETA: 3s - loss: 1.4775 - acc: 0.356 - ETA: 2s - loss: 1.4800 - acc: 0.355 - ETA: 2s - loss: 1.4824 - acc: 0.352 - ETA: 2s - loss: 1.4806 - acc: 0.349 - ETA: 2s - loss: 1.4813 - acc: 0.349 - ETA: 2s - loss: 1.4735 - acc: 0.353 - ETA: 2s - loss: 1.4743 - acc: 0.352 - ETA: 2s - loss: 1.4745 - acc: 0.349 - ETA: 2s - loss: 1.4808 - acc: 0.346 - ETA: 1s - loss: 1.4765 - acc: 0.352 - ETA: 1s - loss: 1.4774 - acc: 0.350 - ETA: 1s - loss: 1.4798 - acc: 0.351 - ETA: 1s - loss: 1.4815 - acc: 0.349 - ETA: 1s - loss: 1.4800 - acc: 0.349 - ETA: 1s - loss: 1.4763 - acc: 0.350 - ETA: 1s - loss: 1.4725 - acc: 0.347 - ETA: 0s - loss: 1.4706 - acc: 0.350 - ETA: 0s - loss: 1.4747 - acc: 0.346 - ETA: 0s - loss: 1.4775 - acc: 0.345 - ETA: 0s - loss: 1.4763 - acc: 0.343 - ETA: 0s - loss: 1.4731 - acc: 0.343 - ETA: 0s - loss: 1.4731 - acc: 0.341 - ETA: 0s - loss: 1.4737 - acc: 0.343 - ETA: 0s - loss: 1.4780 - acc: 0.339 - 7s 5ms/step - loss: 1.4782 - acc: 0.3402 - val_loss: 1.4448 - val_acc: 0.3793\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.36340 to 0.37931, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 5s - loss: 1.4934 - acc: 0.250 - ETA: 5s - loss: 1.4414 - acc: 0.328 - ETA: 5s - loss: 1.4815 - acc: 0.312 - ETA: 5s - loss: 1.4977 - acc: 0.328 - ETA: 5s - loss: 1.5142 - acc: 0.306 - ETA: 5s - loss: 1.5280 - acc: 0.317 - ETA: 5s - loss: 1.5430 - acc: 0.303 - ETA: 5s - loss: 1.5321 - acc: 0.312 - ETA: 5s - loss: 1.5072 - acc: 0.329 - ETA: 4s - loss: 1.5076 - acc: 0.331 - ETA: 4s - loss: 1.5177 - acc: 0.321 - ETA: 4s - loss: 1.5115 - acc: 0.325 - ETA: 4s - loss: 1.4968 - acc: 0.329 - ETA: 4s - loss: 1.4876 - acc: 0.334 - ETA: 4s - loss: 1.4870 - acc: 0.337 - ETA: 4s - loss: 1.4901 - acc: 0.337 - ETA: 4s - loss: 1.4924 - acc: 0.336 - ETA: 3s - loss: 1.5006 - acc: 0.331 - ETA: 3s - loss: 1.5008 - acc: 0.330 - ETA: 3s - loss: 1.5000 - acc: 0.328 - ETA: 3s - loss: 1.5032 - acc: 0.327 - ETA: 3s - loss: 1.5042 - acc: 0.323 - ETA: 3s - loss: 1.5086 - acc: 0.322 - ETA: 3s - loss: 1.5093 - acc: 0.321 - ETA: 2s - loss: 1.5094 - acc: 0.320 - ETA: 2s - loss: 1.5073 - acc: 0.324 - ETA: 2s - loss: 1.5075 - acc: 0.320 - ETA: 2s - loss: 1.5068 - acc: 0.318 - ETA: 2s - loss: 1.5054 - acc: 0.319 - ETA: 2s - loss: 1.5007 - acc: 0.322 - ETA: 2s - loss: 1.5009 - acc: 0.321 - ETA: 2s - loss: 1.4961 - acc: 0.326 - ETA: 1s - loss: 1.4945 - acc: 0.331 - ETA: 1s - loss: 1.4932 - acc: 0.331 - ETA: 1s - loss: 1.4932 - acc: 0.332 - ETA: 1s - loss: 1.4915 - acc: 0.330 - ETA: 1s - loss: 1.4919 - acc: 0.328 - ETA: 1s - loss: 1.4909 - acc: 0.328 - ETA: 1s - loss: 1.4916 - acc: 0.327 - ETA: 0s - loss: 1.4913 - acc: 0.327 - ETA: 0s - loss: 1.4881 - acc: 0.330 - ETA: 0s - loss: 1.4859 - acc: 0.330 - ETA: 0s - loss: 1.4832 - acc: 0.332 - ETA: 0s - loss: 1.4840 - acc: 0.332 - ETA: 0s - loss: 1.4813 - acc: 0.336 - ETA: 0s - loss: 1.4802 - acc: 0.337 - ETA: 0s - loss: 1.4807 - acc: 0.335 - 7s 5ms/step - loss: 1.4806 - acc: 0.3369 - val_loss: 1.4125 - val_acc: 0.4377\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.37931 to 0.43767, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 9/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.3502 - acc: 0.406 - ETA: 6s - loss: 1.4277 - acc: 0.343 - ETA: 5s - loss: 1.4315 - acc: 0.343 - ETA: 5s - loss: 1.4076 - acc: 0.382 - ETA: 5s - loss: 1.4268 - acc: 0.381 - ETA: 5s - loss: 1.4863 - acc: 0.349 - ETA: 5s - loss: 1.5209 - acc: 0.317 - ETA: 5s - loss: 1.5341 - acc: 0.308 - ETA: 5s - loss: 1.5347 - acc: 0.316 - ETA: 5s - loss: 1.5294 - acc: 0.315 - ETA: 4s - loss: 1.5283 - acc: 0.309 - ETA: 4s - loss: 1.5212 - acc: 0.307 - ETA: 4s - loss: 1.5146 - acc: 0.312 - ETA: 4s - loss: 1.5157 - acc: 0.317 - ETA: 4s - loss: 1.5128 - acc: 0.316 - ETA: 4s - loss: 1.5171 - acc: 0.310 - ETA: 4s - loss: 1.5166 - acc: 0.319 - ETA: 3s - loss: 1.5127 - acc: 0.316 - ETA: 3s - loss: 1.5177 - acc: 0.315 - ETA: 3s - loss: 1.5149 - acc: 0.315 - ETA: 3s - loss: 1.5140 - acc: 0.312 - ETA: 3s - loss: 1.5100 - acc: 0.313 - ETA: 3s - loss: 1.5119 - acc: 0.313 - ETA: 3s - loss: 1.5078 - acc: 0.315 - ETA: 2s - loss: 1.5048 - acc: 0.313 - ETA: 2s - loss: 1.4975 - acc: 0.323 - ETA: 2s - loss: 1.4969 - acc: 0.324 - ETA: 2s - loss: 1.4959 - acc: 0.323 - ETA: 2s - loss: 1.4907 - acc: 0.323 - ETA: 2s - loss: 1.4936 - acc: 0.320 - ETA: 2s - loss: 1.4969 - acc: 0.319 - ETA: 2s - loss: 1.4914 - acc: 0.324 - ETA: 1s - loss: 1.4909 - acc: 0.328 - ETA: 1s - loss: 1.4874 - acc: 0.330 - ETA: 1s - loss: 1.4920 - acc: 0.330 - ETA: 1s - loss: 1.4890 - acc: 0.331 - ETA: 1s - loss: 1.4877 - acc: 0.331 - ETA: 1s - loss: 1.4849 - acc: 0.333 - ETA: 1s - loss: 1.4833 - acc: 0.334 - ETA: 0s - loss: 1.4827 - acc: 0.336 - ETA: 0s - loss: 1.4849 - acc: 0.337 - ETA: 0s - loss: 1.4868 - acc: 0.334 - ETA: 0s - loss: 1.4851 - acc: 0.337 - ETA: 0s - loss: 1.4821 - acc: 0.338 - ETA: 0s - loss: 1.4815 - acc: 0.337 - ETA: 0s - loss: 1.4814 - acc: 0.334 - ETA: 0s - loss: 1.4809 - acc: 0.334 - 7s 5ms/step - loss: 1.4806 - acc: 0.3342 - val_loss: 1.4182 - val_acc: 0.4005\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.43767\n",
      "Epoch 10/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.4918 - acc: 0.437 - ETA: 5s - loss: 1.4372 - acc: 0.484 - ETA: 5s - loss: 1.4342 - acc: 0.427 - ETA: 5s - loss: 1.4685 - acc: 0.359 - ETA: 5s - loss: 1.4995 - acc: 0.331 - ETA: 5s - loss: 1.4868 - acc: 0.328 - ETA: 5s - loss: 1.4886 - acc: 0.325 - ETA: 5s - loss: 1.4848 - acc: 0.316 - ETA: 5s - loss: 1.4701 - acc: 0.329 - ETA: 4s - loss: 1.4688 - acc: 0.328 - ETA: 4s - loss: 1.4595 - acc: 0.332 - ETA: 4s - loss: 1.4796 - acc: 0.325 - ETA: 4s - loss: 1.4838 - acc: 0.324 - ETA: 4s - loss: 1.4710 - acc: 0.332 - ETA: 4s - loss: 1.4611 - acc: 0.339 - ETA: 4s - loss: 1.4618 - acc: 0.351 - ETA: 4s - loss: 1.4561 - acc: 0.354 - ETA: 3s - loss: 1.4702 - acc: 0.350 - ETA: 3s - loss: 1.4610 - acc: 0.356 - ETA: 3s - loss: 1.4544 - acc: 0.364 - ETA: 3s - loss: 1.4624 - acc: 0.361 - ETA: 3s - loss: 1.4579 - acc: 0.366 - ETA: 3s - loss: 1.4557 - acc: 0.365 - ETA: 3s - loss: 1.4473 - acc: 0.371 - ETA: 2s - loss: 1.4469 - acc: 0.375 - ETA: 2s - loss: 1.4413 - acc: 0.381 - ETA: 2s - loss: 1.4333 - acc: 0.384 - ETA: 2s - loss: 1.4365 - acc: 0.380 - ETA: 2s - loss: 1.4413 - acc: 0.375 - ETA: 2s - loss: 1.4423 - acc: 0.371 - ETA: 2s - loss: 1.4419 - acc: 0.373 - ETA: 2s - loss: 1.4465 - acc: 0.369 - ETA: 1s - loss: 1.4454 - acc: 0.369 - ETA: 1s - loss: 1.4453 - acc: 0.365 - ETA: 1s - loss: 1.4441 - acc: 0.366 - ETA: 1s - loss: 1.4425 - acc: 0.370 - ETA: 1s - loss: 1.4412 - acc: 0.369 - ETA: 1s - loss: 1.4353 - acc: 0.372 - ETA: 1s - loss: 1.4315 - acc: 0.375 - ETA: 0s - loss: 1.4300 - acc: 0.375 - ETA: 0s - loss: 1.4304 - acc: 0.375 - ETA: 0s - loss: 1.4389 - acc: 0.373 - ETA: 0s - loss: 1.4402 - acc: 0.372 - ETA: 0s - loss: 1.4379 - acc: 0.371 - ETA: 0s - loss: 1.4390 - acc: 0.372 - ETA: 0s - loss: 1.4374 - acc: 0.373 - ETA: 0s - loss: 1.4360 - acc: 0.375 - 7s 5ms/step - loss: 1.4368 - acc: 0.3760 - val_loss: 1.3551 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.43767\n",
      "Epoch 11/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0836 - acc: 0.625 - ETA: 6s - loss: 1.1997 - acc: 0.515 - ETA: 5s - loss: 1.3270 - acc: 0.437 - ETA: 5s - loss: 1.3301 - acc: 0.421 - ETA: 5s - loss: 1.3263 - acc: 0.425 - ETA: 5s - loss: 1.3351 - acc: 0.421 - ETA: 5s - loss: 1.3344 - acc: 0.433 - ETA: 5s - loss: 1.3516 - acc: 0.429 - ETA: 5s - loss: 1.3333 - acc: 0.441 - ETA: 5s - loss: 1.3379 - acc: 0.440 - ETA: 4s - loss: 1.3377 - acc: 0.434 - ETA: 4s - loss: 1.3413 - acc: 0.432 - ETA: 4s - loss: 1.3391 - acc: 0.420 - ETA: 4s - loss: 1.3525 - acc: 0.410 - ETA: 4s - loss: 1.3709 - acc: 0.400 - ETA: 4s - loss: 1.3663 - acc: 0.400 - ETA: 4s - loss: 1.3744 - acc: 0.395 - ETA: 3s - loss: 1.3819 - acc: 0.401 - ETA: 3s - loss: 1.3854 - acc: 0.394 - ETA: 3s - loss: 1.3818 - acc: 0.392 - ETA: 3s - loss: 1.3872 - acc: 0.385 - ETA: 3s - loss: 1.3896 - acc: 0.384 - ETA: 3s - loss: 1.3861 - acc: 0.388 - ETA: 3s - loss: 1.3907 - acc: 0.384 - ETA: 3s - loss: 1.3929 - acc: 0.383 - ETA: 2s - loss: 1.3989 - acc: 0.378 - ETA: 2s - loss: 1.4014 - acc: 0.377 - ETA: 2s - loss: 1.4046 - acc: 0.378 - ETA: 2s - loss: 1.4071 - acc: 0.378 - ETA: 2s - loss: 1.4069 - acc: 0.379 - ETA: 2s - loss: 1.4093 - acc: 0.379 - ETA: 2s - loss: 1.4052 - acc: 0.381 - ETA: 1s - loss: 1.4053 - acc: 0.381 - ETA: 1s - loss: 1.4054 - acc: 0.382 - ETA: 1s - loss: 1.4061 - acc: 0.382 - ETA: 1s - loss: 1.4046 - acc: 0.381 - ETA: 1s - loss: 1.4091 - acc: 0.380 - ETA: 1s - loss: 1.4115 - acc: 0.379 - ETA: 1s - loss: 1.4046 - acc: 0.383 - ETA: 0s - loss: 1.4041 - acc: 0.385 - ETA: 0s - loss: 1.4046 - acc: 0.384 - ETA: 0s - loss: 1.4018 - acc: 0.385 - ETA: 0s - loss: 1.4053 - acc: 0.383 - ETA: 0s - loss: 1.4058 - acc: 0.383 - ETA: 0s - loss: 1.4051 - acc: 0.384 - ETA: 0s - loss: 1.4038 - acc: 0.383 - ETA: 0s - loss: 1.4005 - acc: 0.384 - 7s 5ms/step - loss: 1.4033 - acc: 0.3833 - val_loss: 1.5040 - val_acc: 0.3050\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.43767\n",
      "Epoch 12/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.6535 - acc: 0.250 - ETA: 6s - loss: 1.6336 - acc: 0.296 - ETA: 5s - loss: 1.5565 - acc: 0.312 - ETA: 5s - loss: 1.5052 - acc: 0.343 - ETA: 5s - loss: 1.4887 - acc: 0.356 - ETA: 5s - loss: 1.4788 - acc: 0.354 - ETA: 5s - loss: 1.4720 - acc: 0.379 - ETA: 5s - loss: 1.4722 - acc: 0.386 - ETA: 5s - loss: 1.4649 - acc: 0.381 - ETA: 4s - loss: 1.4603 - acc: 0.378 - ETA: 4s - loss: 1.4583 - acc: 0.366 - ETA: 4s - loss: 1.4574 - acc: 0.362 - ETA: 4s - loss: 1.4434 - acc: 0.375 - ETA: 4s - loss: 1.4419 - acc: 0.379 - ETA: 4s - loss: 1.4423 - acc: 0.381 - ETA: 4s - loss: 1.4451 - acc: 0.371 - ETA: 4s - loss: 1.4451 - acc: 0.364 - ETA: 3s - loss: 1.4467 - acc: 0.364 - ETA: 3s - loss: 1.4424 - acc: 0.366 - ETA: 3s - loss: 1.4346 - acc: 0.375 - ETA: 3s - loss: 1.4296 - acc: 0.381 - ETA: 3s - loss: 1.4316 - acc: 0.375 - ETA: 3s - loss: 1.4314 - acc: 0.372 - ETA: 3s - loss: 1.4314 - acc: 0.375 - ETA: 2s - loss: 1.4323 - acc: 0.372 - ETA: 2s - loss: 1.4308 - acc: 0.370 - ETA: 2s - loss: 1.4319 - acc: 0.371 - ETA: 2s - loss: 1.4318 - acc: 0.369 - ETA: 2s - loss: 1.4325 - acc: 0.372 - ETA: 2s - loss: 1.4296 - acc: 0.375 - ETA: 2s - loss: 1.4309 - acc: 0.376 - ETA: 2s - loss: 1.4262 - acc: 0.379 - ETA: 1s - loss: 1.4321 - acc: 0.375 - ETA: 1s - loss: 1.4287 - acc: 0.377 - ETA: 1s - loss: 1.4241 - acc: 0.378 - ETA: 1s - loss: 1.4226 - acc: 0.378 - ETA: 1s - loss: 1.4279 - acc: 0.377 - ETA: 1s - loss: 1.4312 - acc: 0.375 - ETA: 1s - loss: 1.4313 - acc: 0.377 - ETA: 0s - loss: 1.4297 - acc: 0.378 - ETA: 0s - loss: 1.4299 - acc: 0.378 - ETA: 0s - loss: 1.4256 - acc: 0.381 - ETA: 0s - loss: 1.4244 - acc: 0.383 - ETA: 0s - loss: 1.4269 - acc: 0.381 - ETA: 0s - loss: 1.4242 - acc: 0.383 - ETA: 0s - loss: 1.4260 - acc: 0.384 - ETA: 0s - loss: 1.4221 - acc: 0.386 - 7s 5ms/step - loss: 1.4213 - acc: 0.3866 - val_loss: 1.3543 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.43767\n",
      "Epoch 13/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.4030 - acc: 0.375 - ETA: 5s - loss: 1.3961 - acc: 0.390 - ETA: 5s - loss: 1.3115 - acc: 0.458 - ETA: 5s - loss: 1.3063 - acc: 0.453 - ETA: 5s - loss: 1.3088 - acc: 0.431 - ETA: 5s - loss: 1.2841 - acc: 0.453 - ETA: 5s - loss: 1.2948 - acc: 0.450 - ETA: 5s - loss: 1.3109 - acc: 0.449 - ETA: 5s - loss: 1.3245 - acc: 0.447 - ETA: 4s - loss: 1.3184 - acc: 0.443 - ETA: 4s - loss: 1.3397 - acc: 0.426 - ETA: 4s - loss: 1.3472 - acc: 0.427 - ETA: 4s - loss: 1.3496 - acc: 0.427 - ETA: 4s - loss: 1.3534 - acc: 0.417 - ETA: 4s - loss: 1.3665 - acc: 0.418 - ETA: 4s - loss: 1.3702 - acc: 0.418 - ETA: 4s - loss: 1.3642 - acc: 0.419 - ETA: 3s - loss: 1.3601 - acc: 0.420 - ETA: 3s - loss: 1.3462 - acc: 0.434 - ETA: 3s - loss: 1.3378 - acc: 0.442 - ETA: 3s - loss: 1.3472 - acc: 0.436 - ETA: 3s - loss: 1.3465 - acc: 0.433 - ETA: 3s - loss: 1.3485 - acc: 0.433 - ETA: 3s - loss: 1.3525 - acc: 0.423 - ETA: 2s - loss: 1.3476 - acc: 0.428 - ETA: 2s - loss: 1.3492 - acc: 0.429 - ETA: 2s - loss: 1.3505 - acc: 0.424 - ETA: 2s - loss: 1.3505 - acc: 0.428 - ETA: 2s - loss: 1.3564 - acc: 0.423 - ETA: 2s - loss: 1.3569 - acc: 0.422 - ETA: 2s - loss: 1.3581 - acc: 0.419 - ETA: 2s - loss: 1.3575 - acc: 0.418 - ETA: 1s - loss: 1.3577 - acc: 0.420 - ETA: 1s - loss: 1.3584 - acc: 0.420 - ETA: 1s - loss: 1.3538 - acc: 0.421 - ETA: 1s - loss: 1.3596 - acc: 0.418 - ETA: 1s - loss: 1.3611 - acc: 0.418 - ETA: 1s - loss: 1.3629 - acc: 0.417 - ETA: 1s - loss: 1.3609 - acc: 0.415 - ETA: 0s - loss: 1.3607 - acc: 0.414 - ETA: 0s - loss: 1.3630 - acc: 0.412 - ETA: 0s - loss: 1.3659 - acc: 0.410 - ETA: 0s - loss: 1.3666 - acc: 0.409 - ETA: 0s - loss: 1.3661 - acc: 0.407 - ETA: 0s - loss: 1.3691 - acc: 0.409 - ETA: 0s - loss: 1.3701 - acc: 0.406 - ETA: 0s - loss: 1.3687 - acc: 0.406 - 7s 5ms/step - loss: 1.3688 - acc: 0.4065 - val_loss: 1.4549 - val_acc: 0.3395\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.43767\n",
      "Epoch 14/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.4236 - acc: 0.406 - ETA: 5s - loss: 1.3978 - acc: 0.406 - ETA: 5s - loss: 1.3360 - acc: 0.416 - ETA: 5s - loss: 1.4216 - acc: 0.390 - ETA: 5s - loss: 1.3948 - acc: 0.412 - ETA: 5s - loss: 1.4046 - acc: 0.411 - ETA: 5s - loss: 1.4056 - acc: 0.392 - ETA: 5s - loss: 1.3861 - acc: 0.390 - ETA: 5s - loss: 1.4057 - acc: 0.381 - ETA: 4s - loss: 1.3938 - acc: 0.400 - ETA: 4s - loss: 1.3851 - acc: 0.397 - ETA: 4s - loss: 1.3954 - acc: 0.395 - ETA: 4s - loss: 1.3821 - acc: 0.401 - ETA: 4s - loss: 1.3925 - acc: 0.404 - ETA: 4s - loss: 1.4012 - acc: 0.393 - ETA: 4s - loss: 1.4067 - acc: 0.388 - ETA: 4s - loss: 1.4016 - acc: 0.393 - ETA: 3s - loss: 1.3947 - acc: 0.394 - ETA: 3s - loss: 1.3909 - acc: 0.398 - ETA: 3s - loss: 1.3888 - acc: 0.396 - ETA: 3s - loss: 1.3981 - acc: 0.397 - ETA: 3s - loss: 1.3946 - acc: 0.396 - ETA: 3s - loss: 1.3913 - acc: 0.398 - ETA: 3s - loss: 1.3896 - acc: 0.399 - ETA: 2s - loss: 1.3854 - acc: 0.400 - ETA: 2s - loss: 1.3862 - acc: 0.400 - ETA: 2s - loss: 1.3846 - acc: 0.402 - ETA: 2s - loss: 1.3798 - acc: 0.408 - ETA: 2s - loss: 1.3752 - acc: 0.409 - ETA: 2s - loss: 1.3758 - acc: 0.408 - ETA: 2s - loss: 1.3761 - acc: 0.409 - ETA: 2s - loss: 1.3744 - acc: 0.408 - ETA: 1s - loss: 1.3697 - acc: 0.414 - ETA: 1s - loss: 1.3816 - acc: 0.407 - ETA: 1s - loss: 1.3917 - acc: 0.402 - ETA: 1s - loss: 1.3912 - acc: 0.398 - ETA: 1s - loss: 1.3935 - acc: 0.397 - ETA: 1s - loss: 1.3923 - acc: 0.398 - ETA: 1s - loss: 1.3876 - acc: 0.401 - ETA: 0s - loss: 1.3825 - acc: 0.405 - ETA: 0s - loss: 1.3859 - acc: 0.405 - ETA: 0s - loss: 1.3878 - acc: 0.403 - ETA: 0s - loss: 1.3806 - acc: 0.412 - ETA: 0s - loss: 1.3752 - acc: 0.412 - ETA: 0s - loss: 1.3761 - acc: 0.411 - ETA: 0s - loss: 1.3772 - acc: 0.410 - ETA: 0s - loss: 1.3791 - acc: 0.408 - 7s 5ms/step - loss: 1.3789 - acc: 0.4085 - val_loss: 1.4328 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.43767\n",
      "Epoch 15/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.3416 - acc: 0.406 - ETA: 5s - loss: 1.2428 - acc: 0.453 - ETA: 5s - loss: 1.3183 - acc: 0.416 - ETA: 5s - loss: 1.3439 - acc: 0.437 - ETA: 5s - loss: 1.3722 - acc: 0.425 - ETA: 5s - loss: 1.3580 - acc: 0.421 - ETA: 5s - loss: 1.3698 - acc: 0.406 - ETA: 5s - loss: 1.3619 - acc: 0.418 - ETA: 5s - loss: 1.4035 - acc: 0.402 - ETA: 4s - loss: 1.4151 - acc: 0.390 - ETA: 4s - loss: 1.4275 - acc: 0.380 - ETA: 4s - loss: 1.4162 - acc: 0.395 - ETA: 4s - loss: 1.4128 - acc: 0.394 - ETA: 4s - loss: 1.4330 - acc: 0.386 - ETA: 4s - loss: 1.4322 - acc: 0.389 - ETA: 4s - loss: 1.4252 - acc: 0.394 - ETA: 4s - loss: 1.4172 - acc: 0.389 - ETA: 3s - loss: 1.4128 - acc: 0.390 - ETA: 3s - loss: 1.4149 - acc: 0.383 - ETA: 3s - loss: 1.4094 - acc: 0.385 - ETA: 3s - loss: 1.4028 - acc: 0.394 - ETA: 3s - loss: 1.4005 - acc: 0.394 - ETA: 3s - loss: 1.4001 - acc: 0.395 - ETA: 3s - loss: 1.4023 - acc: 0.394 - ETA: 2s - loss: 1.4046 - acc: 0.391 - ETA: 2s - loss: 1.3995 - acc: 0.391 - ETA: 2s - loss: 1.3969 - acc: 0.394 - ETA: 2s - loss: 1.3959 - acc: 0.391 - ETA: 2s - loss: 1.3917 - acc: 0.394 - ETA: 2s - loss: 1.3872 - acc: 0.395 - ETA: 2s - loss: 1.3871 - acc: 0.398 - ETA: 2s - loss: 1.3888 - acc: 0.400 - ETA: 1s - loss: 1.3950 - acc: 0.395 - ETA: 1s - loss: 1.3941 - acc: 0.398 - ETA: 1s - loss: 1.3908 - acc: 0.400 - ETA: 1s - loss: 1.3894 - acc: 0.401 - ETA: 1s - loss: 1.3910 - acc: 0.402 - ETA: 1s - loss: 1.3909 - acc: 0.400 - ETA: 1s - loss: 1.3853 - acc: 0.402 - ETA: 0s - loss: 1.3875 - acc: 0.401 - ETA: 0s - loss: 1.3852 - acc: 0.403 - ETA: 0s - loss: 1.3840 - acc: 0.403 - ETA: 0s - loss: 1.3823 - acc: 0.404 - ETA: 0s - loss: 1.3816 - acc: 0.406 - ETA: 0s - loss: 1.3839 - acc: 0.404 - ETA: 0s - loss: 1.3803 - acc: 0.406 - ETA: 0s - loss: 1.3779 - acc: 0.406 - 7s 5ms/step - loss: 1.3772 - acc: 0.4065 - val_loss: 1.3765 - val_acc: 0.4032\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.43767\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 1.5304 - acc: 0.312 - ETA: 6s - loss: 1.4257 - acc: 0.328 - ETA: 5s - loss: 1.3727 - acc: 0.364 - ETA: 5s - loss: 1.3711 - acc: 0.367 - ETA: 5s - loss: 1.3452 - acc: 0.381 - ETA: 5s - loss: 1.3527 - acc: 0.364 - ETA: 5s - loss: 1.3229 - acc: 0.375 - ETA: 5s - loss: 1.3491 - acc: 0.382 - ETA: 5s - loss: 1.3499 - acc: 0.392 - ETA: 4s - loss: 1.3572 - acc: 0.396 - ETA: 4s - loss: 1.3307 - acc: 0.411 - ETA: 4s - loss: 1.3402 - acc: 0.411 - ETA: 4s - loss: 1.3289 - acc: 0.415 - ETA: 4s - loss: 1.3364 - acc: 0.412 - ETA: 4s - loss: 1.3278 - acc: 0.414 - ETA: 4s - loss: 1.3331 - acc: 0.410 - ETA: 4s - loss: 1.3309 - acc: 0.411 - ETA: 3s - loss: 1.3267 - acc: 0.416 - ETA: 3s - loss: 1.3421 - acc: 0.414 - ETA: 3s - loss: 1.3334 - acc: 0.421 - ETA: 3s - loss: 1.3260 - acc: 0.424 - ETA: 3s - loss: 1.3287 - acc: 0.420 - ETA: 3s - loss: 1.3260 - acc: 0.422 - ETA: 3s - loss: 1.3349 - acc: 0.418 - ETA: 2s - loss: 1.3349 - acc: 0.420 - ETA: 2s - loss: 1.3291 - acc: 0.418 - ETA: 2s - loss: 1.3261 - acc: 0.421 - ETA: 2s - loss: 1.3285 - acc: 0.418 - ETA: 2s - loss: 1.3245 - acc: 0.421 - ETA: 2s - loss: 1.3260 - acc: 0.422 - ETA: 2s - loss: 1.3244 - acc: 0.425 - ETA: 2s - loss: 1.3210 - acc: 0.425 - ETA: 1s - loss: 1.3228 - acc: 0.422 - ETA: 1s - loss: 1.3305 - acc: 0.417 - ETA: 1s - loss: 1.3344 - acc: 0.417 - ETA: 1s - loss: 1.3371 - acc: 0.415 - ETA: 1s - loss: 1.3386 - acc: 0.414 - ETA: 1s - loss: 1.3342 - acc: 0.417 - ETA: 1s - loss: 1.3369 - acc: 0.415 - ETA: 0s - loss: 1.3396 - acc: 0.418 - ETA: 0s - loss: 1.3409 - acc: 0.415 - ETA: 0s - loss: 1.3353 - acc: 0.418 - ETA: 0s - loss: 1.3352 - acc: 0.418 - ETA: 0s - loss: 1.3403 - acc: 0.416 - ETA: 0s - loss: 1.3380 - acc: 0.418 - ETA: 0s - loss: 1.3341 - acc: 0.420 - ETA: 0s - loss: 1.3317 - acc: 0.423 - 7s 5ms/step - loss: 1.3318 - acc: 0.4231 - val_loss: 1.3680 - val_acc: 0.3926\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.43767\n",
      "Epoch 17/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.6019 - acc: 0.343 - ETA: 6s - loss: 1.4407 - acc: 0.390 - ETA: 5s - loss: 1.3980 - acc: 0.395 - ETA: 5s - loss: 1.4297 - acc: 0.375 - ETA: 5s - loss: 1.4253 - acc: 0.337 - ETA: 5s - loss: 1.3836 - acc: 0.364 - ETA: 5s - loss: 1.3985 - acc: 0.366 - ETA: 5s - loss: 1.4127 - acc: 0.363 - ETA: 5s - loss: 1.4144 - acc: 0.364 - ETA: 4s - loss: 1.4095 - acc: 0.375 - ETA: 4s - loss: 1.4030 - acc: 0.386 - ETA: 4s - loss: 1.4112 - acc: 0.385 - ETA: 4s - loss: 1.4127 - acc: 0.394 - ETA: 4s - loss: 1.4142 - acc: 0.388 - ETA: 4s - loss: 1.4051 - acc: 0.400 - ETA: 4s - loss: 1.3961 - acc: 0.402 - ETA: 4s - loss: 1.3859 - acc: 0.409 - ETA: 3s - loss: 1.3730 - acc: 0.416 - ETA: 3s - loss: 1.3716 - acc: 0.414 - ETA: 3s - loss: 1.3677 - acc: 0.418 - ETA: 3s - loss: 1.3643 - acc: 0.422 - ETA: 3s - loss: 1.3529 - acc: 0.426 - ETA: 3s - loss: 1.3444 - acc: 0.432 - ETA: 3s - loss: 1.3358 - acc: 0.437 - ETA: 2s - loss: 1.3284 - acc: 0.441 - ETA: 2s - loss: 1.3276 - acc: 0.437 - ETA: 2s - loss: 1.3253 - acc: 0.438 - ETA: 2s - loss: 1.3332 - acc: 0.431 - ETA: 2s - loss: 1.3332 - acc: 0.430 - ETA: 2s - loss: 1.3277 - acc: 0.433 - ETA: 2s - loss: 1.3301 - acc: 0.433 - ETA: 2s - loss: 1.3246 - acc: 0.439 - ETA: 1s - loss: 1.3269 - acc: 0.434 - ETA: 1s - loss: 1.3258 - acc: 0.434 - ETA: 1s - loss: 1.3225 - acc: 0.439 - ETA: 1s - loss: 1.3185 - acc: 0.441 - ETA: 1s - loss: 1.3187 - acc: 0.440 - ETA: 1s - loss: 1.3216 - acc: 0.435 - ETA: 1s - loss: 1.3217 - acc: 0.435 - ETA: 0s - loss: 1.3224 - acc: 0.434 - ETA: 0s - loss: 1.3199 - acc: 0.436 - ETA: 0s - loss: 1.3262 - acc: 0.430 - ETA: 0s - loss: 1.3286 - acc: 0.428 - ETA: 0s - loss: 1.3284 - acc: 0.427 - ETA: 0s - loss: 1.3289 - acc: 0.427 - ETA: 0s - loss: 1.3265 - acc: 0.430 - ETA: 0s - loss: 1.3273 - acc: 0.428 - 7s 5ms/step - loss: 1.3269 - acc: 0.4290 - val_loss: 1.4851 - val_acc: 0.3236\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.43767\n",
      "Epoch 18/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.4648 - acc: 0.312 - ETA: 6s - loss: 1.4086 - acc: 0.390 - ETA: 6s - loss: 1.3487 - acc: 0.416 - ETA: 5s - loss: 1.3637 - acc: 0.390 - ETA: 5s - loss: 1.3415 - acc: 0.387 - ETA: 5s - loss: 1.2959 - acc: 0.406 - ETA: 5s - loss: 1.2843 - acc: 0.419 - ETA: 5s - loss: 1.2638 - acc: 0.433 - ETA: 5s - loss: 1.2627 - acc: 0.441 - ETA: 5s - loss: 1.2600 - acc: 0.450 - ETA: 4s - loss: 1.2508 - acc: 0.460 - ETA: 4s - loss: 1.2754 - acc: 0.453 - ETA: 4s - loss: 1.2815 - acc: 0.459 - ETA: 4s - loss: 1.2780 - acc: 0.450 - ETA: 4s - loss: 1.2712 - acc: 0.458 - ETA: 4s - loss: 1.2768 - acc: 0.457 - ETA: 4s - loss: 1.2832 - acc: 0.457 - ETA: 3s - loss: 1.2836 - acc: 0.456 - ETA: 3s - loss: 1.2825 - acc: 0.453 - ETA: 3s - loss: 1.2900 - acc: 0.450 - ETA: 3s - loss: 1.2905 - acc: 0.447 - ETA: 3s - loss: 1.2888 - acc: 0.447 - ETA: 3s - loss: 1.2849 - acc: 0.448 - ETA: 3s - loss: 1.2912 - acc: 0.445 - ETA: 2s - loss: 1.2776 - acc: 0.453 - ETA: 2s - loss: 1.2791 - acc: 0.449 - ETA: 2s - loss: 1.2815 - acc: 0.444 - ETA: 2s - loss: 1.2864 - acc: 0.440 - ETA: 2s - loss: 1.2903 - acc: 0.436 - ETA: 2s - loss: 1.2809 - acc: 0.442 - ETA: 2s - loss: 1.2845 - acc: 0.438 - ETA: 2s - loss: 1.2835 - acc: 0.437 - ETA: 1s - loss: 1.2802 - acc: 0.441 - ETA: 1s - loss: 1.2838 - acc: 0.440 - ETA: 1s - loss: 1.2893 - acc: 0.438 - ETA: 1s - loss: 1.2944 - acc: 0.436 - ETA: 1s - loss: 1.2951 - acc: 0.435 - ETA: 1s - loss: 1.2947 - acc: 0.434 - ETA: 1s - loss: 1.2966 - acc: 0.435 - ETA: 0s - loss: 1.2970 - acc: 0.436 - ETA: 0s - loss: 1.3005 - acc: 0.436 - ETA: 0s - loss: 1.3008 - acc: 0.435 - ETA: 0s - loss: 1.3016 - acc: 0.434 - ETA: 0s - loss: 1.3039 - acc: 0.431 - ETA: 0s - loss: 1.3073 - acc: 0.432 - ETA: 0s - loss: 1.3066 - acc: 0.434 - ETA: 0s - loss: 1.3043 - acc: 0.434 - 7s 5ms/step - loss: 1.3054 - acc: 0.4330 - val_loss: 1.3087 - val_acc: 0.4403\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.43767 to 0.44032, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 19/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.3928 - acc: 0.343 - ETA: 5s - loss: 1.3551 - acc: 0.375 - ETA: 5s - loss: 1.3346 - acc: 0.416 - ETA: 5s - loss: 1.3196 - acc: 0.429 - ETA: 5s - loss: 1.3060 - acc: 0.450 - ETA: 5s - loss: 1.3138 - acc: 0.463 - ETA: 5s - loss: 1.3064 - acc: 0.464 - ETA: 5s - loss: 1.3002 - acc: 0.460 - ETA: 5s - loss: 1.2930 - acc: 0.465 - ETA: 4s - loss: 1.3011 - acc: 0.462 - ETA: 4s - loss: 1.2899 - acc: 0.468 - ETA: 4s - loss: 1.3040 - acc: 0.460 - ETA: 4s - loss: 1.3091 - acc: 0.449 - ETA: 4s - loss: 1.3190 - acc: 0.435 - ETA: 4s - loss: 1.3092 - acc: 0.445 - ETA: 4s - loss: 1.3332 - acc: 0.441 - ETA: 4s - loss: 1.3399 - acc: 0.441 - ETA: 3s - loss: 1.3257 - acc: 0.446 - ETA: 3s - loss: 1.3242 - acc: 0.445 - ETA: 3s - loss: 1.3228 - acc: 0.445 - ETA: 3s - loss: 1.3254 - acc: 0.446 - ETA: 3s - loss: 1.3258 - acc: 0.443 - ETA: 3s - loss: 1.3233 - acc: 0.448 - ETA: 3s - loss: 1.3400 - acc: 0.438 - ETA: 2s - loss: 1.3367 - acc: 0.438 - ETA: 2s - loss: 1.3379 - acc: 0.433 - ETA: 2s - loss: 1.3399 - acc: 0.434 - ETA: 2s - loss: 1.3319 - acc: 0.438 - ETA: 2s - loss: 1.3328 - acc: 0.439 - ETA: 2s - loss: 1.3284 - acc: 0.444 - ETA: 2s - loss: 1.3236 - acc: 0.448 - ETA: 2s - loss: 1.3139 - acc: 0.452 - ETA: 1s - loss: 1.3150 - acc: 0.449 - ETA: 1s - loss: 1.3120 - acc: 0.453 - ETA: 1s - loss: 1.3145 - acc: 0.450 - ETA: 1s - loss: 1.3113 - acc: 0.450 - ETA: 1s - loss: 1.3130 - acc: 0.448 - ETA: 1s - loss: 1.3144 - acc: 0.444 - ETA: 1s - loss: 1.3171 - acc: 0.440 - ETA: 0s - loss: 1.3180 - acc: 0.440 - ETA: 0s - loss: 1.3154 - acc: 0.442 - ETA: 0s - loss: 1.3199 - acc: 0.439 - ETA: 0s - loss: 1.3230 - acc: 0.438 - ETA: 0s - loss: 1.3200 - acc: 0.440 - ETA: 0s - loss: 1.3210 - acc: 0.436 - ETA: 0s - loss: 1.3171 - acc: 0.437 - ETA: 0s - loss: 1.3248 - acc: 0.434 - 7s 5ms/step - loss: 1.3238 - acc: 0.4350 - val_loss: 1.4492 - val_acc: 0.3634\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.44032\n",
      "Epoch 20/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.4977 - acc: 0.281 - ETA: 5s - loss: 1.5224 - acc: 0.328 - ETA: 5s - loss: 1.4522 - acc: 0.364 - ETA: 5s - loss: 1.3954 - acc: 0.382 - ETA: 5s - loss: 1.3671 - acc: 0.375 - ETA: 5s - loss: 1.3776 - acc: 0.401 - ETA: 5s - loss: 1.3687 - acc: 0.406 - ETA: 5s - loss: 1.3572 - acc: 0.410 - ETA: 5s - loss: 1.3427 - acc: 0.416 - ETA: 5s - loss: 1.3513 - acc: 0.418 - ETA: 4s - loss: 1.3465 - acc: 0.423 - ETA: 4s - loss: 1.3396 - acc: 0.429 - ETA: 4s - loss: 1.3364 - acc: 0.435 - ETA: 4s - loss: 1.3331 - acc: 0.430 - ETA: 4s - loss: 1.3224 - acc: 0.431 - ETA: 4s - loss: 1.3239 - acc: 0.429 - ETA: 4s - loss: 1.3161 - acc: 0.435 - ETA: 3s - loss: 1.3155 - acc: 0.434 - ETA: 3s - loss: 1.3170 - acc: 0.429 - ETA: 3s - loss: 1.3054 - acc: 0.437 - ETA: 3s - loss: 1.3025 - acc: 0.442 - ETA: 3s - loss: 1.3001 - acc: 0.438 - ETA: 3s - loss: 1.3076 - acc: 0.436 - ETA: 3s - loss: 1.3006 - acc: 0.437 - ETA: 2s - loss: 1.3101 - acc: 0.432 - ETA: 2s - loss: 1.3127 - acc: 0.432 - ETA: 2s - loss: 1.3177 - acc: 0.427 - ETA: 2s - loss: 1.3083 - acc: 0.436 - ETA: 2s - loss: 1.3078 - acc: 0.440 - ETA: 2s - loss: 1.3064 - acc: 0.437 - ETA: 2s - loss: 1.3084 - acc: 0.438 - ETA: 2s - loss: 1.3180 - acc: 0.433 - ETA: 1s - loss: 1.3180 - acc: 0.436 - ETA: 1s - loss: 1.3166 - acc: 0.437 - ETA: 1s - loss: 1.3176 - acc: 0.435 - ETA: 1s - loss: 1.3154 - acc: 0.436 - ETA: 1s - loss: 1.3155 - acc: 0.436 - ETA: 1s - loss: 1.3184 - acc: 0.433 - ETA: 1s - loss: 1.3201 - acc: 0.432 - ETA: 0s - loss: 1.3208 - acc: 0.433 - ETA: 0s - loss: 1.3128 - acc: 0.437 - ETA: 0s - loss: 1.3225 - acc: 0.435 - ETA: 0s - loss: 1.3184 - acc: 0.436 - ETA: 0s - loss: 1.3173 - acc: 0.438 - ETA: 0s - loss: 1.3177 - acc: 0.438 - ETA: 0s - loss: 1.3169 - acc: 0.438 - ETA: 0s - loss: 1.3173 - acc: 0.438 - 7s 5ms/step - loss: 1.3175 - acc: 0.4390 - val_loss: 1.4129 - val_acc: 0.3634\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.44032\n",
      "Epoch 21/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.4769 - acc: 0.437 - ETA: 6s - loss: 1.4254 - acc: 0.421 - ETA: 5s - loss: 1.3502 - acc: 0.447 - ETA: 5s - loss: 1.3305 - acc: 0.468 - ETA: 5s - loss: 1.3121 - acc: 0.468 - ETA: 5s - loss: 1.3001 - acc: 0.468 - ETA: 5s - loss: 1.2879 - acc: 0.473 - ETA: 5s - loss: 1.2626 - acc: 0.496 - ETA: 5s - loss: 1.2354 - acc: 0.506 - ETA: 5s - loss: 1.2219 - acc: 0.503 - ETA: 4s - loss: 1.2171 - acc: 0.505 - ETA: 4s - loss: 1.2178 - acc: 0.502 - ETA: 4s - loss: 1.2119 - acc: 0.497 - ETA: 4s - loss: 1.2076 - acc: 0.502 - ETA: 4s - loss: 1.2126 - acc: 0.497 - ETA: 4s - loss: 1.2219 - acc: 0.494 - ETA: 4s - loss: 1.2316 - acc: 0.494 - ETA: 3s - loss: 1.2260 - acc: 0.500 - ETA: 3s - loss: 1.2278 - acc: 0.503 - ETA: 3s - loss: 1.2247 - acc: 0.506 - ETA: 3s - loss: 1.2206 - acc: 0.507 - ETA: 3s - loss: 1.2106 - acc: 0.511 - ETA: 3s - loss: 1.2103 - acc: 0.510 - ETA: 3s - loss: 1.2217 - acc: 0.501 - ETA: 2s - loss: 1.2367 - acc: 0.498 - ETA: 2s - loss: 1.2428 - acc: 0.498 - ETA: 2s - loss: 1.2446 - acc: 0.497 - ETA: 2s - loss: 1.2439 - acc: 0.494 - ETA: 2s - loss: 1.2470 - acc: 0.497 - ETA: 2s - loss: 1.2466 - acc: 0.497 - ETA: 2s - loss: 1.2475 - acc: 0.499 - ETA: 2s - loss: 1.2512 - acc: 0.494 - ETA: 1s - loss: 1.2450 - acc: 0.494 - ETA: 1s - loss: 1.2416 - acc: 0.496 - ETA: 1s - loss: 1.2412 - acc: 0.499 - ETA: 1s - loss: 1.2481 - acc: 0.492 - ETA: 1s - loss: 1.2477 - acc: 0.492 - ETA: 1s - loss: 1.2473 - acc: 0.491 - ETA: 1s - loss: 1.2474 - acc: 0.490 - ETA: 0s - loss: 1.2503 - acc: 0.485 - ETA: 0s - loss: 1.2514 - acc: 0.484 - ETA: 0s - loss: 1.2480 - acc: 0.487 - ETA: 0s - loss: 1.2535 - acc: 0.484 - ETA: 0s - loss: 1.2538 - acc: 0.484 - ETA: 0s - loss: 1.2546 - acc: 0.484 - ETA: 0s - loss: 1.2574 - acc: 0.484 - ETA: 0s - loss: 1.2568 - acc: 0.484 - 7s 5ms/step - loss: 1.2551 - acc: 0.4847 - val_loss: 2.1342 - val_acc: 0.2546\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.44032\n",
      "Epoch 22/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.9705 - acc: 0.218 - ETA: 6s - loss: 1.8552 - acc: 0.250 - ETA: 5s - loss: 1.6903 - acc: 0.302 - ETA: 5s - loss: 1.5842 - acc: 0.312 - ETA: 5s - loss: 1.5664 - acc: 0.343 - ETA: 5s - loss: 1.5536 - acc: 0.328 - ETA: 5s - loss: 1.5092 - acc: 0.357 - ETA: 5s - loss: 1.4831 - acc: 0.375 - ETA: 5s - loss: 1.4687 - acc: 0.381 - ETA: 5s - loss: 1.4628 - acc: 0.381 - ETA: 4s - loss: 1.4473 - acc: 0.383 - ETA: 4s - loss: 1.4356 - acc: 0.388 - ETA: 4s - loss: 1.4292 - acc: 0.384 - ETA: 4s - loss: 1.4222 - acc: 0.395 - ETA: 4s - loss: 1.4160 - acc: 0.391 - ETA: 4s - loss: 1.4192 - acc: 0.390 - ETA: 4s - loss: 1.4211 - acc: 0.389 - ETA: 3s - loss: 1.4134 - acc: 0.392 - ETA: 3s - loss: 1.4101 - acc: 0.391 - ETA: 3s - loss: 1.4092 - acc: 0.395 - ETA: 3s - loss: 1.4136 - acc: 0.386 - ETA: 3s - loss: 1.4137 - acc: 0.382 - ETA: 3s - loss: 1.3969 - acc: 0.388 - ETA: 3s - loss: 1.3956 - acc: 0.391 - ETA: 2s - loss: 1.3965 - acc: 0.386 - ETA: 2s - loss: 1.3901 - acc: 0.389 - ETA: 2s - loss: 1.3859 - acc: 0.395 - ETA: 2s - loss: 1.3791 - acc: 0.399 - ETA: 2s - loss: 1.3749 - acc: 0.399 - ETA: 2s - loss: 1.3773 - acc: 0.395 - ETA: 2s - loss: 1.3727 - acc: 0.400 - ETA: 2s - loss: 1.3661 - acc: 0.402 - ETA: 1s - loss: 1.3575 - acc: 0.404 - ETA: 1s - loss: 1.3539 - acc: 0.408 - ETA: 1s - loss: 1.3508 - acc: 0.408 - ETA: 1s - loss: 1.3527 - acc: 0.410 - ETA: 1s - loss: 1.3543 - acc: 0.411 - ETA: 1s - loss: 1.3478 - acc: 0.413 - ETA: 1s - loss: 1.3423 - acc: 0.417 - ETA: 0s - loss: 1.3414 - acc: 0.418 - ETA: 0s - loss: 1.3406 - acc: 0.416 - ETA: 0s - loss: 1.3399 - acc: 0.418 - ETA: 0s - loss: 1.3366 - acc: 0.419 - ETA: 0s - loss: 1.3353 - acc: 0.419 - ETA: 0s - loss: 1.3339 - acc: 0.421 - ETA: 0s - loss: 1.3302 - acc: 0.424 - ETA: 0s - loss: 1.3296 - acc: 0.424 - 7s 5ms/step - loss: 1.3289 - acc: 0.4251 - val_loss: 1.4837 - val_acc: 0.4032\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.44032\n",
      "Epoch 23/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.1339 - acc: 0.437 - ETA: 6s - loss: 1.1714 - acc: 0.437 - ETA: 5s - loss: 1.1757 - acc: 0.447 - ETA: 5s - loss: 1.2316 - acc: 0.421 - ETA: 5s - loss: 1.2229 - acc: 0.456 - ETA: 5s - loss: 1.2719 - acc: 0.458 - ETA: 5s - loss: 1.3084 - acc: 0.437 - ETA: 5s - loss: 1.3416 - acc: 0.425 - ETA: 5s - loss: 1.3392 - acc: 0.437 - ETA: 4s - loss: 1.3463 - acc: 0.431 - ETA: 4s - loss: 1.3464 - acc: 0.434 - ETA: 4s - loss: 1.3445 - acc: 0.440 - ETA: 4s - loss: 1.3329 - acc: 0.449 - ETA: 4s - loss: 1.3193 - acc: 0.459 - ETA: 4s - loss: 1.3026 - acc: 0.458 - ETA: 4s - loss: 1.3031 - acc: 0.453 - ETA: 4s - loss: 1.2853 - acc: 0.463 - ETA: 3s - loss: 1.2832 - acc: 0.463 - ETA: 3s - loss: 1.2865 - acc: 0.460 - ETA: 3s - loss: 1.2800 - acc: 0.462 - ETA: 3s - loss: 1.2802 - acc: 0.461 - ETA: 3s - loss: 1.2740 - acc: 0.467 - ETA: 3s - loss: 1.2621 - acc: 0.470 - ETA: 3s - loss: 1.2640 - acc: 0.471 - ETA: 2s - loss: 1.2607 - acc: 0.471 - ETA: 2s - loss: 1.2596 - acc: 0.474 - ETA: 2s - loss: 1.2532 - acc: 0.478 - ETA: 2s - loss: 1.2495 - acc: 0.479 - ETA: 2s - loss: 1.2435 - acc: 0.481 - ETA: 2s - loss: 1.2338 - acc: 0.485 - ETA: 2s - loss: 1.2416 - acc: 0.478 - ETA: 2s - loss: 1.2460 - acc: 0.477 - ETA: 1s - loss: 1.2485 - acc: 0.479 - ETA: 1s - loss: 1.2537 - acc: 0.477 - ETA: 1s - loss: 1.2549 - acc: 0.477 - ETA: 1s - loss: 1.2526 - acc: 0.479 - ETA: 1s - loss: 1.2513 - acc: 0.481 - ETA: 1s - loss: 1.2551 - acc: 0.480 - ETA: 1s - loss: 1.2517 - acc: 0.482 - ETA: 0s - loss: 1.2480 - acc: 0.485 - ETA: 0s - loss: 1.2458 - acc: 0.484 - ETA: 0s - loss: 1.2474 - acc: 0.484 - ETA: 0s - loss: 1.2466 - acc: 0.486 - ETA: 0s - loss: 1.2485 - acc: 0.483 - ETA: 0s - loss: 1.2516 - acc: 0.480 - ETA: 0s - loss: 1.2516 - acc: 0.482 - ETA: 0s - loss: 1.2538 - acc: 0.484 - 7s 5ms/step - loss: 1.2534 - acc: 0.4847 - val_loss: 1.4154 - val_acc: 0.4191\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.44032\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 1.6470 - acc: 0.250 - ETA: 6s - loss: 1.4304 - acc: 0.343 - ETA: 5s - loss: 1.3693 - acc: 0.416 - ETA: 5s - loss: 1.3089 - acc: 0.445 - ETA: 5s - loss: 1.2962 - acc: 0.437 - ETA: 5s - loss: 1.3124 - acc: 0.427 - ETA: 5s - loss: 1.3206 - acc: 0.433 - ETA: 5s - loss: 1.3255 - acc: 0.429 - ETA: 5s - loss: 1.3214 - acc: 0.423 - ETA: 5s - loss: 1.3115 - acc: 0.434 - ETA: 4s - loss: 1.3023 - acc: 0.443 - ETA: 4s - loss: 1.2792 - acc: 0.463 - ETA: 4s - loss: 1.2867 - acc: 0.451 - ETA: 4s - loss: 1.2845 - acc: 0.450 - ETA: 4s - loss: 1.2724 - acc: 0.454 - ETA: 4s - loss: 1.2732 - acc: 0.453 - ETA: 4s - loss: 1.2776 - acc: 0.459 - ETA: 3s - loss: 1.2622 - acc: 0.470 - ETA: 3s - loss: 1.2498 - acc: 0.472 - ETA: 3s - loss: 1.2458 - acc: 0.476 - ETA: 3s - loss: 1.2500 - acc: 0.474 - ETA: 3s - loss: 1.2567 - acc: 0.475 - ETA: 3s - loss: 1.2560 - acc: 0.476 - ETA: 3s - loss: 1.2622 - acc: 0.471 - ETA: 2s - loss: 1.2608 - acc: 0.472 - ETA: 2s - loss: 1.2576 - acc: 0.474 - ETA: 2s - loss: 1.2615 - acc: 0.473 - ETA: 2s - loss: 1.2546 - acc: 0.476 - ETA: 2s - loss: 1.2527 - acc: 0.477 - ETA: 2s - loss: 1.2443 - acc: 0.482 - ETA: 2s - loss: 1.2420 - acc: 0.481 - ETA: 2s - loss: 1.2470 - acc: 0.479 - ETA: 1s - loss: 1.2469 - acc: 0.482 - ETA: 1s - loss: 1.2481 - acc: 0.482 - ETA: 1s - loss: 1.2468 - acc: 0.482 - ETA: 1s - loss: 1.2507 - acc: 0.483 - ETA: 1s - loss: 1.2514 - acc: 0.482 - ETA: 1s - loss: 1.2523 - acc: 0.481 - ETA: 1s - loss: 1.2520 - acc: 0.485 - ETA: 0s - loss: 1.2561 - acc: 0.481 - ETA: 0s - loss: 1.2521 - acc: 0.482 - ETA: 0s - loss: 1.2516 - acc: 0.483 - ETA: 0s - loss: 1.2510 - acc: 0.481 - ETA: 0s - loss: 1.2501 - acc: 0.480 - ETA: 0s - loss: 1.2525 - acc: 0.478 - ETA: 0s - loss: 1.2569 - acc: 0.475 - ETA: 0s - loss: 1.2588 - acc: 0.472 - 7s 5ms/step - loss: 1.2583 - acc: 0.4735 - val_loss: 1.4550 - val_acc: 0.3926\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.44032\n",
      "Epoch 25/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.4918 - acc: 0.468 - ETA: 5s - loss: 1.4114 - acc: 0.421 - ETA: 5s - loss: 1.3873 - acc: 0.427 - ETA: 5s - loss: 1.3254 - acc: 0.445 - ETA: 5s - loss: 1.2824 - acc: 0.462 - ETA: 5s - loss: 1.3119 - acc: 0.437 - ETA: 5s - loss: 1.3042 - acc: 0.437 - ETA: 5s - loss: 1.3043 - acc: 0.437 - ETA: 5s - loss: 1.2933 - acc: 0.451 - ETA: 4s - loss: 1.2964 - acc: 0.453 - ETA: 4s - loss: 1.2671 - acc: 0.465 - ETA: 4s - loss: 1.2440 - acc: 0.481 - ETA: 4s - loss: 1.2300 - acc: 0.488 - ETA: 4s - loss: 1.2092 - acc: 0.491 - ETA: 4s - loss: 1.1999 - acc: 0.506 - ETA: 4s - loss: 1.1932 - acc: 0.511 - ETA: 4s - loss: 1.1843 - acc: 0.512 - ETA: 3s - loss: 1.1800 - acc: 0.517 - ETA: 3s - loss: 1.1740 - acc: 0.523 - ETA: 3s - loss: 1.1824 - acc: 0.520 - ETA: 3s - loss: 1.1732 - acc: 0.523 - ETA: 3s - loss: 1.1742 - acc: 0.521 - ETA: 3s - loss: 1.1832 - acc: 0.520 - ETA: 3s - loss: 1.1804 - acc: 0.520 - ETA: 2s - loss: 1.1909 - acc: 0.512 - ETA: 2s - loss: 1.1947 - acc: 0.512 - ETA: 2s - loss: 1.1940 - acc: 0.510 - ETA: 2s - loss: 1.1979 - acc: 0.507 - ETA: 2s - loss: 1.2035 - acc: 0.506 - ETA: 2s - loss: 1.2070 - acc: 0.505 - ETA: 2s - loss: 1.2072 - acc: 0.505 - ETA: 2s - loss: 1.2024 - acc: 0.508 - ETA: 1s - loss: 1.2081 - acc: 0.508 - ETA: 1s - loss: 1.2075 - acc: 0.509 - ETA: 1s - loss: 1.2098 - acc: 0.509 - ETA: 1s - loss: 1.2146 - acc: 0.509 - ETA: 1s - loss: 1.2192 - acc: 0.503 - ETA: 1s - loss: 1.2167 - acc: 0.506 - ETA: 1s - loss: 1.2184 - acc: 0.504 - ETA: 0s - loss: 1.2171 - acc: 0.503 - ETA: 0s - loss: 1.2138 - acc: 0.506 - ETA: 0s - loss: 1.2119 - acc: 0.508 - ETA: 0s - loss: 1.2122 - acc: 0.509 - ETA: 0s - loss: 1.2158 - acc: 0.507 - ETA: 0s - loss: 1.2216 - acc: 0.505 - ETA: 0s - loss: 1.2176 - acc: 0.508 - ETA: 0s - loss: 1.2133 - acc: 0.510 - 7s 5ms/step - loss: 1.2141 - acc: 0.5093 - val_loss: 1.9681 - val_acc: 0.3501\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.44032\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 26/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.7506 - acc: 0.406 - ETA: 5s - loss: 1.7398 - acc: 0.359 - ETA: 5s - loss: 2.1152 - acc: 0.333 - ETA: 5s - loss: 2.3504 - acc: 0.304 - ETA: 5s - loss: 2.6429 - acc: 0.262 - ETA: 5s - loss: 2.6021 - acc: 0.255 - ETA: 5s - loss: 2.5284 - acc: 0.263 - ETA: 5s - loss: 2.4846 - acc: 0.253 - ETA: 5s - loss: 2.3744 - acc: 0.263 - ETA: 4s - loss: 2.3430 - acc: 0.265 - ETA: 4s - loss: 2.2988 - acc: 0.261 - ETA: 4s - loss: 2.2164 - acc: 0.283 - ETA: 4s - loss: 2.1675 - acc: 0.290 - ETA: 4s - loss: 2.1288 - acc: 0.294 - ETA: 4s - loss: 2.0889 - acc: 0.295 - ETA: 4s - loss: 2.0509 - acc: 0.302 - ETA: 4s - loss: 2.0210 - acc: 0.305 - ETA: 3s - loss: 1.9894 - acc: 0.314 - ETA: 3s - loss: 1.9794 - acc: 0.315 - ETA: 3s - loss: 1.9539 - acc: 0.318 - ETA: 3s - loss: 1.9401 - acc: 0.317 - ETA: 3s - loss: 1.9217 - acc: 0.316 - ETA: 3s - loss: 1.9065 - acc: 0.312 - ETA: 3s - loss: 1.8941 - acc: 0.315 - ETA: 2s - loss: 1.8828 - acc: 0.316 - ETA: 2s - loss: 1.8637 - acc: 0.324 - ETA: 2s - loss: 1.8505 - acc: 0.327 - ETA: 2s - loss: 1.8382 - acc: 0.333 - ETA: 2s - loss: 1.8266 - acc: 0.338 - ETA: 2s - loss: 1.8188 - acc: 0.336 - ETA: 2s - loss: 1.8078 - acc: 0.332 - ETA: 2s - loss: 1.8036 - acc: 0.331 - ETA: 1s - loss: 1.7932 - acc: 0.330 - ETA: 1s - loss: 1.7828 - acc: 0.332 - ETA: 1s - loss: 1.7754 - acc: 0.331 - ETA: 1s - loss: 1.7650 - acc: 0.335 - ETA: 1s - loss: 1.7597 - acc: 0.335 - ETA: 1s - loss: 1.7494 - acc: 0.338 - ETA: 1s - loss: 1.7400 - acc: 0.340 - ETA: 0s - loss: 1.7295 - acc: 0.343 - ETA: 0s - loss: 1.7219 - acc: 0.343 - ETA: 0s - loss: 1.7122 - acc: 0.348 - ETA: 0s - loss: 1.7042 - acc: 0.348 - ETA: 0s - loss: 1.6955 - acc: 0.351 - ETA: 0s - loss: 1.6874 - acc: 0.354 - ETA: 0s - loss: 1.6790 - acc: 0.360 - ETA: 0s - loss: 1.6809 - acc: 0.359 - 7s 5ms/step - loss: 1.6801 - acc: 0.3588 - val_loss: 1.5127 - val_acc: 0.3316\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.44032\n",
      "Epoch 27/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.3635 - acc: 0.375 - ETA: 6s - loss: 1.3829 - acc: 0.328 - ETA: 5s - loss: 1.3896 - acc: 0.343 - ETA: 5s - loss: 1.3235 - acc: 0.367 - ETA: 5s - loss: 1.3679 - acc: 0.368 - ETA: 5s - loss: 1.3794 - acc: 0.375 - ETA: 5s - loss: 1.3627 - acc: 0.397 - ETA: 5s - loss: 1.3623 - acc: 0.410 - ETA: 5s - loss: 1.3447 - acc: 0.427 - ETA: 5s - loss: 1.3619 - acc: 0.421 - ETA: 4s - loss: 1.3696 - acc: 0.411 - ETA: 4s - loss: 1.3778 - acc: 0.427 - ETA: 4s - loss: 1.3766 - acc: 0.425 - ETA: 4s - loss: 1.3821 - acc: 0.412 - ETA: 4s - loss: 1.3901 - acc: 0.410 - ETA: 4s - loss: 1.3849 - acc: 0.419 - ETA: 4s - loss: 1.3820 - acc: 0.417 - ETA: 3s - loss: 1.3830 - acc: 0.418 - ETA: 3s - loss: 1.3805 - acc: 0.424 - ETA: 3s - loss: 1.3679 - acc: 0.428 - ETA: 3s - loss: 1.3658 - acc: 0.430 - ETA: 3s - loss: 1.3614 - acc: 0.433 - ETA: 3s - loss: 1.3533 - acc: 0.434 - ETA: 3s - loss: 1.3498 - acc: 0.433 - ETA: 2s - loss: 1.3502 - acc: 0.430 - ETA: 2s - loss: 1.3464 - acc: 0.432 - ETA: 2s - loss: 1.3493 - acc: 0.432 - ETA: 2s - loss: 1.3527 - acc: 0.431 - ETA: 2s - loss: 1.3523 - acc: 0.427 - ETA: 2s - loss: 1.3487 - acc: 0.428 - ETA: 2s - loss: 1.3521 - acc: 0.426 - ETA: 2s - loss: 1.3517 - acc: 0.425 - ETA: 1s - loss: 1.3550 - acc: 0.421 - ETA: 1s - loss: 1.3531 - acc: 0.423 - ETA: 1s - loss: 1.3519 - acc: 0.425 - ETA: 1s - loss: 1.3502 - acc: 0.428 - ETA: 1s - loss: 1.3426 - acc: 0.435 - ETA: 1s - loss: 1.3441 - acc: 0.436 - ETA: 1s - loss: 1.3436 - acc: 0.436 - ETA: 0s - loss: 1.3393 - acc: 0.438 - ETA: 0s - loss: 1.3401 - acc: 0.439 - ETA: 0s - loss: 1.3374 - acc: 0.440 - ETA: 0s - loss: 1.3401 - acc: 0.439 - ETA: 0s - loss: 1.3393 - acc: 0.439 - ETA: 0s - loss: 1.3337 - acc: 0.443 - ETA: 0s - loss: 1.3333 - acc: 0.444 - ETA: 0s - loss: 1.3338 - acc: 0.444 - 7s 5ms/step - loss: 1.3332 - acc: 0.4450 - val_loss: 1.3315 - val_acc: 0.4191\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.44032\n",
      "Epoch 28/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0629 - acc: 0.625 - ETA: 6s - loss: 1.0916 - acc: 0.625 - ETA: 5s - loss: 1.1433 - acc: 0.593 - ETA: 5s - loss: 1.1585 - acc: 0.585 - ETA: 5s - loss: 1.2132 - acc: 0.550 - ETA: 5s - loss: 1.2038 - acc: 0.546 - ETA: 5s - loss: 1.2546 - acc: 0.508 - ETA: 5s - loss: 1.2653 - acc: 0.488 - ETA: 5s - loss: 1.2907 - acc: 0.465 - ETA: 5s - loss: 1.2898 - acc: 0.465 - ETA: 4s - loss: 1.2960 - acc: 0.468 - ETA: 4s - loss: 1.2790 - acc: 0.479 - ETA: 4s - loss: 1.2768 - acc: 0.480 - ETA: 4s - loss: 1.2690 - acc: 0.482 - ETA: 4s - loss: 1.2592 - acc: 0.485 - ETA: 4s - loss: 1.2578 - acc: 0.480 - ETA: 4s - loss: 1.2634 - acc: 0.476 - ETA: 3s - loss: 1.2656 - acc: 0.468 - ETA: 3s - loss: 1.2650 - acc: 0.472 - ETA: 3s - loss: 1.2635 - acc: 0.470 - ETA: 3s - loss: 1.2615 - acc: 0.476 - ETA: 3s - loss: 1.2556 - acc: 0.480 - ETA: 3s - loss: 1.2630 - acc: 0.475 - ETA: 3s - loss: 1.2624 - acc: 0.475 - ETA: 2s - loss: 1.2645 - acc: 0.472 - ETA: 2s - loss: 1.2588 - acc: 0.473 - ETA: 2s - loss: 1.2586 - acc: 0.474 - ETA: 2s - loss: 1.2542 - acc: 0.473 - ETA: 2s - loss: 1.2575 - acc: 0.469 - ETA: 2s - loss: 1.2520 - acc: 0.470 - ETA: 2s - loss: 1.2594 - acc: 0.469 - ETA: 2s - loss: 1.2581 - acc: 0.468 - ETA: 1s - loss: 1.2608 - acc: 0.466 - ETA: 1s - loss: 1.2637 - acc: 0.466 - ETA: 1s - loss: 1.2626 - acc: 0.465 - ETA: 1s - loss: 1.2605 - acc: 0.467 - ETA: 1s - loss: 1.2574 - acc: 0.470 - ETA: 1s - loss: 1.2569 - acc: 0.471 - ETA: 1s - loss: 1.2565 - acc: 0.469 - ETA: 0s - loss: 1.2567 - acc: 0.465 - ETA: 0s - loss: 1.2543 - acc: 0.465 - ETA: 0s - loss: 1.2520 - acc: 0.468 - ETA: 0s - loss: 1.2489 - acc: 0.470 - ETA: 0s - loss: 1.2477 - acc: 0.471 - ETA: 0s - loss: 1.2445 - acc: 0.472 - ETA: 0s - loss: 1.2436 - acc: 0.471 - ETA: 0s - loss: 1.2435 - acc: 0.471 - 7s 5ms/step - loss: 1.2447 - acc: 0.4715 - val_loss: 1.2731 - val_acc: 0.4403\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.44032\n",
      "Epoch 29/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.3870 - acc: 0.406 - ETA: 6s - loss: 1.3174 - acc: 0.468 - ETA: 5s - loss: 1.2142 - acc: 0.510 - ETA: 5s - loss: 1.2097 - acc: 0.531 - ETA: 5s - loss: 1.1693 - acc: 0.550 - ETA: 5s - loss: 1.1692 - acc: 0.541 - ETA: 5s - loss: 1.1988 - acc: 0.513 - ETA: 5s - loss: 1.1836 - acc: 0.523 - ETA: 5s - loss: 1.1800 - acc: 0.513 - ETA: 5s - loss: 1.1789 - acc: 0.518 - ETA: 4s - loss: 1.1694 - acc: 0.525 - ETA: 4s - loss: 1.1682 - acc: 0.526 - ETA: 4s - loss: 1.1789 - acc: 0.516 - ETA: 4s - loss: 1.1775 - acc: 0.517 - ETA: 4s - loss: 1.1690 - acc: 0.525 - ETA: 4s - loss: 1.1708 - acc: 0.529 - ETA: 4s - loss: 1.1742 - acc: 0.523 - ETA: 3s - loss: 1.1724 - acc: 0.524 - ETA: 3s - loss: 1.1693 - acc: 0.524 - ETA: 3s - loss: 1.1720 - acc: 0.523 - ETA: 3s - loss: 1.1840 - acc: 0.516 - ETA: 3s - loss: 1.1842 - acc: 0.519 - ETA: 3s - loss: 1.1811 - acc: 0.517 - ETA: 3s - loss: 1.1759 - acc: 0.518 - ETA: 2s - loss: 1.1811 - acc: 0.510 - ETA: 2s - loss: 1.1797 - acc: 0.508 - ETA: 2s - loss: 1.1789 - acc: 0.510 - ETA: 2s - loss: 1.1744 - acc: 0.507 - ETA: 2s - loss: 1.1726 - acc: 0.510 - ETA: 2s - loss: 1.1729 - acc: 0.513 - ETA: 2s - loss: 1.1736 - acc: 0.511 - ETA: 2s - loss: 1.1724 - acc: 0.509 - ETA: 1s - loss: 1.1727 - acc: 0.510 - ETA: 1s - loss: 1.1759 - acc: 0.509 - ETA: 1s - loss: 1.1735 - acc: 0.509 - ETA: 1s - loss: 1.1701 - acc: 0.511 - ETA: 1s - loss: 1.1687 - acc: 0.511 - ETA: 1s - loss: 1.1703 - acc: 0.514 - ETA: 1s - loss: 1.1714 - acc: 0.510 - ETA: 0s - loss: 1.1686 - acc: 0.514 - ETA: 0s - loss: 1.1685 - acc: 0.515 - ETA: 0s - loss: 1.1702 - acc: 0.514 - ETA: 0s - loss: 1.1670 - acc: 0.516 - ETA: 0s - loss: 1.1671 - acc: 0.514 - ETA: 0s - loss: 1.1738 - acc: 0.513 - ETA: 0s - loss: 1.1714 - acc: 0.514 - ETA: 0s - loss: 1.1667 - acc: 0.518 - 7s 5ms/step - loss: 1.1655 - acc: 0.5186 - val_loss: 1.3099 - val_acc: 0.4801\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.44032 to 0.48011, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 30/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.2707 - acc: 0.531 - ETA: 5s - loss: 1.2425 - acc: 0.546 - ETA: 5s - loss: 1.2191 - acc: 0.520 - ETA: 5s - loss: 1.2327 - acc: 0.507 - ETA: 5s - loss: 1.2324 - acc: 0.512 - ETA: 5s - loss: 1.2433 - acc: 0.494 - ETA: 5s - loss: 1.2134 - acc: 0.513 - ETA: 5s - loss: 1.2300 - acc: 0.500 - ETA: 5s - loss: 1.2112 - acc: 0.503 - ETA: 4s - loss: 1.1923 - acc: 0.521 - ETA: 4s - loss: 1.1777 - acc: 0.528 - ETA: 4s - loss: 1.1806 - acc: 0.520 - ETA: 4s - loss: 1.1827 - acc: 0.519 - ETA: 4s - loss: 1.1943 - acc: 0.511 - ETA: 4s - loss: 1.1945 - acc: 0.508 - ETA: 4s - loss: 1.1770 - acc: 0.517 - ETA: 4s - loss: 1.1716 - acc: 0.522 - ETA: 3s - loss: 1.1581 - acc: 0.534 - ETA: 3s - loss: 1.1544 - acc: 0.542 - ETA: 3s - loss: 1.1581 - acc: 0.537 - ETA: 3s - loss: 1.1608 - acc: 0.537 - ETA: 3s - loss: 1.1549 - acc: 0.538 - ETA: 3s - loss: 1.1665 - acc: 0.529 - ETA: 3s - loss: 1.1608 - acc: 0.529 - ETA: 2s - loss: 1.1584 - acc: 0.531 - ETA: 2s - loss: 1.1594 - acc: 0.536 - ETA: 2s - loss: 1.1648 - acc: 0.533 - ETA: 2s - loss: 1.1653 - acc: 0.533 - ETA: 2s - loss: 1.1743 - acc: 0.529 - ETA: 2s - loss: 1.1735 - acc: 0.527 - ETA: 2s - loss: 1.1661 - acc: 0.528 - ETA: 2s - loss: 1.1628 - acc: 0.530 - ETA: 1s - loss: 1.1644 - acc: 0.529 - ETA: 1s - loss: 1.1644 - acc: 0.527 - ETA: 1s - loss: 1.1716 - acc: 0.522 - ETA: 1s - loss: 1.1748 - acc: 0.520 - ETA: 1s - loss: 1.1692 - acc: 0.523 - ETA: 1s - loss: 1.1690 - acc: 0.522 - ETA: 1s - loss: 1.1657 - acc: 0.522 - ETA: 0s - loss: 1.1597 - acc: 0.527 - ETA: 0s - loss: 1.1567 - acc: 0.528 - ETA: 0s - loss: 1.1602 - acc: 0.523 - ETA: 0s - loss: 1.1556 - acc: 0.526 - ETA: 0s - loss: 1.1553 - acc: 0.527 - ETA: 0s - loss: 1.1536 - acc: 0.528 - ETA: 0s - loss: 1.1523 - acc: 0.528 - ETA: 0s - loss: 1.1468 - acc: 0.531 - 7s 5ms/step - loss: 1.1464 - acc: 0.5312 - val_loss: 1.2277 - val_acc: 0.4721\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.48011\n",
      "Epoch 31/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.2067 - acc: 0.500 - ETA: 6s - loss: 1.1677 - acc: 0.453 - ETA: 5s - loss: 1.1203 - acc: 0.500 - ETA: 5s - loss: 1.1227 - acc: 0.500 - ETA: 5s - loss: 1.1283 - acc: 0.487 - ETA: 5s - loss: 1.1463 - acc: 0.468 - ETA: 5s - loss: 1.1237 - acc: 0.495 - ETA: 5s - loss: 1.1564 - acc: 0.488 - ETA: 5s - loss: 1.1707 - acc: 0.496 - ETA: 4s - loss: 1.1735 - acc: 0.512 - ETA: 4s - loss: 1.1867 - acc: 0.508 - ETA: 4s - loss: 1.1790 - acc: 0.510 - ETA: 4s - loss: 1.1701 - acc: 0.514 - ETA: 4s - loss: 1.1840 - acc: 0.502 - ETA: 4s - loss: 1.1781 - acc: 0.504 - ETA: 4s - loss: 1.1719 - acc: 0.511 - ETA: 4s - loss: 1.1654 - acc: 0.516 - ETA: 3s - loss: 1.1619 - acc: 0.519 - ETA: 3s - loss: 1.1622 - acc: 0.523 - ETA: 3s - loss: 1.1644 - acc: 0.521 - ETA: 3s - loss: 1.1663 - acc: 0.523 - ETA: 3s - loss: 1.1671 - acc: 0.527 - ETA: 3s - loss: 1.1670 - acc: 0.523 - ETA: 3s - loss: 1.1744 - acc: 0.518 - ETA: 2s - loss: 1.1603 - acc: 0.526 - ETA: 2s - loss: 1.1617 - acc: 0.527 - ETA: 2s - loss: 1.1633 - acc: 0.525 - ETA: 2s - loss: 1.1683 - acc: 0.520 - ETA: 2s - loss: 1.1675 - acc: 0.518 - ETA: 2s - loss: 1.1633 - acc: 0.517 - ETA: 2s - loss: 1.1560 - acc: 0.523 - ETA: 2s - loss: 1.1506 - acc: 0.527 - ETA: 1s - loss: 1.1481 - acc: 0.528 - ETA: 1s - loss: 1.1478 - acc: 0.530 - ETA: 1s - loss: 1.1386 - acc: 0.533 - ETA: 1s - loss: 1.1369 - acc: 0.533 - ETA: 1s - loss: 1.1359 - acc: 0.538 - ETA: 1s - loss: 1.1393 - acc: 0.537 - ETA: 1s - loss: 1.1335 - acc: 0.539 - ETA: 0s - loss: 1.1317 - acc: 0.540 - ETA: 0s - loss: 1.1321 - acc: 0.540 - ETA: 0s - loss: 1.1352 - acc: 0.539 - ETA: 0s - loss: 1.1351 - acc: 0.541 - ETA: 0s - loss: 1.1349 - acc: 0.541 - ETA: 0s - loss: 1.1354 - acc: 0.538 - ETA: 0s - loss: 1.1342 - acc: 0.536 - ETA: 0s - loss: 1.1301 - acc: 0.537 - 7s 5ms/step - loss: 1.1297 - acc: 0.5385 - val_loss: 1.2215 - val_acc: 0.4589\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.48011\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9799 - acc: 0.593 - ETA: 5s - loss: 1.1003 - acc: 0.546 - ETA: 5s - loss: 1.0925 - acc: 0.541 - ETA: 5s - loss: 1.0995 - acc: 0.539 - ETA: 5s - loss: 1.0907 - acc: 0.543 - ETA: 5s - loss: 1.1161 - acc: 0.536 - ETA: 5s - loss: 1.1124 - acc: 0.553 - ETA: 5s - loss: 1.1326 - acc: 0.535 - ETA: 5s - loss: 1.1546 - acc: 0.520 - ETA: 4s - loss: 1.1375 - acc: 0.528 - ETA: 4s - loss: 1.1535 - acc: 0.517 - ETA: 4s - loss: 1.1474 - acc: 0.520 - ETA: 4s - loss: 1.1506 - acc: 0.521 - ETA: 4s - loss: 1.1391 - acc: 0.533 - ETA: 4s - loss: 1.1301 - acc: 0.541 - ETA: 4s - loss: 1.1343 - acc: 0.544 - ETA: 4s - loss: 1.1382 - acc: 0.551 - ETA: 3s - loss: 1.1289 - acc: 0.552 - ETA: 3s - loss: 1.1299 - acc: 0.554 - ETA: 3s - loss: 1.1285 - acc: 0.553 - ETA: 3s - loss: 1.1362 - acc: 0.547 - ETA: 3s - loss: 1.1276 - acc: 0.548 - ETA: 3s - loss: 1.1231 - acc: 0.550 - ETA: 3s - loss: 1.1204 - acc: 0.554 - ETA: 2s - loss: 1.1149 - acc: 0.557 - ETA: 2s - loss: 1.1069 - acc: 0.561 - ETA: 2s - loss: 1.1044 - acc: 0.562 - ETA: 2s - loss: 1.1128 - acc: 0.556 - ETA: 2s - loss: 1.1108 - acc: 0.558 - ETA: 2s - loss: 1.1099 - acc: 0.559 - ETA: 2s - loss: 1.1112 - acc: 0.558 - ETA: 2s - loss: 1.1111 - acc: 0.556 - ETA: 1s - loss: 1.1107 - acc: 0.556 - ETA: 1s - loss: 1.1091 - acc: 0.557 - ETA: 1s - loss: 1.1132 - acc: 0.555 - ETA: 1s - loss: 1.1154 - acc: 0.552 - ETA: 1s - loss: 1.1137 - acc: 0.550 - ETA: 1s - loss: 1.1164 - acc: 0.550 - ETA: 1s - loss: 1.1140 - acc: 0.550 - ETA: 0s - loss: 1.1080 - acc: 0.553 - ETA: 0s - loss: 1.1079 - acc: 0.552 - ETA: 0s - loss: 1.1099 - acc: 0.550 - ETA: 0s - loss: 1.1080 - acc: 0.553 - ETA: 0s - loss: 1.1106 - acc: 0.553 - ETA: 0s - loss: 1.1089 - acc: 0.553 - ETA: 0s - loss: 1.1088 - acc: 0.553 - ETA: 0s - loss: 1.1080 - acc: 0.553 - 7s 5ms/step - loss: 1.1074 - acc: 0.5537 - val_loss: 1.2221 - val_acc: 0.4854\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.48011 to 0.48541, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 33/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.1171 - acc: 0.500 - ETA: 5s - loss: 1.2550 - acc: 0.484 - ETA: 5s - loss: 1.1686 - acc: 0.520 - ETA: 5s - loss: 1.1914 - acc: 0.507 - ETA: 5s - loss: 1.1610 - acc: 0.512 - ETA: 5s - loss: 1.1337 - acc: 0.520 - ETA: 5s - loss: 1.1203 - acc: 0.526 - ETA: 5s - loss: 1.1320 - acc: 0.503 - ETA: 5s - loss: 1.1339 - acc: 0.510 - ETA: 4s - loss: 1.0998 - acc: 0.537 - ETA: 4s - loss: 1.0903 - acc: 0.545 - ETA: 4s - loss: 1.0816 - acc: 0.559 - ETA: 4s - loss: 1.0845 - acc: 0.552 - ETA: 4s - loss: 1.0812 - acc: 0.553 - ETA: 4s - loss: 1.0851 - acc: 0.550 - ETA: 4s - loss: 1.0806 - acc: 0.556 - ETA: 4s - loss: 1.0859 - acc: 0.557 - ETA: 3s - loss: 1.0819 - acc: 0.557 - ETA: 3s - loss: 1.0959 - acc: 0.551 - ETA: 3s - loss: 1.1022 - acc: 0.546 - ETA: 3s - loss: 1.1093 - acc: 0.541 - ETA: 3s - loss: 1.1103 - acc: 0.542 - ETA: 3s - loss: 1.1035 - acc: 0.548 - ETA: 3s - loss: 1.1021 - acc: 0.546 - ETA: 2s - loss: 1.0990 - acc: 0.547 - ETA: 2s - loss: 1.1037 - acc: 0.543 - ETA: 2s - loss: 1.1037 - acc: 0.542 - ETA: 2s - loss: 1.0941 - acc: 0.550 - ETA: 2s - loss: 1.0919 - acc: 0.550 - ETA: 2s - loss: 1.0983 - acc: 0.541 - ETA: 2s - loss: 1.1021 - acc: 0.541 - ETA: 2s - loss: 1.1058 - acc: 0.540 - ETA: 1s - loss: 1.1042 - acc: 0.540 - ETA: 1s - loss: 1.1026 - acc: 0.539 - ETA: 1s - loss: 1.1077 - acc: 0.538 - ETA: 1s - loss: 1.1059 - acc: 0.539 - ETA: 1s - loss: 1.1023 - acc: 0.541 - ETA: 1s - loss: 1.0996 - acc: 0.541 - ETA: 1s - loss: 1.1005 - acc: 0.542 - ETA: 0s - loss: 1.1008 - acc: 0.543 - ETA: 0s - loss: 1.1024 - acc: 0.541 - ETA: 0s - loss: 1.1001 - acc: 0.542 - ETA: 0s - loss: 1.1014 - acc: 0.542 - ETA: 0s - loss: 1.1007 - acc: 0.544 - ETA: 0s - loss: 1.1008 - acc: 0.545 - ETA: 0s - loss: 1.1015 - acc: 0.544 - ETA: 0s - loss: 1.1014 - acc: 0.544 - 7s 5ms/step - loss: 1.1014 - acc: 0.5444 - val_loss: 1.2349 - val_acc: 0.4907\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.48541 to 0.49072, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 34/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.1498 - acc: 0.531 - ETA: 5s - loss: 1.1018 - acc: 0.546 - ETA: 5s - loss: 1.0610 - acc: 0.531 - ETA: 5s - loss: 1.0268 - acc: 0.546 - ETA: 5s - loss: 1.0062 - acc: 0.550 - ETA: 5s - loss: 0.9930 - acc: 0.567 - ETA: 5s - loss: 1.0574 - acc: 0.544 - ETA: 5s - loss: 1.0671 - acc: 0.535 - ETA: 5s - loss: 1.1096 - acc: 0.520 - ETA: 5s - loss: 1.0976 - acc: 0.531 - ETA: 4s - loss: 1.1073 - acc: 0.531 - ETA: 4s - loss: 1.1110 - acc: 0.528 - ETA: 4s - loss: 1.0947 - acc: 0.531 - ETA: 4s - loss: 1.0995 - acc: 0.529 - ETA: 4s - loss: 1.1062 - acc: 0.535 - ETA: 4s - loss: 1.1176 - acc: 0.529 - ETA: 4s - loss: 1.1068 - acc: 0.531 - ETA: 3s - loss: 1.1085 - acc: 0.533 - ETA: 3s - loss: 1.0957 - acc: 0.542 - ETA: 3s - loss: 1.1037 - acc: 0.540 - ETA: 3s - loss: 1.0992 - acc: 0.546 - ETA: 3s - loss: 1.1044 - acc: 0.544 - ETA: 3s - loss: 1.1006 - acc: 0.546 - ETA: 3s - loss: 1.0987 - acc: 0.546 - ETA: 2s - loss: 1.1036 - acc: 0.541 - ETA: 2s - loss: 1.1006 - acc: 0.542 - ETA: 2s - loss: 1.0949 - acc: 0.541 - ETA: 2s - loss: 1.0878 - acc: 0.546 - ETA: 2s - loss: 1.0854 - acc: 0.549 - ETA: 2s - loss: 1.0812 - acc: 0.551 - ETA: 2s - loss: 1.0884 - acc: 0.547 - ETA: 2s - loss: 1.0901 - acc: 0.548 - ETA: 1s - loss: 1.0907 - acc: 0.552 - ETA: 1s - loss: 1.0904 - acc: 0.548 - ETA: 1s - loss: 1.0858 - acc: 0.550 - ETA: 1s - loss: 1.0891 - acc: 0.549 - ETA: 1s - loss: 1.0923 - acc: 0.548 - ETA: 1s - loss: 1.0852 - acc: 0.551 - ETA: 1s - loss: 1.0846 - acc: 0.554 - ETA: 0s - loss: 1.0874 - acc: 0.554 - ETA: 0s - loss: 1.0851 - acc: 0.553 - ETA: 0s - loss: 1.0808 - acc: 0.555 - ETA: 0s - loss: 1.0850 - acc: 0.554 - ETA: 0s - loss: 1.0864 - acc: 0.551 - ETA: 0s - loss: 1.0864 - acc: 0.550 - ETA: 0s - loss: 1.0861 - acc: 0.551 - ETA: 0s - loss: 1.0900 - acc: 0.547 - 7s 5ms/step - loss: 1.0910 - acc: 0.5471 - val_loss: 1.2530 - val_acc: 0.4907\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.49072 to 0.49072, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 35/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 0.8871 - acc: 0.687 - ETA: 5s - loss: 1.0165 - acc: 0.515 - ETA: 5s - loss: 1.0265 - acc: 0.541 - ETA: 5s - loss: 1.1329 - acc: 0.507 - ETA: 5s - loss: 1.1078 - acc: 0.512 - ETA: 5s - loss: 1.1004 - acc: 0.510 - ETA: 5s - loss: 1.1065 - acc: 0.513 - ETA: 5s - loss: 1.0792 - acc: 0.523 - ETA: 5s - loss: 1.1057 - acc: 0.510 - ETA: 4s - loss: 1.1098 - acc: 0.500 - ETA: 4s - loss: 1.1110 - acc: 0.508 - ETA: 4s - loss: 1.1302 - acc: 0.487 - ETA: 4s - loss: 1.1230 - acc: 0.500 - ETA: 4s - loss: 1.1304 - acc: 0.497 - ETA: 4s - loss: 1.1316 - acc: 0.502 - ETA: 4s - loss: 1.1330 - acc: 0.505 - ETA: 4s - loss: 1.1391 - acc: 0.503 - ETA: 3s - loss: 1.1499 - acc: 0.496 - ETA: 3s - loss: 1.1530 - acc: 0.495 - ETA: 3s - loss: 1.1501 - acc: 0.500 - ETA: 3s - loss: 1.1346 - acc: 0.508 - ETA: 3s - loss: 1.1162 - acc: 0.521 - ETA: 3s - loss: 1.1281 - acc: 0.520 - ETA: 3s - loss: 1.1235 - acc: 0.519 - ETA: 2s - loss: 1.1096 - acc: 0.527 - ETA: 2s - loss: 1.1140 - acc: 0.525 - ETA: 2s - loss: 1.1091 - acc: 0.528 - ETA: 2s - loss: 1.1022 - acc: 0.536 - ETA: 2s - loss: 1.0984 - acc: 0.537 - ETA: 2s - loss: 1.0951 - acc: 0.540 - ETA: 2s - loss: 1.0943 - acc: 0.543 - ETA: 2s - loss: 1.0906 - acc: 0.546 - ETA: 1s - loss: 1.0884 - acc: 0.546 - ETA: 1s - loss: 1.0845 - acc: 0.548 - ETA: 1s - loss: 1.0801 - acc: 0.550 - ETA: 1s - loss: 1.0806 - acc: 0.552 - ETA: 1s - loss: 1.0762 - acc: 0.553 - ETA: 1s - loss: 1.0743 - acc: 0.554 - ETA: 1s - loss: 1.0786 - acc: 0.549 - ETA: 0s - loss: 1.0822 - acc: 0.545 - ETA: 0s - loss: 1.0827 - acc: 0.542 - ETA: 0s - loss: 1.0822 - acc: 0.544 - ETA: 0s - loss: 1.0855 - acc: 0.543 - ETA: 0s - loss: 1.0838 - acc: 0.544 - ETA: 0s - loss: 1.0870 - acc: 0.541 - ETA: 0s - loss: 1.0859 - acc: 0.542 - ETA: 0s - loss: 1.0837 - acc: 0.542 - 7s 5ms/step - loss: 1.0829 - acc: 0.5438 - val_loss: 1.2189 - val_acc: 0.4775\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.49072\n",
      "Epoch 36/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9987 - acc: 0.750 - ETA: 6s - loss: 1.0403 - acc: 0.640 - ETA: 5s - loss: 1.0093 - acc: 0.625 - ETA: 5s - loss: 1.0524 - acc: 0.585 - ETA: 5s - loss: 1.0448 - acc: 0.587 - ETA: 5s - loss: 1.0364 - acc: 0.588 - ETA: 5s - loss: 1.0417 - acc: 0.580 - ETA: 5s - loss: 1.0617 - acc: 0.574 - ETA: 5s - loss: 1.0274 - acc: 0.590 - ETA: 4s - loss: 1.0238 - acc: 0.590 - ETA: 4s - loss: 1.0194 - acc: 0.593 - ETA: 4s - loss: 1.0169 - acc: 0.593 - ETA: 4s - loss: 1.0152 - acc: 0.593 - ETA: 4s - loss: 1.0086 - acc: 0.600 - ETA: 4s - loss: 1.0203 - acc: 0.597 - ETA: 4s - loss: 1.0262 - acc: 0.593 - ETA: 4s - loss: 1.0295 - acc: 0.595 - ETA: 3s - loss: 1.0264 - acc: 0.595 - ETA: 3s - loss: 1.0229 - acc: 0.593 - ETA: 3s - loss: 1.0208 - acc: 0.595 - ETA: 3s - loss: 1.0177 - acc: 0.593 - ETA: 3s - loss: 1.0236 - acc: 0.593 - ETA: 3s - loss: 1.0262 - acc: 0.591 - ETA: 3s - loss: 1.0173 - acc: 0.597 - ETA: 2s - loss: 1.0167 - acc: 0.597 - ETA: 2s - loss: 1.0180 - acc: 0.596 - ETA: 2s - loss: 1.0203 - acc: 0.598 - ETA: 2s - loss: 1.0272 - acc: 0.597 - ETA: 2s - loss: 1.0343 - acc: 0.593 - ETA: 2s - loss: 1.0374 - acc: 0.590 - ETA: 2s - loss: 1.0361 - acc: 0.589 - ETA: 2s - loss: 1.0378 - acc: 0.584 - ETA: 1s - loss: 1.0399 - acc: 0.583 - ETA: 1s - loss: 1.0414 - acc: 0.582 - ETA: 1s - loss: 1.0485 - acc: 0.580 - ETA: 1s - loss: 1.0568 - acc: 0.578 - ETA: 1s - loss: 1.0536 - acc: 0.580 - ETA: 1s - loss: 1.0557 - acc: 0.581 - ETA: 1s - loss: 1.0543 - acc: 0.582 - ETA: 0s - loss: 1.0570 - acc: 0.578 - ETA: 0s - loss: 1.0556 - acc: 0.581 - ETA: 0s - loss: 1.0566 - acc: 0.580 - ETA: 0s - loss: 1.0592 - acc: 0.578 - ETA: 0s - loss: 1.0611 - acc: 0.577 - ETA: 0s - loss: 1.0631 - acc: 0.575 - ETA: 0s - loss: 1.0711 - acc: 0.570 - ETA: 0s - loss: 1.0699 - acc: 0.570 - 7s 5ms/step - loss: 1.0697 - acc: 0.5703 - val_loss: 1.2060 - val_acc: 0.4668\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.49072\n",
      "Epoch 37/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0236 - acc: 0.500 - ETA: 5s - loss: 1.1288 - acc: 0.468 - ETA: 5s - loss: 1.1192 - acc: 0.500 - ETA: 5s - loss: 1.1332 - acc: 0.500 - ETA: 5s - loss: 1.0879 - acc: 0.543 - ETA: 5s - loss: 1.0532 - acc: 0.572 - ETA: 5s - loss: 1.0465 - acc: 0.571 - ETA: 5s - loss: 1.0577 - acc: 0.574 - ETA: 5s - loss: 1.0980 - acc: 0.559 - ETA: 5s - loss: 1.0892 - acc: 0.565 - ETA: 4s - loss: 1.0834 - acc: 0.568 - ETA: 4s - loss: 1.0822 - acc: 0.565 - ETA: 4s - loss: 1.0800 - acc: 0.562 - ETA: 4s - loss: 1.0681 - acc: 0.571 - ETA: 4s - loss: 1.0586 - acc: 0.572 - ETA: 4s - loss: 1.0634 - acc: 0.568 - ETA: 4s - loss: 1.0720 - acc: 0.568 - ETA: 3s - loss: 1.0900 - acc: 0.559 - ETA: 3s - loss: 1.0882 - acc: 0.564 - ETA: 3s - loss: 1.0825 - acc: 0.567 - ETA: 3s - loss: 1.0713 - acc: 0.574 - ETA: 3s - loss: 1.0736 - acc: 0.569 - ETA: 3s - loss: 1.0748 - acc: 0.569 - ETA: 3s - loss: 1.0794 - acc: 0.565 - ETA: 2s - loss: 1.0730 - acc: 0.568 - ETA: 2s - loss: 1.0727 - acc: 0.563 - ETA: 2s - loss: 1.0715 - acc: 0.564 - ETA: 2s - loss: 1.0699 - acc: 0.565 - ETA: 2s - loss: 1.0763 - acc: 0.561 - ETA: 2s - loss: 1.0777 - acc: 0.561 - ETA: 2s - loss: 1.0755 - acc: 0.561 - ETA: 2s - loss: 1.0696 - acc: 0.565 - ETA: 1s - loss: 1.0609 - acc: 0.568 - ETA: 1s - loss: 1.0584 - acc: 0.569 - ETA: 1s - loss: 1.0570 - acc: 0.570 - ETA: 1s - loss: 1.0534 - acc: 0.570 - ETA: 1s - loss: 1.0510 - acc: 0.570 - ETA: 1s - loss: 1.0505 - acc: 0.568 - ETA: 1s - loss: 1.0520 - acc: 0.570 - ETA: 0s - loss: 1.0564 - acc: 0.568 - ETA: 0s - loss: 1.0623 - acc: 0.564 - ETA: 0s - loss: 1.0686 - acc: 0.561 - ETA: 0s - loss: 1.0668 - acc: 0.562 - ETA: 0s - loss: 1.0724 - acc: 0.560 - ETA: 0s - loss: 1.0715 - acc: 0.561 - ETA: 0s - loss: 1.0700 - acc: 0.565 - ETA: 0s - loss: 1.0678 - acc: 0.564 - 7s 5ms/step - loss: 1.0658 - acc: 0.5656 - val_loss: 1.2144 - val_acc: 0.4828\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.49072\n",
      "Epoch 38/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0094 - acc: 0.562 - ETA: 6s - loss: 0.9888 - acc: 0.593 - ETA: 5s - loss: 0.9949 - acc: 0.583 - ETA: 5s - loss: 0.9880 - acc: 0.578 - ETA: 5s - loss: 0.9797 - acc: 0.593 - ETA: 5s - loss: 1.0051 - acc: 0.599 - ETA: 5s - loss: 0.9738 - acc: 0.620 - ETA: 5s - loss: 0.9828 - acc: 0.609 - ETA: 5s - loss: 0.9962 - acc: 0.604 - ETA: 5s - loss: 1.0064 - acc: 0.596 - ETA: 4s - loss: 1.0049 - acc: 0.596 - ETA: 4s - loss: 1.0231 - acc: 0.591 - ETA: 4s - loss: 1.0350 - acc: 0.588 - ETA: 4s - loss: 1.0482 - acc: 0.573 - ETA: 4s - loss: 1.0487 - acc: 0.568 - ETA: 4s - loss: 1.0577 - acc: 0.558 - ETA: 4s - loss: 1.0477 - acc: 0.560 - ETA: 3s - loss: 1.0623 - acc: 0.555 - ETA: 3s - loss: 1.0547 - acc: 0.559 - ETA: 3s - loss: 1.0480 - acc: 0.567 - ETA: 3s - loss: 1.0460 - acc: 0.565 - ETA: 3s - loss: 1.0492 - acc: 0.561 - ETA: 3s - loss: 1.0459 - acc: 0.566 - ETA: 3s - loss: 1.0415 - acc: 0.574 - ETA: 2s - loss: 1.0408 - acc: 0.577 - ETA: 2s - loss: 1.0458 - acc: 0.575 - ETA: 2s - loss: 1.0426 - acc: 0.576 - ETA: 2s - loss: 1.0433 - acc: 0.575 - ETA: 2s - loss: 1.0454 - acc: 0.574 - ETA: 2s - loss: 1.0450 - acc: 0.574 - ETA: 2s - loss: 1.0434 - acc: 0.571 - ETA: 2s - loss: 1.0406 - acc: 0.575 - ETA: 1s - loss: 1.0407 - acc: 0.573 - ETA: 1s - loss: 1.0417 - acc: 0.575 - ETA: 1s - loss: 1.0440 - acc: 0.574 - ETA: 1s - loss: 1.0485 - acc: 0.576 - ETA: 1s - loss: 1.0531 - acc: 0.574 - ETA: 1s - loss: 1.0570 - acc: 0.572 - ETA: 1s - loss: 1.0535 - acc: 0.573 - ETA: 0s - loss: 1.0485 - acc: 0.577 - ETA: 0s - loss: 1.0467 - acc: 0.578 - ETA: 0s - loss: 1.0447 - acc: 0.577 - ETA: 0s - loss: 1.0476 - acc: 0.573 - ETA: 0s - loss: 1.0465 - acc: 0.573 - ETA: 0s - loss: 1.0485 - acc: 0.570 - ETA: 0s - loss: 1.0473 - acc: 0.570 - ETA: 0s - loss: 1.0470 - acc: 0.571 - 7s 5ms/step - loss: 1.0469 - acc: 0.5710 - val_loss: 1.2178 - val_acc: 0.4748\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.49072\n",
      "Epoch 39/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0748 - acc: 0.593 - ETA: 5s - loss: 1.0224 - acc: 0.640 - ETA: 5s - loss: 0.9945 - acc: 0.656 - ETA: 5s - loss: 1.0817 - acc: 0.609 - ETA: 5s - loss: 1.0924 - acc: 0.587 - ETA: 5s - loss: 1.0962 - acc: 0.578 - ETA: 5s - loss: 1.0835 - acc: 0.571 - ETA: 5s - loss: 1.1187 - acc: 0.558 - ETA: 5s - loss: 1.1063 - acc: 0.566 - ETA: 4s - loss: 1.1031 - acc: 0.562 - ETA: 4s - loss: 1.0908 - acc: 0.568 - ETA: 4s - loss: 1.1157 - acc: 0.552 - ETA: 4s - loss: 1.0960 - acc: 0.557 - ETA: 4s - loss: 1.0977 - acc: 0.551 - ETA: 4s - loss: 1.0797 - acc: 0.558 - ETA: 4s - loss: 1.0695 - acc: 0.568 - ETA: 4s - loss: 1.0703 - acc: 0.568 - ETA: 3s - loss: 1.0658 - acc: 0.572 - ETA: 3s - loss: 1.0592 - acc: 0.574 - ETA: 3s - loss: 1.0587 - acc: 0.573 - ETA: 3s - loss: 1.0690 - acc: 0.565 - ETA: 3s - loss: 1.0714 - acc: 0.565 - ETA: 3s - loss: 1.0844 - acc: 0.557 - ETA: 3s - loss: 1.0781 - acc: 0.559 - ETA: 2s - loss: 1.0756 - acc: 0.565 - ETA: 2s - loss: 1.0755 - acc: 0.568 - ETA: 2s - loss: 1.0833 - acc: 0.564 - ETA: 2s - loss: 1.0735 - acc: 0.571 - ETA: 2s - loss: 1.0718 - acc: 0.572 - ETA: 2s - loss: 1.0746 - acc: 0.570 - ETA: 2s - loss: 1.0704 - acc: 0.570 - ETA: 2s - loss: 1.0694 - acc: 0.574 - ETA: 1s - loss: 1.0665 - acc: 0.575 - ETA: 1s - loss: 1.0676 - acc: 0.573 - ETA: 1s - loss: 1.0729 - acc: 0.573 - ETA: 1s - loss: 1.0813 - acc: 0.567 - ETA: 1s - loss: 1.0796 - acc: 0.570 - ETA: 1s - loss: 1.0776 - acc: 0.574 - ETA: 1s - loss: 1.0789 - acc: 0.574 - ETA: 0s - loss: 1.0735 - acc: 0.577 - ETA: 0s - loss: 1.0715 - acc: 0.577 - ETA: 0s - loss: 1.0704 - acc: 0.575 - ETA: 0s - loss: 1.0663 - acc: 0.577 - ETA: 0s - loss: 1.0626 - acc: 0.578 - ETA: 0s - loss: 1.0681 - acc: 0.575 - ETA: 0s - loss: 1.0707 - acc: 0.574 - ETA: 0s - loss: 1.0698 - acc: 0.575 - 7s 5ms/step - loss: 1.0696 - acc: 0.5763 - val_loss: 1.2104 - val_acc: 0.5332\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.49072 to 0.53316, saving model to C:\\Users\\ilezad\\models\\model_CNN450_bal=True.h5\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0540 - acc: 0.718 - ETA: 6s - loss: 1.0919 - acc: 0.578 - ETA: 5s - loss: 1.1026 - acc: 0.583 - ETA: 5s - loss: 1.0616 - acc: 0.609 - ETA: 5s - loss: 1.0216 - acc: 0.612 - ETA: 5s - loss: 1.0425 - acc: 0.583 - ETA: 5s - loss: 1.0424 - acc: 0.589 - ETA: 5s - loss: 1.0299 - acc: 0.574 - ETA: 5s - loss: 1.0198 - acc: 0.576 - ETA: 5s - loss: 1.0095 - acc: 0.581 - ETA: 4s - loss: 0.9868 - acc: 0.590 - ETA: 4s - loss: 0.9696 - acc: 0.601 - ETA: 4s - loss: 0.9696 - acc: 0.596 - ETA: 4s - loss: 0.9764 - acc: 0.596 - ETA: 4s - loss: 0.9911 - acc: 0.587 - ETA: 4s - loss: 0.9699 - acc: 0.599 - ETA: 4s - loss: 0.9852 - acc: 0.591 - ETA: 3s - loss: 0.9836 - acc: 0.599 - ETA: 3s - loss: 0.9886 - acc: 0.597 - ETA: 3s - loss: 0.9930 - acc: 0.600 - ETA: 3s - loss: 1.0132 - acc: 0.590 - ETA: 3s - loss: 1.0194 - acc: 0.588 - ETA: 3s - loss: 1.0247 - acc: 0.587 - ETA: 3s - loss: 1.0340 - acc: 0.580 - ETA: 2s - loss: 1.0329 - acc: 0.581 - ETA: 2s - loss: 1.0356 - acc: 0.580 - ETA: 2s - loss: 1.0419 - acc: 0.576 - ETA: 2s - loss: 1.0454 - acc: 0.569 - ETA: 2s - loss: 1.0486 - acc: 0.565 - ETA: 2s - loss: 1.0463 - acc: 0.564 - ETA: 2s - loss: 1.0440 - acc: 0.566 - ETA: 2s - loss: 1.0484 - acc: 0.565 - ETA: 1s - loss: 1.0479 - acc: 0.566 - ETA: 1s - loss: 1.0432 - acc: 0.569 - ETA: 1s - loss: 1.0402 - acc: 0.569 - ETA: 1s - loss: 1.0396 - acc: 0.572 - ETA: 1s - loss: 1.0345 - acc: 0.576 - ETA: 1s - loss: 1.0363 - acc: 0.574 - ETA: 1s - loss: 1.0383 - acc: 0.572 - ETA: 0s - loss: 1.0378 - acc: 0.572 - ETA: 0s - loss: 1.0432 - acc: 0.568 - ETA: 0s - loss: 1.0472 - acc: 0.567 - ETA: 0s - loss: 1.0428 - acc: 0.569 - ETA: 0s - loss: 1.0451 - acc: 0.567 - ETA: 0s - loss: 1.0449 - acc: 0.566 - ETA: 0s - loss: 1.0430 - acc: 0.565 - ETA: 0s - loss: 1.0439 - acc: 0.565 - 7s 5ms/step - loss: 1.0431 - acc: 0.5663 - val_loss: 1.2165 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.53316\n",
      "Epoch 41/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0634 - acc: 0.562 - ETA: 6s - loss: 1.2292 - acc: 0.500 - ETA: 5s - loss: 1.1277 - acc: 0.531 - ETA: 5s - loss: 1.1312 - acc: 0.539 - ETA: 5s - loss: 1.0978 - acc: 0.556 - ETA: 5s - loss: 1.1036 - acc: 0.552 - ETA: 5s - loss: 1.1103 - acc: 0.544 - ETA: 5s - loss: 1.1018 - acc: 0.550 - ETA: 5s - loss: 1.0929 - acc: 0.555 - ETA: 5s - loss: 1.0671 - acc: 0.575 - ETA: 4s - loss: 1.0725 - acc: 0.559 - ETA: 4s - loss: 1.0559 - acc: 0.565 - ETA: 4s - loss: 1.0455 - acc: 0.567 - ETA: 4s - loss: 1.0526 - acc: 0.564 - ETA: 4s - loss: 1.0640 - acc: 0.564 - ETA: 4s - loss: 1.0702 - acc: 0.564 - ETA: 4s - loss: 1.0557 - acc: 0.569 - ETA: 3s - loss: 1.0420 - acc: 0.579 - ETA: 3s - loss: 1.0368 - acc: 0.583 - ETA: 3s - loss: 1.0362 - acc: 0.582 - ETA: 3s - loss: 1.0442 - acc: 0.584 - ETA: 3s - loss: 1.0266 - acc: 0.593 - ETA: 3s - loss: 1.0188 - acc: 0.597 - ETA: 3s - loss: 1.0137 - acc: 0.596 - ETA: 2s - loss: 1.0147 - acc: 0.595 - ETA: 2s - loss: 1.0195 - acc: 0.596 - ETA: 2s - loss: 1.0139 - acc: 0.598 - ETA: 2s - loss: 1.0108 - acc: 0.598 - ETA: 2s - loss: 1.0063 - acc: 0.603 - ETA: 2s - loss: 1.0059 - acc: 0.601 - ETA: 2s - loss: 1.0032 - acc: 0.601 - ETA: 2s - loss: 1.0131 - acc: 0.594 - ETA: 1s - loss: 1.0248 - acc: 0.589 - ETA: 1s - loss: 1.0218 - acc: 0.590 - ETA: 1s - loss: 1.0203 - acc: 0.591 - ETA: 1s - loss: 1.0230 - acc: 0.589 - ETA: 1s - loss: 1.0328 - acc: 0.583 - ETA: 1s - loss: 1.0283 - acc: 0.585 - ETA: 1s - loss: 1.0272 - acc: 0.585 - ETA: 0s - loss: 1.0221 - acc: 0.585 - ETA: 0s - loss: 1.0181 - acc: 0.587 - ETA: 0s - loss: 1.0158 - acc: 0.587 - ETA: 0s - loss: 1.0139 - acc: 0.587 - ETA: 0s - loss: 1.0129 - acc: 0.587 - ETA: 0s - loss: 1.0148 - acc: 0.588 - ETA: 0s - loss: 1.0191 - acc: 0.582 - ETA: 0s - loss: 1.0164 - acc: 0.582 - 7s 5ms/step - loss: 1.0156 - acc: 0.5829 - val_loss: 1.2250 - val_acc: 0.5172\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.53316\n",
      "Epoch 42/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.1139 - acc: 0.531 - ETA: 6s - loss: 1.1433 - acc: 0.500 - ETA: 5s - loss: 1.0401 - acc: 0.552 - ETA: 5s - loss: 1.0974 - acc: 0.539 - ETA: 5s - loss: 1.0982 - acc: 0.556 - ETA: 5s - loss: 1.0890 - acc: 0.562 - ETA: 5s - loss: 1.0470 - acc: 0.580 - ETA: 5s - loss: 1.0503 - acc: 0.589 - ETA: 5s - loss: 1.0824 - acc: 0.572 - ETA: 4s - loss: 1.0950 - acc: 0.562 - ETA: 4s - loss: 1.0610 - acc: 0.576 - ETA: 4s - loss: 1.0603 - acc: 0.580 - ETA: 4s - loss: 1.0605 - acc: 0.572 - ETA: 4s - loss: 1.0407 - acc: 0.580 - ETA: 4s - loss: 1.0482 - acc: 0.579 - ETA: 4s - loss: 1.0359 - acc: 0.585 - ETA: 4s - loss: 1.0340 - acc: 0.579 - ETA: 3s - loss: 1.0300 - acc: 0.576 - ETA: 3s - loss: 1.0333 - acc: 0.570 - ETA: 3s - loss: 1.0239 - acc: 0.575 - ETA: 3s - loss: 1.0186 - acc: 0.575 - ETA: 3s - loss: 1.0198 - acc: 0.575 - ETA: 3s - loss: 1.0244 - acc: 0.576 - ETA: 3s - loss: 1.0318 - acc: 0.575 - ETA: 2s - loss: 1.0228 - acc: 0.581 - ETA: 2s - loss: 1.0115 - acc: 0.584 - ETA: 2s - loss: 1.0205 - acc: 0.581 - ETA: 2s - loss: 1.0108 - acc: 0.584 - ETA: 2s - loss: 1.0194 - acc: 0.580 - ETA: 2s - loss: 1.0185 - acc: 0.581 - ETA: 2s - loss: 1.0216 - acc: 0.575 - ETA: 2s - loss: 1.0245 - acc: 0.572 - ETA: 1s - loss: 1.0169 - acc: 0.577 - ETA: 1s - loss: 1.0201 - acc: 0.579 - ETA: 1s - loss: 1.0179 - acc: 0.580 - ETA: 1s - loss: 1.0183 - acc: 0.580 - ETA: 1s - loss: 1.0169 - acc: 0.581 - ETA: 1s - loss: 1.0132 - acc: 0.586 - ETA: 1s - loss: 1.0145 - acc: 0.585 - ETA: 0s - loss: 1.0127 - acc: 0.587 - ETA: 0s - loss: 1.0168 - acc: 0.584 - ETA: 0s - loss: 1.0184 - acc: 0.582 - ETA: 0s - loss: 1.0175 - acc: 0.581 - ETA: 0s - loss: 1.0194 - acc: 0.582 - ETA: 0s - loss: 1.0191 - acc: 0.578 - ETA: 0s - loss: 1.0157 - acc: 0.579 - ETA: 0s - loss: 1.0174 - acc: 0.579 - 7s 5ms/step - loss: 1.0185 - acc: 0.5796 - val_loss: 1.2249 - val_acc: 0.4960\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.53316\n",
      "Epoch 43/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9327 - acc: 0.656 - ETA: 6s - loss: 1.0380 - acc: 0.531 - ETA: 5s - loss: 0.9751 - acc: 0.572 - ETA: 5s - loss: 0.9457 - acc: 0.601 - ETA: 5s - loss: 1.0122 - acc: 0.550 - ETA: 5s - loss: 1.0198 - acc: 0.572 - ETA: 5s - loss: 1.0482 - acc: 0.571 - ETA: 5s - loss: 1.0193 - acc: 0.578 - ETA: 5s - loss: 1.0245 - acc: 0.576 - ETA: 5s - loss: 1.0048 - acc: 0.587 - ETA: 4s - loss: 0.9973 - acc: 0.579 - ETA: 4s - loss: 0.9889 - acc: 0.583 - ETA: 4s - loss: 0.9969 - acc: 0.576 - ETA: 4s - loss: 0.9816 - acc: 0.575 - ETA: 4s - loss: 1.0030 - acc: 0.564 - ETA: 4s - loss: 0.9993 - acc: 0.570 - ETA: 4s - loss: 0.9887 - acc: 0.577 - ETA: 3s - loss: 0.9823 - acc: 0.583 - ETA: 3s - loss: 0.9772 - acc: 0.582 - ETA: 3s - loss: 0.9802 - acc: 0.584 - ETA: 3s - loss: 0.9655 - acc: 0.593 - ETA: 3s - loss: 0.9651 - acc: 0.590 - ETA: 3s - loss: 0.9689 - acc: 0.591 - ETA: 3s - loss: 0.9671 - acc: 0.591 - ETA: 3s - loss: 0.9667 - acc: 0.592 - ETA: 2s - loss: 0.9663 - acc: 0.592 - ETA: 2s - loss: 0.9732 - acc: 0.591 - ETA: 2s - loss: 0.9844 - acc: 0.585 - ETA: 2s - loss: 0.9959 - acc: 0.580 - ETA: 2s - loss: 0.9948 - acc: 0.581 - ETA: 2s - loss: 0.9995 - acc: 0.583 - ETA: 2s - loss: 1.0017 - acc: 0.581 - ETA: 1s - loss: 1.0063 - acc: 0.579 - ETA: 1s - loss: 1.0110 - acc: 0.580 - ETA: 1s - loss: 1.0120 - acc: 0.581 - ETA: 1s - loss: 1.0051 - acc: 0.582 - ETA: 1s - loss: 1.0053 - acc: 0.581 - ETA: 1s - loss: 1.0062 - acc: 0.581 - ETA: 1s - loss: 1.0055 - acc: 0.580 - ETA: 0s - loss: 1.0093 - acc: 0.580 - ETA: 0s - loss: 1.0120 - acc: 0.579 - ETA: 0s - loss: 1.0171 - acc: 0.576 - ETA: 0s - loss: 1.0206 - acc: 0.576 - ETA: 0s - loss: 1.0282 - acc: 0.571 - ETA: 0s - loss: 1.0314 - acc: 0.569 - ETA: 0s - loss: 1.0260 - acc: 0.572 - ETA: 0s - loss: 1.0270 - acc: 0.571 - 7s 5ms/step - loss: 1.0269 - acc: 0.5723 - val_loss: 1.2038 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.53316\n",
      "Epoch 44/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9395 - acc: 0.562 - ETA: 5s - loss: 1.0280 - acc: 0.531 - ETA: 5s - loss: 1.0075 - acc: 0.562 - ETA: 5s - loss: 1.0219 - acc: 0.554 - ETA: 5s - loss: 0.9793 - acc: 0.606 - ETA: 5s - loss: 0.9696 - acc: 0.599 - ETA: 5s - loss: 0.9741 - acc: 0.589 - ETA: 5s - loss: 0.9737 - acc: 0.597 - ETA: 5s - loss: 0.9613 - acc: 0.614 - ETA: 4s - loss: 0.9476 - acc: 0.621 - ETA: 4s - loss: 0.9491 - acc: 0.625 - ETA: 4s - loss: 0.9606 - acc: 0.619 - ETA: 4s - loss: 0.9618 - acc: 0.622 - ETA: 4s - loss: 0.9709 - acc: 0.609 - ETA: 4s - loss: 0.9779 - acc: 0.604 - ETA: 4s - loss: 0.9797 - acc: 0.599 - ETA: 4s - loss: 0.9835 - acc: 0.599 - ETA: 3s - loss: 0.9976 - acc: 0.588 - ETA: 3s - loss: 1.0068 - acc: 0.585 - ETA: 3s - loss: 1.0099 - acc: 0.587 - ETA: 3s - loss: 1.0021 - acc: 0.590 - ETA: 3s - loss: 1.0016 - acc: 0.590 - ETA: 3s - loss: 0.9991 - acc: 0.589 - ETA: 3s - loss: 1.0022 - acc: 0.591 - ETA: 2s - loss: 0.9952 - acc: 0.597 - ETA: 2s - loss: 0.9881 - acc: 0.599 - ETA: 2s - loss: 0.9896 - acc: 0.600 - ETA: 2s - loss: 0.9958 - acc: 0.598 - ETA: 2s - loss: 1.0061 - acc: 0.591 - ETA: 2s - loss: 1.0046 - acc: 0.587 - ETA: 2s - loss: 1.0053 - acc: 0.587 - ETA: 2s - loss: 1.0042 - acc: 0.585 - ETA: 1s - loss: 1.0006 - acc: 0.586 - ETA: 1s - loss: 1.0022 - acc: 0.585 - ETA: 1s - loss: 1.0077 - acc: 0.582 - ETA: 1s - loss: 1.0084 - acc: 0.577 - ETA: 1s - loss: 1.0114 - acc: 0.576 - ETA: 1s - loss: 1.0176 - acc: 0.572 - ETA: 1s - loss: 1.0143 - acc: 0.575 - ETA: 0s - loss: 1.0112 - acc: 0.578 - ETA: 0s - loss: 1.0085 - acc: 0.580 - ETA: 0s - loss: 1.0041 - acc: 0.583 - ETA: 0s - loss: 1.0037 - acc: 0.582 - ETA: 0s - loss: 1.0017 - acc: 0.583 - ETA: 0s - loss: 1.0081 - acc: 0.581 - ETA: 0s - loss: 1.0102 - acc: 0.580 - ETA: 0s - loss: 1.0068 - acc: 0.583 - 7s 5ms/step - loss: 1.0071 - acc: 0.5829 - val_loss: 1.2609 - val_acc: 0.4828\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.53316\n",
      "Epoch 45/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9014 - acc: 0.625 - ETA: 6s - loss: 0.9760 - acc: 0.625 - ETA: 5s - loss: 0.9870 - acc: 0.635 - ETA: 5s - loss: 0.9647 - acc: 0.617 - ETA: 5s - loss: 0.9460 - acc: 0.625 - ETA: 5s - loss: 0.9483 - acc: 0.630 - ETA: 5s - loss: 0.9277 - acc: 0.638 - ETA: 5s - loss: 0.9370 - acc: 0.636 - ETA: 5s - loss: 0.9566 - acc: 0.638 - ETA: 4s - loss: 0.9641 - acc: 0.628 - ETA: 4s - loss: 0.9745 - acc: 0.622 - ETA: 4s - loss: 0.9809 - acc: 0.609 - ETA: 4s - loss: 0.9939 - acc: 0.601 - ETA: 4s - loss: 0.9959 - acc: 0.598 - ETA: 4s - loss: 0.9810 - acc: 0.597 - ETA: 4s - loss: 0.9877 - acc: 0.589 - ETA: 4s - loss: 0.9853 - acc: 0.593 - ETA: 3s - loss: 0.9824 - acc: 0.595 - ETA: 3s - loss: 0.9834 - acc: 0.592 - ETA: 3s - loss: 0.9805 - acc: 0.595 - ETA: 3s - loss: 0.9922 - acc: 0.583 - ETA: 3s - loss: 0.9856 - acc: 0.585 - ETA: 3s - loss: 0.9893 - acc: 0.584 - ETA: 3s - loss: 0.9873 - acc: 0.583 - ETA: 2s - loss: 0.9856 - acc: 0.583 - ETA: 2s - loss: 0.9916 - acc: 0.580 - ETA: 2s - loss: 0.9885 - acc: 0.584 - ETA: 2s - loss: 0.9929 - acc: 0.581 - ETA: 2s - loss: 0.9917 - acc: 0.580 - ETA: 2s - loss: 0.9906 - acc: 0.579 - ETA: 2s - loss: 1.0039 - acc: 0.576 - ETA: 2s - loss: 1.0041 - acc: 0.573 - ETA: 1s - loss: 0.9984 - acc: 0.579 - ETA: 1s - loss: 0.9957 - acc: 0.583 - ETA: 1s - loss: 1.0090 - acc: 0.577 - ETA: 1s - loss: 1.0098 - acc: 0.577 - ETA: 1s - loss: 1.0077 - acc: 0.578 - ETA: 1s - loss: 1.0046 - acc: 0.579 - ETA: 1s - loss: 1.0065 - acc: 0.577 - ETA: 0s - loss: 1.0046 - acc: 0.577 - ETA: 0s - loss: 1.0018 - acc: 0.580 - ETA: 0s - loss: 1.0044 - acc: 0.578 - ETA: 0s - loss: 1.0025 - acc: 0.580 - ETA: 0s - loss: 1.0025 - acc: 0.580 - ETA: 0s - loss: 1.0057 - acc: 0.581 - ETA: 0s - loss: 1.0064 - acc: 0.582 - ETA: 0s - loss: 1.0051 - acc: 0.583 - 7s 5ms/step - loss: 1.0042 - acc: 0.5836 - val_loss: 1.2138 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.53316\n",
      "Epoch 46/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.2236 - acc: 0.593 - ETA: 6s - loss: 1.0594 - acc: 0.625 - ETA: 5s - loss: 1.0290 - acc: 0.614 - ETA: 5s - loss: 1.0151 - acc: 0.640 - ETA: 5s - loss: 1.0265 - acc: 0.637 - ETA: 5s - loss: 1.0048 - acc: 0.630 - ETA: 5s - loss: 0.9907 - acc: 0.625 - ETA: 5s - loss: 0.9863 - acc: 0.613 - ETA: 5s - loss: 0.9837 - acc: 0.600 - ETA: 5s - loss: 0.9872 - acc: 0.596 - ETA: 4s - loss: 0.9945 - acc: 0.593 - ETA: 4s - loss: 1.0007 - acc: 0.596 - ETA: 4s - loss: 0.9909 - acc: 0.596 - ETA: 4s - loss: 0.9884 - acc: 0.593 - ETA: 4s - loss: 0.9763 - acc: 0.597 - ETA: 4s - loss: 0.9678 - acc: 0.605 - ETA: 4s - loss: 0.9694 - acc: 0.601 - ETA: 3s - loss: 0.9834 - acc: 0.595 - ETA: 3s - loss: 0.9862 - acc: 0.595 - ETA: 3s - loss: 0.9904 - acc: 0.590 - ETA: 3s - loss: 0.9970 - acc: 0.589 - ETA: 3s - loss: 1.0051 - acc: 0.588 - ETA: 3s - loss: 0.9963 - acc: 0.589 - ETA: 3s - loss: 0.9994 - acc: 0.588 - ETA: 2s - loss: 1.0017 - acc: 0.588 - ETA: 2s - loss: 1.0047 - acc: 0.587 - ETA: 2s - loss: 1.0021 - acc: 0.586 - ETA: 2s - loss: 1.0081 - acc: 0.587 - ETA: 2s - loss: 1.0104 - acc: 0.588 - ETA: 2s - loss: 1.0058 - acc: 0.592 - ETA: 2s - loss: 1.0039 - acc: 0.593 - ETA: 2s - loss: 1.0050 - acc: 0.593 - ETA: 1s - loss: 1.0004 - acc: 0.594 - ETA: 1s - loss: 0.9955 - acc: 0.598 - ETA: 1s - loss: 0.9927 - acc: 0.599 - ETA: 1s - loss: 0.9958 - acc: 0.599 - ETA: 1s - loss: 0.9957 - acc: 0.598 - ETA: 1s - loss: 0.9948 - acc: 0.601 - ETA: 1s - loss: 0.9936 - acc: 0.600 - ETA: 0s - loss: 0.9897 - acc: 0.603 - ETA: 0s - loss: 0.9911 - acc: 0.605 - ETA: 0s - loss: 0.9881 - acc: 0.608 - ETA: 0s - loss: 0.9867 - acc: 0.607 - ETA: 0s - loss: 0.9907 - acc: 0.605 - ETA: 0s - loss: 1.0009 - acc: 0.601 - ETA: 0s - loss: 0.9957 - acc: 0.603 - ETA: 0s - loss: 0.9931 - acc: 0.603 - 7s 5ms/step - loss: 0.9923 - acc: 0.6041 - val_loss: 1.2189 - val_acc: 0.4960\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.53316\n",
      "Epoch 47/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8524 - acc: 0.718 - ETA: 6s - loss: 0.8748 - acc: 0.734 - ETA: 6s - loss: 0.9675 - acc: 0.645 - ETA: 5s - loss: 0.9417 - acc: 0.664 - ETA: 5s - loss: 0.9517 - acc: 0.650 - ETA: 5s - loss: 0.9216 - acc: 0.661 - ETA: 5s - loss: 0.9495 - acc: 0.625 - ETA: 5s - loss: 0.9614 - acc: 0.617 - ETA: 5s - loss: 0.9730 - acc: 0.607 - ETA: 5s - loss: 0.9815 - acc: 0.603 - ETA: 4s - loss: 0.9827 - acc: 0.602 - ETA: 4s - loss: 0.9843 - acc: 0.596 - ETA: 4s - loss: 0.9825 - acc: 0.598 - ETA: 4s - loss: 0.9771 - acc: 0.600 - ETA: 4s - loss: 0.9749 - acc: 0.600 - ETA: 4s - loss: 0.9697 - acc: 0.609 - ETA: 4s - loss: 0.9771 - acc: 0.606 - ETA: 3s - loss: 0.9755 - acc: 0.605 - ETA: 3s - loss: 0.9853 - acc: 0.603 - ETA: 3s - loss: 0.9866 - acc: 0.604 - ETA: 3s - loss: 0.9863 - acc: 0.608 - ETA: 3s - loss: 0.9819 - acc: 0.608 - ETA: 3s - loss: 0.9838 - acc: 0.607 - ETA: 3s - loss: 0.9809 - acc: 0.608 - ETA: 2s - loss: 0.9793 - acc: 0.608 - ETA: 2s - loss: 0.9784 - acc: 0.609 - ETA: 2s - loss: 0.9768 - acc: 0.607 - ETA: 2s - loss: 0.9748 - acc: 0.604 - ETA: 2s - loss: 0.9757 - acc: 0.603 - ETA: 2s - loss: 0.9728 - acc: 0.605 - ETA: 2s - loss: 0.9753 - acc: 0.607 - ETA: 2s - loss: 0.9761 - acc: 0.609 - ETA: 1s - loss: 0.9743 - acc: 0.609 - ETA: 1s - loss: 0.9747 - acc: 0.613 - ETA: 1s - loss: 0.9723 - acc: 0.610 - ETA: 1s - loss: 0.9739 - acc: 0.610 - ETA: 1s - loss: 0.9671 - acc: 0.612 - ETA: 1s - loss: 0.9717 - acc: 0.609 - ETA: 1s - loss: 0.9763 - acc: 0.605 - ETA: 0s - loss: 0.9775 - acc: 0.607 - ETA: 0s - loss: 0.9767 - acc: 0.605 - ETA: 0s - loss: 0.9740 - acc: 0.608 - ETA: 0s - loss: 0.9775 - acc: 0.607 - ETA: 0s - loss: 0.9754 - acc: 0.608 - ETA: 0s - loss: 0.9780 - acc: 0.606 - ETA: 0s - loss: 0.9799 - acc: 0.606 - ETA: 0s - loss: 0.9791 - acc: 0.607 - 7s 5ms/step - loss: 0.9788 - acc: 0.6068 - val_loss: 1.2099 - val_acc: 0.4934\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.53316\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8689 - acc: 0.718 - ETA: 5s - loss: 0.8788 - acc: 0.703 - ETA: 5s - loss: 0.8576 - acc: 0.697 - ETA: 5s - loss: 0.8774 - acc: 0.687 - ETA: 5s - loss: 0.8697 - acc: 0.693 - ETA: 5s - loss: 0.9077 - acc: 0.666 - ETA: 5s - loss: 0.9168 - acc: 0.647 - ETA: 5s - loss: 0.9047 - acc: 0.660 - ETA: 5s - loss: 0.9102 - acc: 0.652 - ETA: 4s - loss: 0.8978 - acc: 0.659 - ETA: 4s - loss: 0.9144 - acc: 0.647 - ETA: 4s - loss: 0.9309 - acc: 0.632 - ETA: 4s - loss: 0.9378 - acc: 0.625 - ETA: 4s - loss: 0.9357 - acc: 0.622 - ETA: 4s - loss: 0.9247 - acc: 0.631 - ETA: 4s - loss: 0.9155 - acc: 0.630 - ETA: 4s - loss: 0.9115 - acc: 0.628 - ETA: 3s - loss: 0.9200 - acc: 0.625 - ETA: 3s - loss: 0.9224 - acc: 0.626 - ETA: 3s - loss: 0.9306 - acc: 0.623 - ETA: 3s - loss: 0.9366 - acc: 0.620 - ETA: 3s - loss: 0.9527 - acc: 0.616 - ETA: 3s - loss: 0.9494 - acc: 0.620 - ETA: 3s - loss: 0.9364 - acc: 0.628 - ETA: 2s - loss: 0.9342 - acc: 0.627 - ETA: 2s - loss: 0.9327 - acc: 0.625 - ETA: 2s - loss: 0.9454 - acc: 0.619 - ETA: 2s - loss: 0.9408 - acc: 0.620 - ETA: 2s - loss: 0.9464 - acc: 0.622 - ETA: 2s - loss: 0.9560 - acc: 0.616 - ETA: 2s - loss: 0.9560 - acc: 0.616 - ETA: 2s - loss: 0.9585 - acc: 0.617 - ETA: 1s - loss: 0.9593 - acc: 0.618 - ETA: 1s - loss: 0.9619 - acc: 0.619 - ETA: 1s - loss: 0.9666 - acc: 0.617 - ETA: 1s - loss: 0.9705 - acc: 0.616 - ETA: 1s - loss: 0.9650 - acc: 0.619 - ETA: 1s - loss: 0.9626 - acc: 0.620 - ETA: 1s - loss: 0.9647 - acc: 0.620 - ETA: 0s - loss: 0.9657 - acc: 0.621 - ETA: 0s - loss: 0.9669 - acc: 0.618 - ETA: 0s - loss: 0.9690 - acc: 0.617 - ETA: 0s - loss: 0.9707 - acc: 0.616 - ETA: 0s - loss: 0.9706 - acc: 0.619 - ETA: 0s - loss: 0.9758 - acc: 0.617 - ETA: 0s - loss: 0.9766 - acc: 0.616 - ETA: 0s - loss: 0.9765 - acc: 0.613 - 7s 5ms/step - loss: 0.9772 - acc: 0.6134 - val_loss: 1.2294 - val_acc: 0.4748\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.53316\n",
      "Epoch 49/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9830 - acc: 0.531 - ETA: 6s - loss: 0.9869 - acc: 0.531 - ETA: 6s - loss: 0.9715 - acc: 0.583 - ETA: 5s - loss: 0.9524 - acc: 0.585 - ETA: 5s - loss: 0.9392 - acc: 0.618 - ETA: 5s - loss: 0.9272 - acc: 0.630 - ETA: 5s - loss: 0.9271 - acc: 0.638 - ETA: 5s - loss: 0.9645 - acc: 0.621 - ETA: 5s - loss: 0.9759 - acc: 0.607 - ETA: 5s - loss: 0.9808 - acc: 0.593 - ETA: 4s - loss: 0.9779 - acc: 0.602 - ETA: 4s - loss: 0.9688 - acc: 0.609 - ETA: 4s - loss: 0.9567 - acc: 0.617 - ETA: 4s - loss: 0.9525 - acc: 0.618 - ETA: 4s - loss: 0.9629 - acc: 0.608 - ETA: 4s - loss: 0.9631 - acc: 0.603 - ETA: 4s - loss: 0.9538 - acc: 0.606 - ETA: 3s - loss: 0.9493 - acc: 0.607 - ETA: 3s - loss: 0.9425 - acc: 0.615 - ETA: 3s - loss: 0.9382 - acc: 0.618 - ETA: 3s - loss: 0.9538 - acc: 0.613 - ETA: 3s - loss: 0.9517 - acc: 0.615 - ETA: 3s - loss: 0.9530 - acc: 0.616 - ETA: 3s - loss: 0.9420 - acc: 0.622 - ETA: 3s - loss: 0.9375 - acc: 0.630 - ETA: 2s - loss: 0.9413 - acc: 0.626 - ETA: 2s - loss: 0.9399 - acc: 0.626 - ETA: 2s - loss: 0.9356 - acc: 0.630 - ETA: 2s - loss: 0.9349 - acc: 0.631 - ETA: 2s - loss: 0.9380 - acc: 0.631 - ETA: 2s - loss: 0.9373 - acc: 0.629 - ETA: 2s - loss: 0.9427 - acc: 0.627 - ETA: 1s - loss: 0.9476 - acc: 0.623 - ETA: 1s - loss: 0.9471 - acc: 0.621 - ETA: 1s - loss: 0.9553 - acc: 0.617 - ETA: 1s - loss: 0.9533 - acc: 0.620 - ETA: 1s - loss: 0.9511 - acc: 0.621 - ETA: 1s - loss: 0.9505 - acc: 0.621 - ETA: 1s - loss: 0.9539 - acc: 0.618 - ETA: 0s - loss: 0.9521 - acc: 0.620 - ETA: 0s - loss: 0.9556 - acc: 0.619 - ETA: 0s - loss: 0.9515 - acc: 0.620 - ETA: 0s - loss: 0.9517 - acc: 0.619 - ETA: 0s - loss: 0.9560 - acc: 0.618 - ETA: 0s - loss: 0.9568 - acc: 0.620 - ETA: 0s - loss: 0.9536 - acc: 0.623 - ETA: 0s - loss: 0.9543 - acc: 0.623 - 7s 5ms/step - loss: 0.9546 - acc: 0.6227 - val_loss: 1.2172 - val_acc: 0.4960\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.53316\n",
      "Epoch 50/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9566 - acc: 0.593 - ETA: 6s - loss: 0.9807 - acc: 0.578 - ETA: 5s - loss: 0.9109 - acc: 0.645 - ETA: 5s - loss: 0.9290 - acc: 0.656 - ETA: 5s - loss: 0.9868 - acc: 0.631 - ETA: 5s - loss: 0.9623 - acc: 0.630 - ETA: 5s - loss: 0.9304 - acc: 0.651 - ETA: 5s - loss: 0.9134 - acc: 0.652 - ETA: 5s - loss: 0.9152 - acc: 0.656 - ETA: 4s - loss: 0.9245 - acc: 0.637 - ETA: 4s - loss: 0.9335 - acc: 0.633 - ETA: 4s - loss: 0.9188 - acc: 0.632 - ETA: 4s - loss: 0.9315 - acc: 0.627 - ETA: 4s - loss: 0.9254 - acc: 0.629 - ETA: 4s - loss: 0.9245 - acc: 0.625 - ETA: 4s - loss: 0.9321 - acc: 0.619 - ETA: 4s - loss: 0.9223 - acc: 0.625 - ETA: 3s - loss: 0.9168 - acc: 0.631 - ETA: 3s - loss: 0.9095 - acc: 0.636 - ETA: 3s - loss: 0.9099 - acc: 0.642 - ETA: 3s - loss: 0.9175 - acc: 0.636 - ETA: 3s - loss: 0.9174 - acc: 0.637 - ETA: 3s - loss: 0.9207 - acc: 0.637 - ETA: 3s - loss: 0.9239 - acc: 0.635 - ETA: 2s - loss: 0.9122 - acc: 0.643 - ETA: 2s - loss: 0.9189 - acc: 0.638 - ETA: 2s - loss: 0.9210 - acc: 0.633 - ETA: 2s - loss: 0.9197 - acc: 0.632 - ETA: 2s - loss: 0.9162 - acc: 0.632 - ETA: 2s - loss: 0.9272 - acc: 0.629 - ETA: 2s - loss: 0.9196 - acc: 0.633 - ETA: 2s - loss: 0.9171 - acc: 0.635 - ETA: 1s - loss: 0.9142 - acc: 0.637 - ETA: 1s - loss: 0.9166 - acc: 0.635 - ETA: 1s - loss: 0.9083 - acc: 0.639 - ETA: 1s - loss: 0.9107 - acc: 0.638 - ETA: 1s - loss: 0.9128 - acc: 0.638 - ETA: 1s - loss: 0.9206 - acc: 0.636 - ETA: 1s - loss: 0.9229 - acc: 0.637 - ETA: 0s - loss: 0.9253 - acc: 0.635 - ETA: 0s - loss: 0.9269 - acc: 0.632 - ETA: 0s - loss: 0.9300 - acc: 0.631 - ETA: 0s - loss: 0.9382 - acc: 0.626 - ETA: 0s - loss: 0.9430 - acc: 0.625 - ETA: 0s - loss: 0.9438 - acc: 0.623 - ETA: 0s - loss: 0.9453 - acc: 0.622 - ETA: 0s - loss: 0.9497 - acc: 0.619 - 7s 5ms/step - loss: 0.9502 - acc: 0.6194 - val_loss: 1.2500 - val_acc: 0.4987\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.53316\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 51/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9261 - acc: 0.593 - ETA: 5s - loss: 0.9466 - acc: 0.593 - ETA: 5s - loss: 1.0261 - acc: 0.572 - ETA: 5s - loss: 1.0447 - acc: 0.546 - ETA: 5s - loss: 1.0490 - acc: 0.537 - ETA: 5s - loss: 1.0302 - acc: 0.552 - ETA: 5s - loss: 1.0166 - acc: 0.558 - ETA: 5s - loss: 1.0580 - acc: 0.539 - ETA: 5s - loss: 1.0673 - acc: 0.552 - ETA: 4s - loss: 1.0738 - acc: 0.559 - ETA: 4s - loss: 1.0944 - acc: 0.551 - ETA: 4s - loss: 1.1067 - acc: 0.549 - ETA: 4s - loss: 1.0862 - acc: 0.557 - ETA: 4s - loss: 1.0687 - acc: 0.567 - ETA: 4s - loss: 1.0568 - acc: 0.577 - ETA: 4s - loss: 1.0570 - acc: 0.574 - ETA: 4s - loss: 1.0513 - acc: 0.575 - ETA: 3s - loss: 1.0455 - acc: 0.576 - ETA: 3s - loss: 1.0364 - acc: 0.575 - ETA: 3s - loss: 1.0276 - acc: 0.578 - ETA: 3s - loss: 1.0300 - acc: 0.577 - ETA: 3s - loss: 1.0245 - acc: 0.583 - ETA: 3s - loss: 1.0234 - acc: 0.585 - ETA: 3s - loss: 1.0264 - acc: 0.587 - ETA: 2s - loss: 1.0382 - acc: 0.581 - ETA: 2s - loss: 1.0282 - acc: 0.582 - ETA: 2s - loss: 1.0157 - acc: 0.589 - ETA: 2s - loss: 1.0159 - acc: 0.591 - ETA: 2s - loss: 1.0103 - acc: 0.592 - ETA: 2s - loss: 1.0210 - acc: 0.588 - ETA: 2s - loss: 1.0232 - acc: 0.585 - ETA: 2s - loss: 1.0260 - acc: 0.585 - ETA: 1s - loss: 1.0261 - acc: 0.585 - ETA: 1s - loss: 1.0211 - acc: 0.587 - ETA: 1s - loss: 1.0176 - acc: 0.588 - ETA: 1s - loss: 1.0191 - acc: 0.585 - ETA: 1s - loss: 1.0114 - acc: 0.592 - ETA: 1s - loss: 1.0077 - acc: 0.595 - ETA: 1s - loss: 1.0029 - acc: 0.596 - ETA: 0s - loss: 1.0033 - acc: 0.596 - ETA: 0s - loss: 1.0013 - acc: 0.596 - ETA: 0s - loss: 1.0065 - acc: 0.596 - ETA: 0s - loss: 1.0031 - acc: 0.598 - ETA: 0s - loss: 1.0044 - acc: 0.597 - ETA: 0s - loss: 1.0056 - acc: 0.596 - ETA: 0s - loss: 1.0075 - acc: 0.597 - ETA: 0s - loss: 1.0048 - acc: 0.598 - 7s 5ms/step - loss: 1.0054 - acc: 0.5975 - val_loss: 1.2151 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.53316\n",
      "Epoch 52/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9958 - acc: 0.500 - ETA: 6s - loss: 1.0083 - acc: 0.546 - ETA: 5s - loss: 1.0255 - acc: 0.593 - ETA: 5s - loss: 1.0077 - acc: 0.593 - ETA: 5s - loss: 1.0444 - acc: 0.562 - ETA: 5s - loss: 1.0460 - acc: 0.557 - ETA: 5s - loss: 1.0182 - acc: 0.571 - ETA: 5s - loss: 1.0393 - acc: 0.558 - ETA: 5s - loss: 1.0285 - acc: 0.569 - ETA: 5s - loss: 1.0034 - acc: 0.590 - ETA: 4s - loss: 1.0035 - acc: 0.596 - ETA: 4s - loss: 0.9866 - acc: 0.606 - ETA: 4s - loss: 0.9879 - acc: 0.608 - ETA: 4s - loss: 0.9834 - acc: 0.611 - ETA: 4s - loss: 0.9800 - acc: 0.608 - ETA: 4s - loss: 0.9790 - acc: 0.609 - ETA: 4s - loss: 0.9910 - acc: 0.604 - ETA: 3s - loss: 0.9822 - acc: 0.611 - ETA: 3s - loss: 0.9826 - acc: 0.613 - ETA: 3s - loss: 0.9765 - acc: 0.615 - ETA: 3s - loss: 0.9741 - acc: 0.617 - ETA: 3s - loss: 0.9780 - acc: 0.615 - ETA: 3s - loss: 0.9739 - acc: 0.612 - ETA: 3s - loss: 0.9700 - acc: 0.614 - ETA: 2s - loss: 0.9696 - acc: 0.616 - ETA: 2s - loss: 0.9640 - acc: 0.616 - ETA: 2s - loss: 0.9604 - acc: 0.616 - ETA: 2s - loss: 0.9543 - acc: 0.623 - ETA: 2s - loss: 0.9580 - acc: 0.619 - ETA: 2s - loss: 0.9512 - acc: 0.621 - ETA: 2s - loss: 0.9430 - acc: 0.625 - ETA: 2s - loss: 0.9389 - acc: 0.626 - ETA: 1s - loss: 0.9395 - acc: 0.623 - ETA: 1s - loss: 0.9403 - acc: 0.624 - ETA: 1s - loss: 0.9402 - acc: 0.622 - ETA: 1s - loss: 0.9393 - acc: 0.620 - ETA: 1s - loss: 0.9334 - acc: 0.623 - ETA: 1s - loss: 0.9340 - acc: 0.622 - ETA: 1s - loss: 0.9337 - acc: 0.624 - ETA: 0s - loss: 0.9348 - acc: 0.623 - ETA: 0s - loss: 0.9326 - acc: 0.626 - ETA: 0s - loss: 0.9315 - acc: 0.625 - ETA: 0s - loss: 0.9320 - acc: 0.625 - ETA: 0s - loss: 0.9332 - acc: 0.623 - ETA: 0s - loss: 0.9319 - acc: 0.625 - ETA: 0s - loss: 0.9357 - acc: 0.625 - ETA: 0s - loss: 0.9351 - acc: 0.627 - 7s 5ms/step - loss: 0.9353 - acc: 0.6273 - val_loss: 1.2096 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.53316\n",
      "Epoch 53/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8077 - acc: 0.781 - ETA: 6s - loss: 0.8942 - acc: 0.718 - ETA: 5s - loss: 0.9798 - acc: 0.656 - ETA: 5s - loss: 0.9550 - acc: 0.664 - ETA: 5s - loss: 0.9157 - acc: 0.668 - ETA: 5s - loss: 0.9334 - acc: 0.651 - ETA: 5s - loss: 0.9517 - acc: 0.642 - ETA: 5s - loss: 0.9712 - acc: 0.640 - ETA: 5s - loss: 0.9699 - acc: 0.638 - ETA: 4s - loss: 0.9679 - acc: 0.631 - ETA: 4s - loss: 0.9607 - acc: 0.636 - ETA: 4s - loss: 0.9663 - acc: 0.630 - ETA: 4s - loss: 0.9640 - acc: 0.632 - ETA: 4s - loss: 0.9499 - acc: 0.640 - ETA: 4s - loss: 0.9528 - acc: 0.637 - ETA: 4s - loss: 0.9576 - acc: 0.630 - ETA: 4s - loss: 0.9621 - acc: 0.626 - ETA: 3s - loss: 0.9662 - acc: 0.621 - ETA: 3s - loss: 0.9585 - acc: 0.625 - ETA: 3s - loss: 0.9573 - acc: 0.625 - ETA: 3s - loss: 0.9497 - acc: 0.631 - ETA: 3s - loss: 0.9481 - acc: 0.636 - ETA: 3s - loss: 0.9421 - acc: 0.637 - ETA: 3s - loss: 0.9410 - acc: 0.641 - ETA: 2s - loss: 0.9343 - acc: 0.643 - ETA: 2s - loss: 0.9346 - acc: 0.644 - ETA: 2s - loss: 0.9295 - acc: 0.648 - ETA: 2s - loss: 0.9343 - acc: 0.642 - ETA: 2s - loss: 0.9273 - acc: 0.648 - ETA: 2s - loss: 0.9303 - acc: 0.645 - ETA: 2s - loss: 0.9284 - acc: 0.644 - ETA: 2s - loss: 0.9302 - acc: 0.645 - ETA: 1s - loss: 0.9317 - acc: 0.646 - ETA: 1s - loss: 0.9322 - acc: 0.646 - ETA: 1s - loss: 0.9296 - acc: 0.644 - ETA: 1s - loss: 0.9250 - acc: 0.646 - ETA: 1s - loss: 0.9222 - acc: 0.648 - ETA: 1s - loss: 0.9280 - acc: 0.649 - ETA: 1s - loss: 0.9210 - acc: 0.652 - ETA: 0s - loss: 0.9234 - acc: 0.647 - ETA: 0s - loss: 0.9215 - acc: 0.647 - ETA: 0s - loss: 0.9283 - acc: 0.643 - ETA: 0s - loss: 0.9282 - acc: 0.643 - ETA: 0s - loss: 0.9257 - acc: 0.642 - ETA: 0s - loss: 0.9257 - acc: 0.642 - ETA: 0s - loss: 0.9255 - acc: 0.644 - ETA: 0s - loss: 0.9255 - acc: 0.645 - 7s 5ms/step - loss: 0.9274 - acc: 0.6446 - val_loss: 1.2111 - val_acc: 0.5119\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.53316\n",
      "Epoch 54/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8020 - acc: 0.750 - ETA: 6s - loss: 0.8174 - acc: 0.703 - ETA: 5s - loss: 0.8253 - acc: 0.708 - ETA: 5s - loss: 0.7998 - acc: 0.703 - ETA: 5s - loss: 0.8333 - acc: 0.700 - ETA: 5s - loss: 0.8173 - acc: 0.703 - ETA: 5s - loss: 0.8372 - acc: 0.692 - ETA: 5s - loss: 0.8826 - acc: 0.664 - ETA: 5s - loss: 0.8925 - acc: 0.652 - ETA: 5s - loss: 0.8617 - acc: 0.665 - ETA: 4s - loss: 0.8563 - acc: 0.670 - ETA: 4s - loss: 0.8915 - acc: 0.653 - ETA: 4s - loss: 0.9105 - acc: 0.639 - ETA: 4s - loss: 0.9245 - acc: 0.636 - ETA: 4s - loss: 0.9168 - acc: 0.635 - ETA: 4s - loss: 0.9112 - acc: 0.638 - ETA: 4s - loss: 0.9179 - acc: 0.636 - ETA: 3s - loss: 0.9174 - acc: 0.633 - ETA: 3s - loss: 0.9088 - acc: 0.634 - ETA: 3s - loss: 0.9083 - acc: 0.634 - ETA: 3s - loss: 0.9094 - acc: 0.629 - ETA: 3s - loss: 0.9040 - acc: 0.630 - ETA: 3s - loss: 0.9040 - acc: 0.631 - ETA: 3s - loss: 0.9030 - acc: 0.635 - ETA: 2s - loss: 0.9084 - acc: 0.635 - ETA: 2s - loss: 0.9212 - acc: 0.626 - ETA: 2s - loss: 0.9198 - acc: 0.627 - ETA: 2s - loss: 0.9212 - acc: 0.629 - ETA: 2s - loss: 0.9181 - acc: 0.633 - ETA: 2s - loss: 0.9086 - acc: 0.640 - ETA: 2s - loss: 0.9111 - acc: 0.639 - ETA: 2s - loss: 0.9064 - acc: 0.642 - ETA: 1s - loss: 0.9192 - acc: 0.637 - ETA: 1s - loss: 0.9191 - acc: 0.635 - ETA: 1s - loss: 0.9228 - acc: 0.633 - ETA: 1s - loss: 0.9215 - acc: 0.635 - ETA: 1s - loss: 0.9215 - acc: 0.635 - ETA: 1s - loss: 0.9252 - acc: 0.634 - ETA: 1s - loss: 0.9245 - acc: 0.633 - ETA: 0s - loss: 0.9218 - acc: 0.634 - ETA: 0s - loss: 0.9151 - acc: 0.638 - ETA: 0s - loss: 0.9146 - acc: 0.639 - ETA: 0s - loss: 0.9137 - acc: 0.638 - ETA: 0s - loss: 0.9131 - acc: 0.638 - ETA: 0s - loss: 0.9133 - acc: 0.638 - ETA: 0s - loss: 0.9160 - acc: 0.636 - ETA: 0s - loss: 0.9172 - acc: 0.637 - 7s 5ms/step - loss: 0.9179 - acc: 0.6373 - val_loss: 1.2134 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.53316\n",
      "Epoch 55/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8090 - acc: 0.625 - ETA: 5s - loss: 0.8858 - acc: 0.609 - ETA: 5s - loss: 0.8912 - acc: 0.625 - ETA: 5s - loss: 0.8746 - acc: 0.617 - ETA: 5s - loss: 0.8389 - acc: 0.643 - ETA: 5s - loss: 0.8345 - acc: 0.666 - ETA: 5s - loss: 0.8577 - acc: 0.660 - ETA: 5s - loss: 0.8789 - acc: 0.652 - ETA: 5s - loss: 0.8957 - acc: 0.638 - ETA: 4s - loss: 0.8889 - acc: 0.640 - ETA: 4s - loss: 0.8938 - acc: 0.636 - ETA: 4s - loss: 0.8992 - acc: 0.635 - ETA: 4s - loss: 0.9038 - acc: 0.632 - ETA: 4s - loss: 0.8882 - acc: 0.642 - ETA: 4s - loss: 0.8816 - acc: 0.652 - ETA: 4s - loss: 0.8917 - acc: 0.652 - ETA: 4s - loss: 0.8806 - acc: 0.656 - ETA: 3s - loss: 0.8901 - acc: 0.651 - ETA: 3s - loss: 0.8898 - acc: 0.649 - ETA: 3s - loss: 0.8840 - acc: 0.653 - ETA: 3s - loss: 0.8886 - acc: 0.647 - ETA: 3s - loss: 0.8838 - acc: 0.652 - ETA: 3s - loss: 0.8865 - acc: 0.649 - ETA: 3s - loss: 0.8925 - acc: 0.647 - ETA: 2s - loss: 0.8876 - acc: 0.652 - ETA: 2s - loss: 0.8889 - acc: 0.646 - ETA: 2s - loss: 0.9004 - acc: 0.640 - ETA: 2s - loss: 0.9017 - acc: 0.641 - ETA: 2s - loss: 0.9053 - acc: 0.636 - ETA: 2s - loss: 0.9101 - acc: 0.635 - ETA: 2s - loss: 0.9032 - acc: 0.640 - ETA: 2s - loss: 0.9074 - acc: 0.638 - ETA: 1s - loss: 0.9048 - acc: 0.639 - ETA: 1s - loss: 0.8970 - acc: 0.644 - ETA: 1s - loss: 0.8976 - acc: 0.642 - ETA: 1s - loss: 0.8955 - acc: 0.641 - ETA: 1s - loss: 0.8930 - acc: 0.641 - ETA: 1s - loss: 0.9060 - acc: 0.638 - ETA: 1s - loss: 0.9052 - acc: 0.639 - ETA: 0s - loss: 0.9048 - acc: 0.638 - ETA: 0s - loss: 0.9047 - acc: 0.636 - ETA: 0s - loss: 0.9064 - acc: 0.636 - ETA: 0s - loss: 0.9073 - acc: 0.635 - ETA: 0s - loss: 0.9059 - acc: 0.638 - ETA: 0s - loss: 0.9031 - acc: 0.640 - ETA: 0s - loss: 0.9036 - acc: 0.639 - ETA: 0s - loss: 0.9036 - acc: 0.638 - 7s 5ms/step - loss: 0.9046 - acc: 0.6379 - val_loss: 1.2161 - val_acc: 0.5013\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.53316\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9268 - acc: 0.593 - ETA: 6s - loss: 0.7780 - acc: 0.734 - ETA: 5s - loss: 0.8940 - acc: 0.656 - ETA: 5s - loss: 0.8777 - acc: 0.695 - ETA: 5s - loss: 0.8717 - acc: 0.681 - ETA: 5s - loss: 0.8721 - acc: 0.661 - ETA: 5s - loss: 0.8762 - acc: 0.660 - ETA: 5s - loss: 0.8602 - acc: 0.671 - ETA: 5s - loss: 0.8516 - acc: 0.673 - ETA: 5s - loss: 0.8375 - acc: 0.678 - ETA: 4s - loss: 0.8477 - acc: 0.670 - ETA: 4s - loss: 0.8257 - acc: 0.687 - ETA: 4s - loss: 0.8348 - acc: 0.685 - ETA: 4s - loss: 0.8374 - acc: 0.680 - ETA: 4s - loss: 0.8534 - acc: 0.670 - ETA: 4s - loss: 0.8825 - acc: 0.656 - ETA: 4s - loss: 0.8911 - acc: 0.648 - ETA: 3s - loss: 0.8925 - acc: 0.645 - ETA: 3s - loss: 0.8873 - acc: 0.648 - ETA: 3s - loss: 0.8892 - acc: 0.650 - ETA: 3s - loss: 0.8778 - acc: 0.656 - ETA: 3s - loss: 0.8783 - acc: 0.654 - ETA: 3s - loss: 0.8842 - acc: 0.646 - ETA: 3s - loss: 0.8799 - acc: 0.649 - ETA: 2s - loss: 0.8687 - acc: 0.655 - ETA: 2s - loss: 0.8668 - acc: 0.658 - ETA: 2s - loss: 0.8681 - acc: 0.660 - ETA: 2s - loss: 0.8592 - acc: 0.666 - ETA: 2s - loss: 0.8572 - acc: 0.667 - ETA: 2s - loss: 0.8617 - acc: 0.664 - ETA: 2s - loss: 0.8605 - acc: 0.664 - ETA: 2s - loss: 0.8670 - acc: 0.660 - ETA: 1s - loss: 0.8734 - acc: 0.655 - ETA: 1s - loss: 0.8733 - acc: 0.657 - ETA: 1s - loss: 0.8763 - acc: 0.656 - ETA: 1s - loss: 0.8838 - acc: 0.652 - ETA: 1s - loss: 0.8816 - acc: 0.652 - ETA: 1s - loss: 0.8836 - acc: 0.652 - ETA: 1s - loss: 0.8882 - acc: 0.651 - ETA: 0s - loss: 0.8937 - acc: 0.647 - ETA: 0s - loss: 0.9004 - acc: 0.645 - ETA: 0s - loss: 0.8961 - acc: 0.645 - ETA: 0s - loss: 0.8963 - acc: 0.646 - ETA: 0s - loss: 0.8984 - acc: 0.644 - ETA: 0s - loss: 0.9014 - acc: 0.640 - ETA: 0s - loss: 0.9019 - acc: 0.639 - ETA: 0s - loss: 0.9025 - acc: 0.639 - 7s 5ms/step - loss: 0.9026 - acc: 0.6393 - val_loss: 1.2179 - val_acc: 0.5146\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.53316\n",
      "Epoch 57/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8120 - acc: 0.781 - ETA: 5s - loss: 0.8789 - acc: 0.671 - ETA: 5s - loss: 1.0093 - acc: 0.593 - ETA: 5s - loss: 0.9993 - acc: 0.585 - ETA: 5s - loss: 0.9745 - acc: 0.612 - ETA: 5s - loss: 0.9320 - acc: 0.640 - ETA: 5s - loss: 0.9205 - acc: 0.656 - ETA: 5s - loss: 0.9218 - acc: 0.660 - ETA: 5s - loss: 0.9348 - acc: 0.642 - ETA: 4s - loss: 0.9412 - acc: 0.634 - ETA: 4s - loss: 0.9313 - acc: 0.639 - ETA: 4s - loss: 0.9469 - acc: 0.635 - ETA: 4s - loss: 0.9515 - acc: 0.627 - ETA: 4s - loss: 0.9429 - acc: 0.631 - ETA: 4s - loss: 0.9358 - acc: 0.631 - ETA: 4s - loss: 0.9234 - acc: 0.646 - ETA: 4s - loss: 0.9217 - acc: 0.645 - ETA: 3s - loss: 0.9094 - acc: 0.651 - ETA: 3s - loss: 0.9101 - acc: 0.654 - ETA: 3s - loss: 0.9108 - acc: 0.653 - ETA: 3s - loss: 0.9083 - acc: 0.656 - ETA: 3s - loss: 0.9071 - acc: 0.659 - ETA: 3s - loss: 0.8996 - acc: 0.661 - ETA: 3s - loss: 0.9009 - acc: 0.661 - ETA: 2s - loss: 0.8983 - acc: 0.665 - ETA: 2s - loss: 0.9004 - acc: 0.664 - ETA: 2s - loss: 0.8987 - acc: 0.664 - ETA: 2s - loss: 0.8994 - acc: 0.662 - ETA: 2s - loss: 0.8941 - acc: 0.662 - ETA: 2s - loss: 0.8908 - acc: 0.664 - ETA: 2s - loss: 0.8976 - acc: 0.660 - ETA: 2s - loss: 0.8973 - acc: 0.662 - ETA: 1s - loss: 0.9038 - acc: 0.657 - ETA: 1s - loss: 0.9013 - acc: 0.658 - ETA: 1s - loss: 0.9026 - acc: 0.655 - ETA: 1s - loss: 0.9040 - acc: 0.651 - ETA: 1s - loss: 0.9020 - acc: 0.652 - ETA: 1s - loss: 0.9005 - acc: 0.652 - ETA: 1s - loss: 0.9017 - acc: 0.651 - ETA: 0s - loss: 0.9016 - acc: 0.651 - ETA: 0s - loss: 0.8977 - acc: 0.654 - ETA: 0s - loss: 0.9042 - acc: 0.651 - ETA: 0s - loss: 0.9069 - acc: 0.647 - ETA: 0s - loss: 0.9055 - acc: 0.647 - ETA: 0s - loss: 0.9026 - acc: 0.646 - ETA: 0s - loss: 0.8999 - acc: 0.650 - ETA: 0s - loss: 0.9032 - acc: 0.651 - 7s 5ms/step - loss: 0.9034 - acc: 0.6512 - val_loss: 1.2186 - val_acc: 0.5119\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.53316\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 58/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9031 - acc: 0.656 - ETA: 5s - loss: 0.9775 - acc: 0.625 - ETA: 5s - loss: 0.9311 - acc: 0.625 - ETA: 5s - loss: 0.9415 - acc: 0.617 - ETA: 5s - loss: 0.9607 - acc: 0.618 - ETA: 5s - loss: 0.9280 - acc: 0.635 - ETA: 5s - loss: 0.9400 - acc: 0.620 - ETA: 5s - loss: 0.9222 - acc: 0.621 - ETA: 5s - loss: 0.9104 - acc: 0.631 - ETA: 4s - loss: 0.8912 - acc: 0.643 - ETA: 4s - loss: 0.8904 - acc: 0.636 - ETA: 4s - loss: 0.8920 - acc: 0.635 - ETA: 4s - loss: 0.9003 - acc: 0.632 - ETA: 4s - loss: 0.9074 - acc: 0.627 - ETA: 4s - loss: 0.9103 - acc: 0.627 - ETA: 4s - loss: 0.9155 - acc: 0.630 - ETA: 4s - loss: 0.9190 - acc: 0.630 - ETA: 3s - loss: 0.9233 - acc: 0.631 - ETA: 3s - loss: 0.9157 - acc: 0.631 - ETA: 3s - loss: 0.9131 - acc: 0.632 - ETA: 3s - loss: 0.9196 - acc: 0.633 - ETA: 3s - loss: 0.9268 - acc: 0.629 - ETA: 3s - loss: 0.9266 - acc: 0.629 - ETA: 3s - loss: 0.9294 - acc: 0.628 - ETA: 2s - loss: 0.9189 - acc: 0.636 - ETA: 2s - loss: 0.9232 - acc: 0.637 - ETA: 2s - loss: 0.9232 - acc: 0.638 - ETA: 2s - loss: 0.9175 - acc: 0.642 - ETA: 2s - loss: 0.9190 - acc: 0.643 - ETA: 2s - loss: 0.9217 - acc: 0.644 - ETA: 2s - loss: 0.9160 - acc: 0.648 - ETA: 2s - loss: 0.9168 - acc: 0.648 - ETA: 1s - loss: 0.9091 - acc: 0.652 - ETA: 1s - loss: 0.9081 - acc: 0.650 - ETA: 1s - loss: 0.9160 - acc: 0.649 - ETA: 1s - loss: 0.9155 - acc: 0.648 - ETA: 1s - loss: 0.9147 - acc: 0.645 - ETA: 1s - loss: 0.9148 - acc: 0.644 - ETA: 1s - loss: 0.9138 - acc: 0.644 - ETA: 0s - loss: 0.9163 - acc: 0.641 - ETA: 0s - loss: 0.9148 - acc: 0.641 - ETA: 0s - loss: 0.9218 - acc: 0.636 - ETA: 0s - loss: 0.9245 - acc: 0.633 - ETA: 0s - loss: 0.9305 - acc: 0.632 - ETA: 0s - loss: 0.9272 - acc: 0.634 - ETA: 0s - loss: 0.9224 - acc: 0.635 - ETA: 0s - loss: 0.9219 - acc: 0.633 - 7s 5ms/step - loss: 0.9224 - acc: 0.6326 - val_loss: 1.2184 - val_acc: 0.5146\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.53316\n",
      "Epoch 59/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9344 - acc: 0.656 - ETA: 5s - loss: 0.9043 - acc: 0.625 - ETA: 5s - loss: 0.8462 - acc: 0.635 - ETA: 5s - loss: 0.8379 - acc: 0.632 - ETA: 5s - loss: 0.8888 - acc: 0.612 - ETA: 5s - loss: 0.9108 - acc: 0.609 - ETA: 5s - loss: 0.9210 - acc: 0.611 - ETA: 5s - loss: 0.9074 - acc: 0.621 - ETA: 5s - loss: 0.9173 - acc: 0.614 - ETA: 4s - loss: 0.9028 - acc: 0.625 - ETA: 4s - loss: 0.8914 - acc: 0.630 - ETA: 4s - loss: 0.8991 - acc: 0.625 - ETA: 4s - loss: 0.8871 - acc: 0.632 - ETA: 4s - loss: 0.8908 - acc: 0.629 - ETA: 4s - loss: 0.8848 - acc: 0.637 - ETA: 4s - loss: 0.8829 - acc: 0.642 - ETA: 4s - loss: 0.9003 - acc: 0.632 - ETA: 3s - loss: 0.9090 - acc: 0.631 - ETA: 3s - loss: 0.9001 - acc: 0.636 - ETA: 3s - loss: 0.9050 - acc: 0.632 - ETA: 3s - loss: 0.8974 - acc: 0.636 - ETA: 3s - loss: 0.9082 - acc: 0.630 - ETA: 3s - loss: 0.9056 - acc: 0.633 - ETA: 3s - loss: 0.9039 - acc: 0.636 - ETA: 2s - loss: 0.9040 - acc: 0.640 - ETA: 2s - loss: 0.9116 - acc: 0.633 - ETA: 2s - loss: 0.9182 - acc: 0.631 - ETA: 2s - loss: 0.9162 - acc: 0.632 - ETA: 2s - loss: 0.9183 - acc: 0.629 - ETA: 2s - loss: 0.9131 - acc: 0.631 - ETA: 2s - loss: 0.9089 - acc: 0.634 - ETA: 2s - loss: 0.9028 - acc: 0.635 - ETA: 1s - loss: 0.9057 - acc: 0.634 - ETA: 1s - loss: 0.9135 - acc: 0.631 - ETA: 1s - loss: 0.9177 - acc: 0.628 - ETA: 1s - loss: 0.9180 - acc: 0.629 - ETA: 1s - loss: 0.9179 - acc: 0.628 - ETA: 1s - loss: 0.9186 - acc: 0.627 - ETA: 1s - loss: 0.9187 - acc: 0.627 - ETA: 0s - loss: 0.9110 - acc: 0.632 - ETA: 0s - loss: 0.9114 - acc: 0.631 - ETA: 0s - loss: 0.9091 - acc: 0.633 - ETA: 0s - loss: 0.9081 - acc: 0.633 - ETA: 0s - loss: 0.9082 - acc: 0.634 - ETA: 0s - loss: 0.9076 - acc: 0.636 - ETA: 0s - loss: 0.9075 - acc: 0.635 - ETA: 0s - loss: 0.9060 - acc: 0.637 - 7s 5ms/step - loss: 0.9057 - acc: 0.6373 - val_loss: 1.2169 - val_acc: 0.5119\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.53316\n",
      "Epoch 60/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.0774 - acc: 0.531 - ETA: 5s - loss: 0.9629 - acc: 0.625 - ETA: 5s - loss: 1.0130 - acc: 0.635 - ETA: 5s - loss: 1.0183 - acc: 0.625 - ETA: 5s - loss: 0.9818 - acc: 0.618 - ETA: 5s - loss: 0.9539 - acc: 0.630 - ETA: 5s - loss: 0.9244 - acc: 0.642 - ETA: 5s - loss: 0.9335 - acc: 0.636 - ETA: 5s - loss: 0.9171 - acc: 0.638 - ETA: 4s - loss: 0.9329 - acc: 0.628 - ETA: 4s - loss: 0.9457 - acc: 0.613 - ETA: 4s - loss: 0.9335 - acc: 0.625 - ETA: 4s - loss: 0.9272 - acc: 0.632 - ETA: 4s - loss: 0.9394 - acc: 0.627 - ETA: 4s - loss: 0.9339 - acc: 0.629 - ETA: 4s - loss: 0.9292 - acc: 0.627 - ETA: 4s - loss: 0.9389 - acc: 0.619 - ETA: 3s - loss: 0.9478 - acc: 0.612 - ETA: 3s - loss: 0.9391 - acc: 0.613 - ETA: 3s - loss: 0.9273 - acc: 0.618 - ETA: 3s - loss: 0.9194 - acc: 0.625 - ETA: 3s - loss: 0.9107 - acc: 0.632 - ETA: 3s - loss: 0.9151 - acc: 0.625 - ETA: 3s - loss: 0.9227 - acc: 0.621 - ETA: 2s - loss: 0.9195 - acc: 0.622 - ETA: 2s - loss: 0.9153 - acc: 0.621 - ETA: 2s - loss: 0.9059 - acc: 0.627 - ETA: 2s - loss: 0.9096 - acc: 0.626 - ETA: 2s - loss: 0.9079 - acc: 0.628 - ETA: 2s - loss: 0.9034 - acc: 0.633 - ETA: 2s - loss: 0.9059 - acc: 0.633 - ETA: 2s - loss: 0.9108 - acc: 0.629 - ETA: 1s - loss: 0.9117 - acc: 0.630 - ETA: 1s - loss: 0.9118 - acc: 0.630 - ETA: 1s - loss: 0.9181 - acc: 0.625 - ETA: 1s - loss: 0.9151 - acc: 0.628 - ETA: 1s - loss: 0.9172 - acc: 0.629 - ETA: 1s - loss: 0.9137 - acc: 0.631 - ETA: 1s - loss: 0.9167 - acc: 0.627 - ETA: 0s - loss: 0.9152 - acc: 0.630 - ETA: 0s - loss: 0.9128 - acc: 0.632 - ETA: 0s - loss: 0.9093 - acc: 0.633 - ETA: 0s - loss: 0.9097 - acc: 0.633 - ETA: 0s - loss: 0.9034 - acc: 0.637 - ETA: 0s - loss: 0.9047 - acc: 0.637 - ETA: 0s - loss: 0.9016 - acc: 0.639 - ETA: 0s - loss: 0.9045 - acc: 0.639 - 7s 5ms/step - loss: 0.9046 - acc: 0.6386 - val_loss: 1.2167 - val_acc: 0.5093\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.53316\n",
      "Epoch 61/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9373 - acc: 0.593 - ETA: 5s - loss: 0.9105 - acc: 0.640 - ETA: 5s - loss: 0.8756 - acc: 0.677 - ETA: 5s - loss: 0.8462 - acc: 0.687 - ETA: 5s - loss: 0.8322 - acc: 0.693 - ETA: 5s - loss: 0.8776 - acc: 0.677 - ETA: 5s - loss: 0.8804 - acc: 0.683 - ETA: 5s - loss: 0.8764 - acc: 0.683 - ETA: 5s - loss: 0.8716 - acc: 0.680 - ETA: 4s - loss: 0.8688 - acc: 0.675 - ETA: 4s - loss: 0.8860 - acc: 0.664 - ETA: 4s - loss: 0.8885 - acc: 0.664 - ETA: 4s - loss: 0.8933 - acc: 0.653 - ETA: 4s - loss: 0.8970 - acc: 0.649 - ETA: 4s - loss: 0.8807 - acc: 0.656 - ETA: 4s - loss: 0.8775 - acc: 0.662 - ETA: 4s - loss: 0.8918 - acc: 0.648 - ETA: 3s - loss: 0.8977 - acc: 0.645 - ETA: 3s - loss: 0.9014 - acc: 0.641 - ETA: 3s - loss: 0.9037 - acc: 0.640 - ETA: 3s - loss: 0.9072 - acc: 0.642 - ETA: 3s - loss: 0.9019 - acc: 0.646 - ETA: 3s - loss: 0.9002 - acc: 0.649 - ETA: 3s - loss: 0.9080 - acc: 0.643 - ETA: 2s - loss: 0.9028 - acc: 0.647 - ETA: 2s - loss: 0.9011 - acc: 0.647 - ETA: 2s - loss: 0.8978 - acc: 0.649 - ETA: 2s - loss: 0.8964 - acc: 0.652 - ETA: 2s - loss: 0.9018 - acc: 0.649 - ETA: 2s - loss: 0.9001 - acc: 0.654 - ETA: 2s - loss: 0.8947 - acc: 0.656 - ETA: 2s - loss: 0.8964 - acc: 0.657 - ETA: 1s - loss: 0.8977 - acc: 0.658 - ETA: 1s - loss: 0.8885 - acc: 0.662 - ETA: 1s - loss: 0.8857 - acc: 0.663 - ETA: 1s - loss: 0.8872 - acc: 0.662 - ETA: 1s - loss: 0.8914 - acc: 0.659 - ETA: 1s - loss: 0.8954 - acc: 0.657 - ETA: 1s - loss: 0.8925 - acc: 0.659 - ETA: 0s - loss: 0.8970 - acc: 0.655 - ETA: 0s - loss: 0.8969 - acc: 0.654 - ETA: 0s - loss: 0.8907 - acc: 0.656 - ETA: 0s - loss: 0.8919 - acc: 0.654 - ETA: 0s - loss: 0.8965 - acc: 0.652 - ETA: 0s - loss: 0.8989 - acc: 0.649 - ETA: 0s - loss: 0.8971 - acc: 0.650 - ETA: 0s - loss: 0.8982 - acc: 0.650 - 7s 5ms/step - loss: 0.8977 - acc: 0.6512 - val_loss: 1.2172 - val_acc: 0.5066\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.53316\n",
      "Epoch 62/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8020 - acc: 0.750 - ETA: 6s - loss: 0.7855 - acc: 0.718 - ETA: 5s - loss: 0.8088 - acc: 0.697 - ETA: 5s - loss: 0.7909 - acc: 0.687 - ETA: 5s - loss: 0.8132 - acc: 0.637 - ETA: 5s - loss: 0.8151 - acc: 0.640 - ETA: 5s - loss: 0.8351 - acc: 0.629 - ETA: 5s - loss: 0.8379 - acc: 0.640 - ETA: 5s - loss: 0.8294 - acc: 0.649 - ETA: 5s - loss: 0.8260 - acc: 0.650 - ETA: 4s - loss: 0.8434 - acc: 0.647 - ETA: 4s - loss: 0.8539 - acc: 0.643 - ETA: 4s - loss: 0.8534 - acc: 0.641 - ETA: 4s - loss: 0.8786 - acc: 0.625 - ETA: 4s - loss: 0.8769 - acc: 0.629 - ETA: 4s - loss: 0.8796 - acc: 0.630 - ETA: 4s - loss: 0.8711 - acc: 0.639 - ETA: 3s - loss: 0.8650 - acc: 0.642 - ETA: 3s - loss: 0.8818 - acc: 0.636 - ETA: 3s - loss: 0.8873 - acc: 0.637 - ETA: 3s - loss: 0.8966 - acc: 0.633 - ETA: 3s - loss: 0.8850 - acc: 0.636 - ETA: 3s - loss: 0.8925 - acc: 0.633 - ETA: 3s - loss: 0.9054 - acc: 0.626 - ETA: 2s - loss: 0.9081 - acc: 0.621 - ETA: 2s - loss: 0.9056 - acc: 0.625 - ETA: 2s - loss: 0.9124 - acc: 0.622 - ETA: 2s - loss: 0.9118 - acc: 0.625 - ETA: 2s - loss: 0.9055 - acc: 0.630 - ETA: 2s - loss: 0.9051 - acc: 0.629 - ETA: 2s - loss: 0.9013 - acc: 0.632 - ETA: 2s - loss: 0.9056 - acc: 0.631 - ETA: 1s - loss: 0.9009 - acc: 0.634 - ETA: 1s - loss: 0.9045 - acc: 0.633 - ETA: 1s - loss: 0.9069 - acc: 0.632 - ETA: 1s - loss: 0.9067 - acc: 0.631 - ETA: 1s - loss: 0.9062 - acc: 0.633 - ETA: 1s - loss: 0.9023 - acc: 0.635 - ETA: 1s - loss: 0.9018 - acc: 0.635 - ETA: 0s - loss: 0.9003 - acc: 0.637 - ETA: 0s - loss: 0.9010 - acc: 0.637 - ETA: 0s - loss: 0.9010 - acc: 0.635 - ETA: 0s - loss: 0.9004 - acc: 0.635 - ETA: 0s - loss: 0.9025 - acc: 0.635 - ETA: 0s - loss: 0.9040 - acc: 0.636 - ETA: 0s - loss: 0.9045 - acc: 0.635 - ETA: 0s - loss: 0.9071 - acc: 0.634 - 7s 5ms/step - loss: 0.9081 - acc: 0.6340 - val_loss: 1.2173 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.53316\n",
      "Epoch 63/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.7457 - acc: 0.718 - ETA: 6s - loss: 0.7926 - acc: 0.703 - ETA: 5s - loss: 0.8699 - acc: 0.666 - ETA: 5s - loss: 0.8570 - acc: 0.664 - ETA: 5s - loss: 0.8654 - acc: 0.643 - ETA: 5s - loss: 0.8452 - acc: 0.661 - ETA: 5s - loss: 0.8655 - acc: 0.638 - ETA: 5s - loss: 0.8815 - acc: 0.628 - ETA: 5s - loss: 0.8662 - acc: 0.642 - ETA: 4s - loss: 0.8663 - acc: 0.640 - ETA: 4s - loss: 0.8666 - acc: 0.639 - ETA: 4s - loss: 0.8699 - acc: 0.643 - ETA: 4s - loss: 0.8643 - acc: 0.646 - ETA: 4s - loss: 0.8698 - acc: 0.645 - ETA: 4s - loss: 0.8738 - acc: 0.647 - ETA: 4s - loss: 0.8710 - acc: 0.654 - ETA: 4s - loss: 0.8806 - acc: 0.648 - ETA: 3s - loss: 0.8863 - acc: 0.642 - ETA: 3s - loss: 0.8877 - acc: 0.649 - ETA: 3s - loss: 0.8907 - acc: 0.646 - ETA: 3s - loss: 0.8911 - acc: 0.647 - ETA: 3s - loss: 0.8935 - acc: 0.643 - ETA: 3s - loss: 0.8908 - acc: 0.646 - ETA: 3s - loss: 0.8827 - acc: 0.651 - ETA: 2s - loss: 0.8797 - acc: 0.651 - ETA: 2s - loss: 0.8833 - acc: 0.647 - ETA: 2s - loss: 0.8855 - acc: 0.648 - ETA: 2s - loss: 0.8857 - acc: 0.646 - ETA: 2s - loss: 0.8852 - acc: 0.649 - ETA: 2s - loss: 0.8842 - acc: 0.651 - ETA: 2s - loss: 0.8846 - acc: 0.652 - ETA: 2s - loss: 0.8886 - acc: 0.649 - ETA: 1s - loss: 0.8951 - acc: 0.645 - ETA: 1s - loss: 0.8918 - acc: 0.649 - ETA: 1s - loss: 0.8848 - acc: 0.655 - ETA: 1s - loss: 0.8876 - acc: 0.654 - ETA: 1s - loss: 0.8969 - acc: 0.650 - ETA: 1s - loss: 0.8956 - acc: 0.653 - ETA: 1s - loss: 0.9001 - acc: 0.652 - ETA: 0s - loss: 0.8973 - acc: 0.653 - ETA: 0s - loss: 0.8966 - acc: 0.653 - ETA: 0s - loss: 0.8906 - acc: 0.657 - ETA: 0s - loss: 0.8974 - acc: 0.654 - ETA: 0s - loss: 0.8996 - acc: 0.654 - ETA: 0s - loss: 0.8999 - acc: 0.652 - ETA: 0s - loss: 0.9024 - acc: 0.649 - ETA: 0s - loss: 0.9001 - acc: 0.649 - 7s 5ms/step - loss: 0.9002 - acc: 0.6499 - val_loss: 1.2176 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.53316\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0508 - acc: 0.562 - ETA: 6s - loss: 0.8936 - acc: 0.625 - ETA: 5s - loss: 0.8713 - acc: 0.635 - ETA: 5s - loss: 0.8208 - acc: 0.656 - ETA: 5s - loss: 0.8942 - acc: 0.618 - ETA: 5s - loss: 0.9239 - acc: 0.614 - ETA: 5s - loss: 0.9374 - acc: 0.611 - ETA: 5s - loss: 0.9454 - acc: 0.605 - ETA: 5s - loss: 0.9421 - acc: 0.611 - ETA: 4s - loss: 0.9260 - acc: 0.628 - ETA: 4s - loss: 0.9480 - acc: 0.625 - ETA: 4s - loss: 0.9289 - acc: 0.638 - ETA: 4s - loss: 0.9091 - acc: 0.656 - ETA: 4s - loss: 0.9094 - acc: 0.658 - ETA: 4s - loss: 0.9261 - acc: 0.647 - ETA: 4s - loss: 0.9276 - acc: 0.640 - ETA: 4s - loss: 0.9255 - acc: 0.637 - ETA: 3s - loss: 0.9251 - acc: 0.635 - ETA: 3s - loss: 0.9269 - acc: 0.638 - ETA: 3s - loss: 0.9231 - acc: 0.642 - ETA: 3s - loss: 0.9185 - acc: 0.642 - ETA: 3s - loss: 0.9185 - acc: 0.647 - ETA: 3s - loss: 0.9105 - acc: 0.653 - ETA: 3s - loss: 0.9026 - acc: 0.657 - ETA: 2s - loss: 0.9035 - acc: 0.656 - ETA: 2s - loss: 0.9012 - acc: 0.656 - ETA: 2s - loss: 0.9020 - acc: 0.657 - ETA: 2s - loss: 0.8986 - acc: 0.656 - ETA: 2s - loss: 0.9009 - acc: 0.654 - ETA: 2s - loss: 0.9008 - acc: 0.652 - ETA: 2s - loss: 0.9010 - acc: 0.653 - ETA: 2s - loss: 0.9048 - acc: 0.650 - ETA: 1s - loss: 0.9063 - acc: 0.648 - ETA: 1s - loss: 0.9025 - acc: 0.650 - ETA: 1s - loss: 0.9011 - acc: 0.651 - ETA: 1s - loss: 0.9082 - acc: 0.649 - ETA: 1s - loss: 0.9058 - acc: 0.654 - ETA: 1s - loss: 0.9030 - acc: 0.656 - ETA: 1s - loss: 0.8980 - acc: 0.658 - ETA: 0s - loss: 0.8947 - acc: 0.658 - ETA: 0s - loss: 0.9000 - acc: 0.655 - ETA: 0s - loss: 0.8978 - acc: 0.658 - ETA: 0s - loss: 0.8932 - acc: 0.661 - ETA: 0s - loss: 0.8947 - acc: 0.662 - ETA: 0s - loss: 0.8996 - acc: 0.658 - ETA: 0s - loss: 0.9004 - acc: 0.658 - ETA: 0s - loss: 0.9016 - acc: 0.656 - 7s 5ms/step - loss: 0.9001 - acc: 0.6578 - val_loss: 1.2178 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.53316\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 65/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8190 - acc: 0.687 - ETA: 5s - loss: 0.8330 - acc: 0.671 - ETA: 5s - loss: 0.8629 - acc: 0.645 - ETA: 5s - loss: 0.8904 - acc: 0.625 - ETA: 5s - loss: 0.8680 - acc: 0.643 - ETA: 5s - loss: 0.8591 - acc: 0.640 - ETA: 5s - loss: 0.8770 - acc: 0.638 - ETA: 5s - loss: 0.8700 - acc: 0.660 - ETA: 5s - loss: 0.9100 - acc: 0.642 - ETA: 5s - loss: 0.9364 - acc: 0.618 - ETA: 4s - loss: 0.9309 - acc: 0.616 - ETA: 4s - loss: 0.9347 - acc: 0.606 - ETA: 4s - loss: 0.9193 - acc: 0.613 - ETA: 4s - loss: 0.9166 - acc: 0.618 - ETA: 4s - loss: 0.9140 - acc: 0.620 - ETA: 4s - loss: 0.9074 - acc: 0.625 - ETA: 4s - loss: 0.9024 - acc: 0.625 - ETA: 3s - loss: 0.8969 - acc: 0.631 - ETA: 3s - loss: 0.8988 - acc: 0.628 - ETA: 3s - loss: 0.8966 - acc: 0.628 - ETA: 3s - loss: 0.9039 - acc: 0.625 - ETA: 3s - loss: 0.9130 - acc: 0.619 - ETA: 3s - loss: 0.9142 - acc: 0.619 - ETA: 3s - loss: 0.9112 - acc: 0.623 - ETA: 2s - loss: 0.9062 - acc: 0.627 - ETA: 2s - loss: 0.9050 - acc: 0.629 - ETA: 2s - loss: 0.9029 - acc: 0.631 - ETA: 2s - loss: 0.8968 - acc: 0.636 - ETA: 2s - loss: 0.9073 - acc: 0.630 - ETA: 2s - loss: 0.9157 - acc: 0.626 - ETA: 2s - loss: 0.9099 - acc: 0.630 - ETA: 2s - loss: 0.9194 - acc: 0.625 - ETA: 1s - loss: 0.9123 - acc: 0.629 - ETA: 1s - loss: 0.9140 - acc: 0.628 - ETA: 1s - loss: 0.9152 - acc: 0.629 - ETA: 1s - loss: 0.9189 - acc: 0.626 - ETA: 1s - loss: 0.9200 - acc: 0.626 - ETA: 1s - loss: 0.9175 - acc: 0.626 - ETA: 1s - loss: 0.9180 - acc: 0.625 - ETA: 0s - loss: 0.9147 - acc: 0.627 - ETA: 0s - loss: 0.9074 - acc: 0.633 - ETA: 0s - loss: 0.9087 - acc: 0.631 - ETA: 0s - loss: 0.9069 - acc: 0.634 - ETA: 0s - loss: 0.9106 - acc: 0.634 - ETA: 0s - loss: 0.9093 - acc: 0.636 - ETA: 0s - loss: 0.9070 - acc: 0.636 - ETA: 0s - loss: 0.9049 - acc: 0.638 - 7s 5ms/step - loss: 0.9042 - acc: 0.6386 - val_loss: 1.2178 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.53316\n",
      "Epoch 66/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0488 - acc: 0.625 - ETA: 6s - loss: 0.8563 - acc: 0.718 - ETA: 5s - loss: 0.8504 - acc: 0.687 - ETA: 5s - loss: 0.8082 - acc: 0.687 - ETA: 5s - loss: 0.7768 - acc: 0.712 - ETA: 5s - loss: 0.7676 - acc: 0.713 - ETA: 5s - loss: 0.7982 - acc: 0.714 - ETA: 5s - loss: 0.8269 - acc: 0.695 - ETA: 5s - loss: 0.8970 - acc: 0.649 - ETA: 5s - loss: 0.9003 - acc: 0.643 - ETA: 4s - loss: 0.9022 - acc: 0.647 - ETA: 4s - loss: 0.9157 - acc: 0.632 - ETA: 4s - loss: 0.9064 - acc: 0.639 - ETA: 4s - loss: 0.9005 - acc: 0.640 - ETA: 4s - loss: 0.9028 - acc: 0.637 - ETA: 4s - loss: 0.8968 - acc: 0.638 - ETA: 4s - loss: 0.9014 - acc: 0.636 - ETA: 3s - loss: 0.9107 - acc: 0.631 - ETA: 3s - loss: 0.9082 - acc: 0.633 - ETA: 3s - loss: 0.9027 - acc: 0.634 - ETA: 3s - loss: 0.9045 - acc: 0.633 - ETA: 3s - loss: 0.9132 - acc: 0.630 - ETA: 3s - loss: 0.9087 - acc: 0.629 - ETA: 3s - loss: 0.9036 - acc: 0.634 - ETA: 2s - loss: 0.8988 - acc: 0.640 - ETA: 2s - loss: 0.9052 - acc: 0.639 - ETA: 2s - loss: 0.9091 - acc: 0.636 - ETA: 2s - loss: 0.9130 - acc: 0.633 - ETA: 2s - loss: 0.9188 - acc: 0.628 - ETA: 2s - loss: 0.9213 - acc: 0.626 - ETA: 2s - loss: 0.9198 - acc: 0.627 - ETA: 2s - loss: 0.9169 - acc: 0.627 - ETA: 1s - loss: 0.9193 - acc: 0.626 - ETA: 1s - loss: 0.9172 - acc: 0.628 - ETA: 1s - loss: 0.9191 - acc: 0.625 - ETA: 1s - loss: 0.9118 - acc: 0.626 - ETA: 1s - loss: 0.9135 - acc: 0.625 - ETA: 1s - loss: 0.9082 - acc: 0.628 - ETA: 1s - loss: 0.9041 - acc: 0.630 - ETA: 0s - loss: 0.9039 - acc: 0.632 - ETA: 0s - loss: 0.9109 - acc: 0.629 - ETA: 0s - loss: 0.9109 - acc: 0.628 - ETA: 0s - loss: 0.9107 - acc: 0.630 - ETA: 0s - loss: 0.9154 - acc: 0.630 - ETA: 0s - loss: 0.9164 - acc: 0.631 - ETA: 0s - loss: 0.9162 - acc: 0.631 - ETA: 0s - loss: 0.9166 - acc: 0.631 - 7s 5ms/step - loss: 0.9157 - acc: 0.6320 - val_loss: 1.2179 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.53316\n",
      "Epoch 67/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.6897 - acc: 0.781 - ETA: 6s - loss: 0.7418 - acc: 0.703 - ETA: 5s - loss: 0.9194 - acc: 0.614 - ETA: 5s - loss: 0.8617 - acc: 0.648 - ETA: 5s - loss: 0.9138 - acc: 0.618 - ETA: 5s - loss: 0.9231 - acc: 0.599 - ETA: 5s - loss: 0.9001 - acc: 0.607 - ETA: 5s - loss: 0.8818 - acc: 0.621 - ETA: 5s - loss: 0.9319 - acc: 0.600 - ETA: 5s - loss: 0.9129 - acc: 0.609 - ETA: 4s - loss: 0.9134 - acc: 0.613 - ETA: 4s - loss: 0.9166 - acc: 0.612 - ETA: 4s - loss: 0.8959 - acc: 0.617 - ETA: 4s - loss: 0.9121 - acc: 0.613 - ETA: 4s - loss: 0.9103 - acc: 0.616 - ETA: 4s - loss: 0.9040 - acc: 0.621 - ETA: 4s - loss: 0.9119 - acc: 0.617 - ETA: 3s - loss: 0.8982 - acc: 0.630 - ETA: 3s - loss: 0.8970 - acc: 0.633 - ETA: 3s - loss: 0.8901 - acc: 0.637 - ETA: 3s - loss: 0.8984 - acc: 0.632 - ETA: 3s - loss: 0.9047 - acc: 0.630 - ETA: 3s - loss: 0.9054 - acc: 0.634 - ETA: 3s - loss: 0.9085 - acc: 0.631 - ETA: 3s - loss: 0.9062 - acc: 0.635 - ETA: 2s - loss: 0.9037 - acc: 0.634 - ETA: 2s - loss: 0.9019 - acc: 0.636 - ETA: 2s - loss: 0.8993 - acc: 0.638 - ETA: 2s - loss: 0.9036 - acc: 0.639 - ETA: 2s - loss: 0.9057 - acc: 0.642 - ETA: 2s - loss: 0.9086 - acc: 0.638 - ETA: 2s - loss: 0.9142 - acc: 0.634 - ETA: 1s - loss: 0.9105 - acc: 0.639 - ETA: 1s - loss: 0.9066 - acc: 0.637 - ETA: 1s - loss: 0.9120 - acc: 0.635 - ETA: 1s - loss: 0.9165 - acc: 0.632 - ETA: 1s - loss: 0.9111 - acc: 0.636 - ETA: 1s - loss: 0.9133 - acc: 0.634 - ETA: 1s - loss: 0.9184 - acc: 0.631 - ETA: 0s - loss: 0.9141 - acc: 0.632 - ETA: 0s - loss: 0.9143 - acc: 0.632 - ETA: 0s - loss: 0.9129 - acc: 0.633 - ETA: 0s - loss: 0.9111 - acc: 0.636 - ETA: 0s - loss: 0.9062 - acc: 0.641 - ETA: 0s - loss: 0.9025 - acc: 0.641 - ETA: 0s - loss: 0.9019 - acc: 0.642 - ETA: 0s - loss: 0.9048 - acc: 0.639 - 7s 5ms/step - loss: 0.9045 - acc: 0.6393 - val_loss: 1.2179 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.53316\n",
      "Epoch 68/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9179 - acc: 0.625 - ETA: 6s - loss: 1.0157 - acc: 0.593 - ETA: 6s - loss: 0.9381 - acc: 0.614 - ETA: 6s - loss: 0.9185 - acc: 0.632 - ETA: 6s - loss: 0.8771 - acc: 0.631 - ETA: 5s - loss: 0.8943 - acc: 0.609 - ETA: 5s - loss: 0.8544 - acc: 0.629 - ETA: 5s - loss: 0.8754 - acc: 0.625 - ETA: 5s - loss: 0.8813 - acc: 0.618 - ETA: 5s - loss: 0.8754 - acc: 0.621 - ETA: 5s - loss: 0.8883 - acc: 0.619 - ETA: 5s - loss: 0.8950 - acc: 0.617 - ETA: 4s - loss: 0.8864 - acc: 0.617 - ETA: 4s - loss: 0.8935 - acc: 0.618 - ETA: 4s - loss: 0.9155 - acc: 0.608 - ETA: 4s - loss: 0.9128 - acc: 0.617 - ETA: 4s - loss: 0.9034 - acc: 0.628 - ETA: 4s - loss: 0.8944 - acc: 0.635 - ETA: 3s - loss: 0.8982 - acc: 0.631 - ETA: 3s - loss: 0.9009 - acc: 0.631 - ETA: 3s - loss: 0.9043 - acc: 0.632 - ETA: 3s - loss: 0.9085 - acc: 0.630 - ETA: 3s - loss: 0.9124 - acc: 0.627 - ETA: 3s - loss: 0.9066 - acc: 0.634 - ETA: 3s - loss: 0.9104 - acc: 0.631 - ETA: 2s - loss: 0.9160 - acc: 0.625 - ETA: 2s - loss: 0.9230 - acc: 0.622 - ETA: 2s - loss: 0.9200 - acc: 0.620 - ETA: 2s - loss: 0.9196 - acc: 0.620 - ETA: 2s - loss: 0.9187 - acc: 0.620 - ETA: 2s - loss: 0.9222 - acc: 0.619 - ETA: 2s - loss: 0.9178 - acc: 0.620 - ETA: 1s - loss: 0.9201 - acc: 0.618 - ETA: 1s - loss: 0.9159 - acc: 0.622 - ETA: 1s - loss: 0.9189 - acc: 0.622 - ETA: 1s - loss: 0.9155 - acc: 0.624 - ETA: 1s - loss: 0.9075 - acc: 0.629 - ETA: 1s - loss: 0.9127 - acc: 0.627 - ETA: 1s - loss: 0.9088 - acc: 0.629 - ETA: 0s - loss: 0.9046 - acc: 0.632 - ETA: 0s - loss: 0.9008 - acc: 0.636 - ETA: 0s - loss: 0.8981 - acc: 0.638 - ETA: 0s - loss: 0.8983 - acc: 0.638 - ETA: 0s - loss: 0.8941 - acc: 0.642 - ETA: 0s - loss: 0.8959 - acc: 0.642 - ETA: 0s - loss: 0.8978 - acc: 0.642 - ETA: 0s - loss: 0.8973 - acc: 0.642 - 7s 5ms/step - loss: 0.8976 - acc: 0.6412 - val_loss: 1.2179 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.53316\n",
      "Epoch 69/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 1.1824 - acc: 0.531 - ETA: 5s - loss: 1.0146 - acc: 0.593 - ETA: 5s - loss: 0.9016 - acc: 0.687 - ETA: 5s - loss: 0.8832 - acc: 0.679 - ETA: 5s - loss: 0.8603 - acc: 0.700 - ETA: 5s - loss: 0.8623 - acc: 0.697 - ETA: 5s - loss: 0.8586 - acc: 0.687 - ETA: 5s - loss: 0.8366 - acc: 0.691 - ETA: 5s - loss: 0.8188 - acc: 0.697 - ETA: 4s - loss: 0.8443 - acc: 0.678 - ETA: 4s - loss: 0.8595 - acc: 0.670 - ETA: 4s - loss: 0.8585 - acc: 0.669 - ETA: 4s - loss: 0.8581 - acc: 0.665 - ETA: 4s - loss: 0.8561 - acc: 0.662 - ETA: 4s - loss: 0.8585 - acc: 0.660 - ETA: 4s - loss: 0.8514 - acc: 0.664 - ETA: 4s - loss: 0.8642 - acc: 0.661 - ETA: 3s - loss: 0.8578 - acc: 0.668 - ETA: 3s - loss: 0.8757 - acc: 0.662 - ETA: 3s - loss: 0.8699 - acc: 0.665 - ETA: 3s - loss: 0.8788 - acc: 0.663 - ETA: 3s - loss: 0.8795 - acc: 0.663 - ETA: 3s - loss: 0.8754 - acc: 0.664 - ETA: 3s - loss: 0.8732 - acc: 0.662 - ETA: 2s - loss: 0.8767 - acc: 0.660 - ETA: 2s - loss: 0.8776 - acc: 0.658 - ETA: 2s - loss: 0.8832 - acc: 0.655 - ETA: 2s - loss: 0.8919 - acc: 0.650 - ETA: 2s - loss: 0.8876 - acc: 0.651 - ETA: 2s - loss: 0.8818 - acc: 0.657 - ETA: 2s - loss: 0.8758 - acc: 0.660 - ETA: 2s - loss: 0.8805 - acc: 0.656 - ETA: 1s - loss: 0.8809 - acc: 0.657 - ETA: 1s - loss: 0.8834 - acc: 0.656 - ETA: 1s - loss: 0.8899 - acc: 0.651 - ETA: 1s - loss: 0.8918 - acc: 0.652 - ETA: 1s - loss: 0.8934 - acc: 0.650 - ETA: 1s - loss: 0.8971 - acc: 0.648 - ETA: 1s - loss: 0.8956 - acc: 0.647 - ETA: 0s - loss: 0.8977 - acc: 0.646 - ETA: 0s - loss: 0.8953 - acc: 0.648 - ETA: 0s - loss: 0.8991 - acc: 0.647 - ETA: 0s - loss: 0.9013 - acc: 0.645 - ETA: 0s - loss: 0.9043 - acc: 0.644 - ETA: 0s - loss: 0.9045 - acc: 0.645 - ETA: 0s - loss: 0.9056 - acc: 0.644 - ETA: 0s - loss: 0.9082 - acc: 0.643 - 7s 5ms/step - loss: 0.9083 - acc: 0.6432 - val_loss: 1.2179 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.53316\n",
      "Epoch 70/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9839 - acc: 0.656 - ETA: 6s - loss: 0.9890 - acc: 0.625 - ETA: 5s - loss: 0.9447 - acc: 0.614 - ETA: 5s - loss: 0.9137 - acc: 0.632 - ETA: 5s - loss: 0.9047 - acc: 0.643 - ETA: 5s - loss: 0.8680 - acc: 0.677 - ETA: 5s - loss: 0.8760 - acc: 0.665 - ETA: 5s - loss: 0.8916 - acc: 0.644 - ETA: 5s - loss: 0.9243 - acc: 0.631 - ETA: 4s - loss: 0.9199 - acc: 0.634 - ETA: 4s - loss: 0.9098 - acc: 0.639 - ETA: 4s - loss: 0.9014 - acc: 0.643 - ETA: 4s - loss: 0.9049 - acc: 0.649 - ETA: 4s - loss: 0.9184 - acc: 0.649 - ETA: 4s - loss: 0.9170 - acc: 0.656 - ETA: 4s - loss: 0.9180 - acc: 0.650 - ETA: 4s - loss: 0.9230 - acc: 0.647 - ETA: 3s - loss: 0.9233 - acc: 0.642 - ETA: 3s - loss: 0.9234 - acc: 0.639 - ETA: 3s - loss: 0.9249 - acc: 0.643 - ETA: 3s - loss: 0.9251 - acc: 0.639 - ETA: 3s - loss: 0.9236 - acc: 0.639 - ETA: 3s - loss: 0.9348 - acc: 0.635 - ETA: 3s - loss: 0.9354 - acc: 0.632 - ETA: 2s - loss: 0.9368 - acc: 0.632 - ETA: 2s - loss: 0.9381 - acc: 0.632 - ETA: 2s - loss: 0.9374 - acc: 0.630 - ETA: 2s - loss: 0.9317 - acc: 0.632 - ETA: 2s - loss: 0.9285 - acc: 0.635 - ETA: 2s - loss: 0.9323 - acc: 0.635 - ETA: 2s - loss: 0.9301 - acc: 0.634 - ETA: 2s - loss: 0.9289 - acc: 0.632 - ETA: 1s - loss: 0.9308 - acc: 0.631 - ETA: 1s - loss: 0.9262 - acc: 0.633 - ETA: 1s - loss: 0.9298 - acc: 0.632 - ETA: 1s - loss: 0.9285 - acc: 0.632 - ETA: 1s - loss: 0.9326 - acc: 0.630 - ETA: 1s - loss: 0.9276 - acc: 0.634 - ETA: 1s - loss: 0.9335 - acc: 0.631 - ETA: 0s - loss: 0.9318 - acc: 0.630 - ETA: 0s - loss: 0.9298 - acc: 0.632 - ETA: 0s - loss: 0.9279 - acc: 0.633 - ETA: 0s - loss: 0.9271 - acc: 0.633 - ETA: 0s - loss: 0.9252 - acc: 0.634 - ETA: 0s - loss: 0.9208 - acc: 0.637 - ETA: 0s - loss: 0.9186 - acc: 0.637 - ETA: 0s - loss: 0.9127 - acc: 0.640 - 7s 5ms/step - loss: 0.9121 - acc: 0.6406 - val_loss: 1.2179 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.53316\n",
      "Epoch 71/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.7999 - acc: 0.718 - ETA: 6s - loss: 0.8185 - acc: 0.718 - ETA: 5s - loss: 0.8131 - acc: 0.687 - ETA: 5s - loss: 0.8880 - acc: 0.664 - ETA: 5s - loss: 0.8960 - acc: 0.643 - ETA: 5s - loss: 0.8679 - acc: 0.666 - ETA: 5s - loss: 0.8799 - acc: 0.656 - ETA: 5s - loss: 0.8883 - acc: 0.652 - ETA: 5s - loss: 0.8817 - acc: 0.666 - ETA: 4s - loss: 0.8942 - acc: 0.668 - ETA: 4s - loss: 0.9145 - acc: 0.647 - ETA: 4s - loss: 0.9150 - acc: 0.653 - ETA: 4s - loss: 0.9002 - acc: 0.658 - ETA: 4s - loss: 0.8965 - acc: 0.665 - ETA: 4s - loss: 0.9014 - acc: 0.664 - ETA: 4s - loss: 0.9027 - acc: 0.666 - ETA: 4s - loss: 0.9061 - acc: 0.667 - ETA: 3s - loss: 0.8967 - acc: 0.671 - ETA: 3s - loss: 0.8932 - acc: 0.674 - ETA: 3s - loss: 0.8938 - acc: 0.673 - ETA: 3s - loss: 0.9001 - acc: 0.669 - ETA: 3s - loss: 0.9080 - acc: 0.664 - ETA: 3s - loss: 0.9109 - acc: 0.664 - ETA: 3s - loss: 0.9057 - acc: 0.665 - ETA: 2s - loss: 0.9101 - acc: 0.661 - ETA: 2s - loss: 0.9119 - acc: 0.656 - ETA: 2s - loss: 0.9074 - acc: 0.659 - ETA: 2s - loss: 0.9018 - acc: 0.661 - ETA: 2s - loss: 0.9047 - acc: 0.657 - ETA: 2s - loss: 0.9089 - acc: 0.656 - ETA: 2s - loss: 0.9025 - acc: 0.657 - ETA: 2s - loss: 0.9034 - acc: 0.655 - ETA: 1s - loss: 0.8970 - acc: 0.656 - ETA: 1s - loss: 0.8962 - acc: 0.656 - ETA: 1s - loss: 0.8903 - acc: 0.659 - ETA: 1s - loss: 0.8892 - acc: 0.658 - ETA: 1s - loss: 0.8933 - acc: 0.658 - ETA: 1s - loss: 0.8960 - acc: 0.655 - ETA: 1s - loss: 0.8941 - acc: 0.654 - ETA: 0s - loss: 0.9040 - acc: 0.652 - ETA: 0s - loss: 0.9049 - acc: 0.650 - ETA: 0s - loss: 0.9088 - acc: 0.650 - ETA: 0s - loss: 0.9096 - acc: 0.649 - ETA: 0s - loss: 0.9085 - acc: 0.648 - ETA: 0s - loss: 0.9088 - acc: 0.649 - ETA: 0s - loss: 0.9106 - acc: 0.650 - ETA: 0s - loss: 0.9079 - acc: 0.653 - 7s 5ms/step - loss: 0.9065 - acc: 0.6538 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.53316\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8814 - acc: 0.687 - ETA: 6s - loss: 0.8775 - acc: 0.703 - ETA: 5s - loss: 0.8531 - acc: 0.687 - ETA: 5s - loss: 0.8509 - acc: 0.679 - ETA: 5s - loss: 0.8737 - acc: 0.668 - ETA: 5s - loss: 0.8850 - acc: 0.661 - ETA: 5s - loss: 0.8959 - acc: 0.660 - ETA: 5s - loss: 0.9245 - acc: 0.644 - ETA: 5s - loss: 0.9177 - acc: 0.649 - ETA: 4s - loss: 0.9072 - acc: 0.656 - ETA: 4s - loss: 0.9079 - acc: 0.653 - ETA: 4s - loss: 0.9058 - acc: 0.658 - ETA: 4s - loss: 0.9018 - acc: 0.663 - ETA: 4s - loss: 0.9100 - acc: 0.651 - ETA: 4s - loss: 0.9155 - acc: 0.645 - ETA: 4s - loss: 0.9092 - acc: 0.650 - ETA: 4s - loss: 0.9180 - acc: 0.636 - ETA: 3s - loss: 0.9074 - acc: 0.638 - ETA: 3s - loss: 0.9074 - acc: 0.639 - ETA: 3s - loss: 0.9159 - acc: 0.637 - ETA: 3s - loss: 0.9264 - acc: 0.626 - ETA: 3s - loss: 0.9202 - acc: 0.633 - ETA: 3s - loss: 0.9139 - acc: 0.635 - ETA: 3s - loss: 0.9110 - acc: 0.638 - ETA: 2s - loss: 0.9050 - acc: 0.638 - ETA: 2s - loss: 0.9106 - acc: 0.635 - ETA: 2s - loss: 0.9180 - acc: 0.635 - ETA: 2s - loss: 0.9085 - acc: 0.640 - ETA: 2s - loss: 0.9090 - acc: 0.637 - ETA: 2s - loss: 0.9130 - acc: 0.635 - ETA: 2s - loss: 0.9147 - acc: 0.634 - ETA: 2s - loss: 0.9202 - acc: 0.629 - ETA: 1s - loss: 0.9171 - acc: 0.631 - ETA: 1s - loss: 0.9121 - acc: 0.635 - ETA: 1s - loss: 0.9095 - acc: 0.636 - ETA: 1s - loss: 0.9155 - acc: 0.631 - ETA: 1s - loss: 0.9201 - acc: 0.630 - ETA: 1s - loss: 0.9138 - acc: 0.631 - ETA: 1s - loss: 0.9161 - acc: 0.632 - ETA: 0s - loss: 0.9186 - acc: 0.629 - ETA: 0s - loss: 0.9178 - acc: 0.631 - ETA: 0s - loss: 0.9156 - acc: 0.634 - ETA: 0s - loss: 0.9146 - acc: 0.635 - ETA: 0s - loss: 0.9193 - acc: 0.634 - ETA: 0s - loss: 0.9236 - acc: 0.631 - ETA: 0s - loss: 0.9262 - acc: 0.632 - ETA: 0s - loss: 0.9222 - acc: 0.636 - 7s 5ms/step - loss: 0.9205 - acc: 0.6373 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.53316\n",
      "Epoch 73/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.1791 - acc: 0.468 - ETA: 5s - loss: 0.9933 - acc: 0.546 - ETA: 5s - loss: 0.9787 - acc: 0.572 - ETA: 5s - loss: 1.0155 - acc: 0.585 - ETA: 5s - loss: 1.0141 - acc: 0.568 - ETA: 5s - loss: 0.9455 - acc: 0.609 - ETA: 5s - loss: 0.9654 - acc: 0.607 - ETA: 5s - loss: 0.9713 - acc: 0.617 - ETA: 5s - loss: 0.9960 - acc: 0.597 - ETA: 4s - loss: 0.9669 - acc: 0.612 - ETA: 4s - loss: 0.9485 - acc: 0.616 - ETA: 4s - loss: 0.9493 - acc: 0.609 - ETA: 4s - loss: 0.9708 - acc: 0.598 - ETA: 4s - loss: 0.9606 - acc: 0.604 - ETA: 4s - loss: 0.9598 - acc: 0.604 - ETA: 4s - loss: 0.9512 - acc: 0.609 - ETA: 4s - loss: 0.9489 - acc: 0.612 - ETA: 3s - loss: 0.9445 - acc: 0.616 - ETA: 3s - loss: 0.9404 - acc: 0.616 - ETA: 3s - loss: 0.9454 - acc: 0.610 - ETA: 3s - loss: 0.9543 - acc: 0.602 - ETA: 3s - loss: 0.9469 - acc: 0.602 - ETA: 3s - loss: 0.9447 - acc: 0.606 - ETA: 3s - loss: 0.9531 - acc: 0.606 - ETA: 2s - loss: 0.9486 - acc: 0.608 - ETA: 2s - loss: 0.9432 - acc: 0.609 - ETA: 2s - loss: 0.9426 - acc: 0.610 - ETA: 2s - loss: 0.9383 - acc: 0.615 - ETA: 2s - loss: 0.9379 - acc: 0.614 - ETA: 2s - loss: 0.9359 - acc: 0.616 - ETA: 2s - loss: 0.9282 - acc: 0.617 - ETA: 2s - loss: 0.9283 - acc: 0.618 - ETA: 1s - loss: 0.9241 - acc: 0.621 - ETA: 1s - loss: 0.9261 - acc: 0.623 - ETA: 1s - loss: 0.9228 - acc: 0.624 - ETA: 1s - loss: 0.9199 - acc: 0.625 - ETA: 1s - loss: 0.9227 - acc: 0.624 - ETA: 1s - loss: 0.9265 - acc: 0.621 - ETA: 1s - loss: 0.9210 - acc: 0.625 - ETA: 0s - loss: 0.9203 - acc: 0.623 - ETA: 0s - loss: 0.9184 - acc: 0.624 - ETA: 0s - loss: 0.9141 - acc: 0.627 - ETA: 0s - loss: 0.9097 - acc: 0.632 - ETA: 0s - loss: 0.9098 - acc: 0.632 - ETA: 0s - loss: 0.9066 - acc: 0.636 - ETA: 0s - loss: 0.9049 - acc: 0.637 - ETA: 0s - loss: 0.9003 - acc: 0.639 - 7s 5ms/step - loss: 0.8996 - acc: 0.6399 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.53316\n",
      "Epoch 74/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.7706 - acc: 0.750 - ETA: 6s - loss: 0.7782 - acc: 0.750 - ETA: 5s - loss: 0.7552 - acc: 0.770 - ETA: 5s - loss: 0.7292 - acc: 0.789 - ETA: 5s - loss: 0.7422 - acc: 0.762 - ETA: 5s - loss: 0.7446 - acc: 0.750 - ETA: 5s - loss: 0.7401 - acc: 0.758 - ETA: 5s - loss: 0.7522 - acc: 0.742 - ETA: 5s - loss: 0.7813 - acc: 0.722 - ETA: 4s - loss: 0.8076 - acc: 0.715 - ETA: 4s - loss: 0.8135 - acc: 0.707 - ETA: 4s - loss: 0.8173 - acc: 0.703 - ETA: 4s - loss: 0.8293 - acc: 0.697 - ETA: 4s - loss: 0.8309 - acc: 0.694 - ETA: 4s - loss: 0.8355 - acc: 0.689 - ETA: 4s - loss: 0.8449 - acc: 0.685 - ETA: 4s - loss: 0.8437 - acc: 0.687 - ETA: 3s - loss: 0.8570 - acc: 0.682 - ETA: 3s - loss: 0.8645 - acc: 0.677 - ETA: 3s - loss: 0.8742 - acc: 0.671 - ETA: 3s - loss: 0.8761 - acc: 0.669 - ETA: 3s - loss: 0.8729 - acc: 0.670 - ETA: 3s - loss: 0.8791 - acc: 0.668 - ETA: 3s - loss: 0.8849 - acc: 0.662 - ETA: 2s - loss: 0.8880 - acc: 0.661 - ETA: 2s - loss: 0.8869 - acc: 0.663 - ETA: 2s - loss: 0.8902 - acc: 0.660 - ETA: 2s - loss: 0.8860 - acc: 0.665 - ETA: 2s - loss: 0.8856 - acc: 0.665 - ETA: 2s - loss: 0.8931 - acc: 0.661 - ETA: 2s - loss: 0.8938 - acc: 0.661 - ETA: 2s - loss: 0.8974 - acc: 0.660 - ETA: 1s - loss: 0.8974 - acc: 0.656 - ETA: 1s - loss: 0.8995 - acc: 0.653 - ETA: 1s - loss: 0.9008 - acc: 0.654 - ETA: 1s - loss: 0.9011 - acc: 0.654 - ETA: 1s - loss: 0.8986 - acc: 0.656 - ETA: 1s - loss: 0.9008 - acc: 0.652 - ETA: 1s - loss: 0.9030 - acc: 0.649 - ETA: 0s - loss: 0.9068 - acc: 0.646 - ETA: 0s - loss: 0.9043 - acc: 0.647 - ETA: 0s - loss: 0.9030 - acc: 0.648 - ETA: 0s - loss: 0.8996 - acc: 0.651 - ETA: 0s - loss: 0.9029 - acc: 0.648 - ETA: 0s - loss: 0.9016 - acc: 0.649 - ETA: 0s - loss: 0.9051 - acc: 0.648 - ETA: 0s - loss: 0.9044 - acc: 0.647 - 7s 5ms/step - loss: 0.9052 - acc: 0.6466 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.53316\n",
      "Epoch 75/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9659 - acc: 0.562 - ETA: 6s - loss: 0.8702 - acc: 0.625 - ETA: 5s - loss: 0.8398 - acc: 0.645 - ETA: 5s - loss: 0.8409 - acc: 0.664 - ETA: 5s - loss: 0.8462 - acc: 0.656 - ETA: 5s - loss: 0.8541 - acc: 0.656 - ETA: 5s - loss: 0.8785 - acc: 0.642 - ETA: 5s - loss: 0.9013 - acc: 0.632 - ETA: 5s - loss: 0.9258 - acc: 0.625 - ETA: 5s - loss: 0.9392 - acc: 0.625 - ETA: 4s - loss: 0.9401 - acc: 0.625 - ETA: 4s - loss: 0.9391 - acc: 0.619 - ETA: 4s - loss: 0.9284 - acc: 0.622 - ETA: 4s - loss: 0.9215 - acc: 0.622 - ETA: 4s - loss: 0.9336 - acc: 0.618 - ETA: 4s - loss: 0.9420 - acc: 0.617 - ETA: 4s - loss: 0.9342 - acc: 0.619 - ETA: 3s - loss: 0.9403 - acc: 0.616 - ETA: 3s - loss: 0.9396 - acc: 0.613 - ETA: 3s - loss: 0.9317 - acc: 0.615 - ETA: 3s - loss: 0.9321 - acc: 0.617 - ETA: 3s - loss: 0.9210 - acc: 0.623 - ETA: 3s - loss: 0.9212 - acc: 0.620 - ETA: 3s - loss: 0.9153 - acc: 0.625 - ETA: 2s - loss: 0.9122 - acc: 0.630 - ETA: 2s - loss: 0.9092 - acc: 0.632 - ETA: 2s - loss: 0.9121 - acc: 0.630 - ETA: 2s - loss: 0.9149 - acc: 0.630 - ETA: 2s - loss: 0.9080 - acc: 0.636 - ETA: 2s - loss: 0.9067 - acc: 0.636 - ETA: 2s - loss: 0.9108 - acc: 0.633 - ETA: 2s - loss: 0.9060 - acc: 0.635 - ETA: 1s - loss: 0.9017 - acc: 0.635 - ETA: 1s - loss: 0.9037 - acc: 0.634 - ETA: 1s - loss: 0.9069 - acc: 0.633 - ETA: 1s - loss: 0.9007 - acc: 0.636 - ETA: 1s - loss: 0.8979 - acc: 0.637 - ETA: 1s - loss: 0.8941 - acc: 0.639 - ETA: 1s - loss: 0.8949 - acc: 0.637 - ETA: 0s - loss: 0.8997 - acc: 0.634 - ETA: 0s - loss: 0.9034 - acc: 0.632 - ETA: 0s - loss: 0.8993 - acc: 0.634 - ETA: 0s - loss: 0.8966 - acc: 0.638 - ETA: 0s - loss: 0.8990 - acc: 0.634 - ETA: 0s - loss: 0.8972 - acc: 0.635 - ETA: 0s - loss: 0.9010 - acc: 0.632 - ETA: 0s - loss: 0.9043 - acc: 0.632 - 7s 5ms/step - loss: 0.9063 - acc: 0.6320 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.53316\n",
      "Epoch 76/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9656 - acc: 0.656 - ETA: 5s - loss: 0.9421 - acc: 0.625 - ETA: 5s - loss: 0.8483 - acc: 0.677 - ETA: 5s - loss: 0.8509 - acc: 0.664 - ETA: 5s - loss: 0.8483 - acc: 0.656 - ETA: 5s - loss: 0.8399 - acc: 0.651 - ETA: 5s - loss: 0.8749 - acc: 0.638 - ETA: 5s - loss: 0.9014 - acc: 0.632 - ETA: 5s - loss: 0.9216 - acc: 0.628 - ETA: 4s - loss: 0.9197 - acc: 0.634 - ETA: 4s - loss: 0.9081 - acc: 0.639 - ETA: 4s - loss: 0.9163 - acc: 0.630 - ETA: 4s - loss: 0.9048 - acc: 0.629 - ETA: 4s - loss: 0.9072 - acc: 0.627 - ETA: 4s - loss: 0.9091 - acc: 0.627 - ETA: 4s - loss: 0.9061 - acc: 0.630 - ETA: 4s - loss: 0.9004 - acc: 0.634 - ETA: 3s - loss: 0.8995 - acc: 0.635 - ETA: 3s - loss: 0.8892 - acc: 0.634 - ETA: 3s - loss: 0.8919 - acc: 0.632 - ETA: 3s - loss: 0.8938 - acc: 0.633 - ETA: 3s - loss: 0.9044 - acc: 0.632 - ETA: 3s - loss: 0.8948 - acc: 0.638 - ETA: 3s - loss: 0.8944 - acc: 0.636 - ETA: 2s - loss: 0.8861 - acc: 0.637 - ETA: 2s - loss: 0.8815 - acc: 0.641 - ETA: 2s - loss: 0.8825 - acc: 0.647 - ETA: 2s - loss: 0.8922 - acc: 0.644 - ETA: 2s - loss: 0.8858 - acc: 0.645 - ETA: 2s - loss: 0.8885 - acc: 0.645 - ETA: 2s - loss: 0.8840 - acc: 0.647 - ETA: 2s - loss: 0.8860 - acc: 0.645 - ETA: 1s - loss: 0.8858 - acc: 0.645 - ETA: 1s - loss: 0.8843 - acc: 0.644 - ETA: 1s - loss: 0.8810 - acc: 0.645 - ETA: 1s - loss: 0.8843 - acc: 0.642 - ETA: 1s - loss: 0.8923 - acc: 0.637 - ETA: 1s - loss: 0.8926 - acc: 0.638 - ETA: 1s - loss: 0.8959 - acc: 0.637 - ETA: 0s - loss: 0.8982 - acc: 0.633 - ETA: 0s - loss: 0.9006 - acc: 0.631 - ETA: 0s - loss: 0.9019 - acc: 0.629 - ETA: 0s - loss: 0.9065 - acc: 0.628 - ETA: 0s - loss: 0.9089 - acc: 0.629 - ETA: 0s - loss: 0.9093 - acc: 0.627 - ETA: 0s - loss: 0.9095 - acc: 0.627 - ETA: 0s - loss: 0.9068 - acc: 0.629 - 7s 5ms/step - loss: 0.9095 - acc: 0.6273 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.53316\n",
      "Epoch 77/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.7777 - acc: 0.781 - ETA: 5s - loss: 0.7775 - acc: 0.718 - ETA: 5s - loss: 0.7600 - acc: 0.739 - ETA: 5s - loss: 0.7918 - acc: 0.710 - ETA: 5s - loss: 0.8524 - acc: 0.687 - ETA: 5s - loss: 0.8351 - acc: 0.692 - ETA: 5s - loss: 0.8386 - acc: 0.696 - ETA: 5s - loss: 0.8617 - acc: 0.675 - ETA: 5s - loss: 0.8680 - acc: 0.666 - ETA: 4s - loss: 0.8685 - acc: 0.662 - ETA: 4s - loss: 0.8666 - acc: 0.650 - ETA: 4s - loss: 0.8695 - acc: 0.645 - ETA: 4s - loss: 0.8806 - acc: 0.649 - ETA: 4s - loss: 0.8888 - acc: 0.645 - ETA: 4s - loss: 0.8861 - acc: 0.645 - ETA: 4s - loss: 0.8854 - acc: 0.644 - ETA: 4s - loss: 0.8838 - acc: 0.645 - ETA: 3s - loss: 0.8910 - acc: 0.640 - ETA: 3s - loss: 0.8916 - acc: 0.638 - ETA: 3s - loss: 0.8899 - acc: 0.639 - ETA: 3s - loss: 0.8884 - acc: 0.641 - ETA: 3s - loss: 0.8877 - acc: 0.640 - ETA: 3s - loss: 0.8925 - acc: 0.635 - ETA: 3s - loss: 0.8908 - acc: 0.636 - ETA: 2s - loss: 0.8988 - acc: 0.632 - ETA: 2s - loss: 0.8978 - acc: 0.638 - ETA: 2s - loss: 0.8967 - acc: 0.636 - ETA: 2s - loss: 0.8981 - acc: 0.636 - ETA: 2s - loss: 0.8899 - acc: 0.640 - ETA: 2s - loss: 0.8909 - acc: 0.639 - ETA: 2s - loss: 0.8940 - acc: 0.639 - ETA: 2s - loss: 0.8948 - acc: 0.636 - ETA: 1s - loss: 0.8965 - acc: 0.636 - ETA: 1s - loss: 0.8985 - acc: 0.634 - ETA: 1s - loss: 0.8996 - acc: 0.637 - ETA: 1s - loss: 0.8983 - acc: 0.638 - ETA: 1s - loss: 0.9039 - acc: 0.636 - ETA: 1s - loss: 0.9041 - acc: 0.637 - ETA: 1s - loss: 0.8998 - acc: 0.639 - ETA: 0s - loss: 0.8982 - acc: 0.642 - ETA: 0s - loss: 0.8954 - acc: 0.646 - ETA: 0s - loss: 0.8951 - acc: 0.645 - ETA: 0s - loss: 0.9045 - acc: 0.640 - ETA: 0s - loss: 0.9083 - acc: 0.637 - ETA: 0s - loss: 0.9061 - acc: 0.639 - ETA: 0s - loss: 0.9044 - acc: 0.641 - ETA: 0s - loss: 0.9066 - acc: 0.639 - 7s 5ms/step - loss: 0.9068 - acc: 0.6386 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.53316\n",
      "Epoch 78/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9161 - acc: 0.562 - ETA: 5s - loss: 0.8741 - acc: 0.625 - ETA: 5s - loss: 0.8737 - acc: 0.656 - ETA: 5s - loss: 0.8811 - acc: 0.640 - ETA: 5s - loss: 0.8734 - acc: 0.643 - ETA: 5s - loss: 0.8397 - acc: 0.671 - ETA: 5s - loss: 0.8651 - acc: 0.665 - ETA: 5s - loss: 0.8540 - acc: 0.668 - ETA: 5s - loss: 0.9142 - acc: 0.642 - ETA: 4s - loss: 0.8979 - acc: 0.650 - ETA: 4s - loss: 0.8976 - acc: 0.653 - ETA: 4s - loss: 0.8819 - acc: 0.656 - ETA: 4s - loss: 0.8832 - acc: 0.653 - ETA: 4s - loss: 0.8862 - acc: 0.647 - ETA: 4s - loss: 0.8951 - acc: 0.641 - ETA: 4s - loss: 0.8885 - acc: 0.646 - ETA: 4s - loss: 0.8985 - acc: 0.643 - ETA: 3s - loss: 0.9027 - acc: 0.638 - ETA: 3s - loss: 0.8919 - acc: 0.643 - ETA: 3s - loss: 0.8918 - acc: 0.639 - ETA: 3s - loss: 0.9024 - acc: 0.629 - ETA: 3s - loss: 0.9137 - acc: 0.626 - ETA: 3s - loss: 0.9182 - acc: 0.623 - ETA: 3s - loss: 0.9239 - acc: 0.618 - ETA: 2s - loss: 0.9248 - acc: 0.620 - ETA: 2s - loss: 0.9326 - acc: 0.622 - ETA: 2s - loss: 0.9253 - acc: 0.625 - ETA: 2s - loss: 0.9217 - acc: 0.626 - ETA: 2s - loss: 0.9178 - acc: 0.630 - ETA: 2s - loss: 0.9153 - acc: 0.631 - ETA: 2s - loss: 0.9114 - acc: 0.636 - ETA: 2s - loss: 0.9118 - acc: 0.634 - ETA: 1s - loss: 0.9123 - acc: 0.638 - ETA: 1s - loss: 0.9131 - acc: 0.636 - ETA: 1s - loss: 0.9107 - acc: 0.639 - ETA: 1s - loss: 0.9111 - acc: 0.638 - ETA: 1s - loss: 0.9126 - acc: 0.636 - ETA: 1s - loss: 0.9079 - acc: 0.639 - ETA: 1s - loss: 0.9059 - acc: 0.641 - ETA: 0s - loss: 0.9016 - acc: 0.641 - ETA: 0s - loss: 0.9002 - acc: 0.641 - ETA: 0s - loss: 0.9012 - acc: 0.639 - ETA: 0s - loss: 0.9033 - acc: 0.637 - ETA: 0s - loss: 0.9024 - acc: 0.637 - ETA: 0s - loss: 0.9111 - acc: 0.636 - ETA: 0s - loss: 0.9090 - acc: 0.637 - ETA: 0s - loss: 0.9103 - acc: 0.638 - 7s 5ms/step - loss: 0.9098 - acc: 0.6379 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.53316\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "Epoch 79/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8425 - acc: 0.687 - ETA: 5s - loss: 0.8175 - acc: 0.703 - ETA: 5s - loss: 0.8442 - acc: 0.687 - ETA: 5s - loss: 0.8563 - acc: 0.695 - ETA: 5s - loss: 0.8571 - acc: 0.687 - ETA: 5s - loss: 0.8749 - acc: 0.682 - ETA: 5s - loss: 0.9083 - acc: 0.660 - ETA: 5s - loss: 0.8784 - acc: 0.679 - ETA: 5s - loss: 0.8631 - acc: 0.694 - ETA: 4s - loss: 0.8958 - acc: 0.684 - ETA: 4s - loss: 0.9027 - acc: 0.681 - ETA: 4s - loss: 0.9028 - acc: 0.674 - ETA: 4s - loss: 0.9044 - acc: 0.680 - ETA: 4s - loss: 0.8982 - acc: 0.680 - ETA: 4s - loss: 0.9025 - acc: 0.677 - ETA: 4s - loss: 0.8976 - acc: 0.679 - ETA: 4s - loss: 0.8882 - acc: 0.685 - ETA: 3s - loss: 0.8868 - acc: 0.685 - ETA: 3s - loss: 0.8885 - acc: 0.680 - ETA: 3s - loss: 0.8939 - acc: 0.671 - ETA: 3s - loss: 0.8989 - acc: 0.665 - ETA: 3s - loss: 0.9024 - acc: 0.659 - ETA: 3s - loss: 0.8978 - acc: 0.661 - ETA: 3s - loss: 0.8947 - acc: 0.662 - ETA: 2s - loss: 0.8885 - acc: 0.671 - ETA: 2s - loss: 0.8902 - acc: 0.668 - ETA: 2s - loss: 0.8869 - acc: 0.665 - ETA: 2s - loss: 0.8938 - acc: 0.662 - ETA: 2s - loss: 0.8958 - acc: 0.663 - ETA: 2s - loss: 0.8997 - acc: 0.660 - ETA: 2s - loss: 0.9026 - acc: 0.657 - ETA: 2s - loss: 0.8957 - acc: 0.659 - ETA: 1s - loss: 0.8956 - acc: 0.655 - ETA: 1s - loss: 0.8886 - acc: 0.657 - ETA: 1s - loss: 0.8916 - acc: 0.655 - ETA: 1s - loss: 0.8949 - acc: 0.651 - ETA: 1s - loss: 0.8957 - acc: 0.651 - ETA: 1s - loss: 0.8917 - acc: 0.653 - ETA: 1s - loss: 0.8938 - acc: 0.652 - ETA: 0s - loss: 0.9012 - acc: 0.648 - ETA: 0s - loss: 0.9001 - acc: 0.649 - ETA: 0s - loss: 0.8995 - acc: 0.648 - ETA: 0s - loss: 0.8991 - acc: 0.648 - ETA: 0s - loss: 0.8962 - acc: 0.649 - ETA: 0s - loss: 0.8949 - acc: 0.651 - ETA: 0s - loss: 0.9047 - acc: 0.644 - ETA: 0s - loss: 0.9080 - acc: 0.643 - 7s 5ms/step - loss: 0.9083 - acc: 0.6432 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.53316\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9158 - acc: 0.593 - ETA: 6s - loss: 1.0557 - acc: 0.578 - ETA: 5s - loss: 1.0275 - acc: 0.583 - ETA: 5s - loss: 1.0455 - acc: 0.578 - ETA: 5s - loss: 1.0378 - acc: 0.581 - ETA: 5s - loss: 1.0429 - acc: 0.588 - ETA: 5s - loss: 0.9832 - acc: 0.625 - ETA: 5s - loss: 0.9801 - acc: 0.621 - ETA: 5s - loss: 0.9629 - acc: 0.631 - ETA: 4s - loss: 0.9461 - acc: 0.646 - ETA: 4s - loss: 0.9246 - acc: 0.650 - ETA: 4s - loss: 0.9260 - acc: 0.653 - ETA: 4s - loss: 0.9287 - acc: 0.646 - ETA: 4s - loss: 0.9226 - acc: 0.654 - ETA: 4s - loss: 0.9360 - acc: 0.645 - ETA: 4s - loss: 0.9458 - acc: 0.640 - ETA: 4s - loss: 0.9453 - acc: 0.639 - ETA: 3s - loss: 0.9398 - acc: 0.642 - ETA: 3s - loss: 0.9303 - acc: 0.648 - ETA: 3s - loss: 0.9334 - acc: 0.646 - ETA: 3s - loss: 0.9296 - acc: 0.648 - ETA: 3s - loss: 0.9210 - acc: 0.656 - ETA: 3s - loss: 0.9177 - acc: 0.654 - ETA: 3s - loss: 0.9208 - acc: 0.652 - ETA: 2s - loss: 0.9248 - acc: 0.650 - ETA: 2s - loss: 0.9175 - acc: 0.653 - ETA: 2s - loss: 0.9203 - acc: 0.653 - ETA: 2s - loss: 0.9174 - acc: 0.655 - ETA: 2s - loss: 0.9227 - acc: 0.649 - ETA: 2s - loss: 0.9194 - acc: 0.650 - ETA: 2s - loss: 0.9202 - acc: 0.647 - ETA: 2s - loss: 0.9167 - acc: 0.650 - ETA: 1s - loss: 0.9116 - acc: 0.651 - ETA: 1s - loss: 0.9104 - acc: 0.651 - ETA: 1s - loss: 0.9048 - acc: 0.654 - ETA: 1s - loss: 0.9067 - acc: 0.651 - ETA: 1s - loss: 0.9103 - acc: 0.648 - ETA: 1s - loss: 0.9157 - acc: 0.647 - ETA: 1s - loss: 0.9153 - acc: 0.649 - ETA: 0s - loss: 0.9177 - acc: 0.646 - ETA: 0s - loss: 0.9170 - acc: 0.646 - ETA: 0s - loss: 0.9165 - acc: 0.647 - ETA: 0s - loss: 0.9146 - acc: 0.648 - ETA: 0s - loss: 0.9100 - acc: 0.649 - ETA: 0s - loss: 0.9101 - acc: 0.648 - ETA: 0s - loss: 0.9075 - acc: 0.648 - ETA: 0s - loss: 0.9059 - acc: 0.648 - 7s 5ms/step - loss: 0.9052 - acc: 0.6492 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.53316\n",
      "Epoch 81/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.7876 - acc: 0.750 - ETA: 6s - loss: 0.7418 - acc: 0.750 - ETA: 5s - loss: 0.8314 - acc: 0.697 - ETA: 5s - loss: 0.8021 - acc: 0.726 - ETA: 5s - loss: 0.8136 - acc: 0.712 - ETA: 5s - loss: 0.8139 - acc: 0.713 - ETA: 5s - loss: 0.8432 - acc: 0.696 - ETA: 5s - loss: 0.8632 - acc: 0.687 - ETA: 5s - loss: 0.8689 - acc: 0.684 - ETA: 5s - loss: 0.8679 - acc: 0.684 - ETA: 4s - loss: 0.8864 - acc: 0.667 - ETA: 4s - loss: 0.8946 - acc: 0.664 - ETA: 4s - loss: 0.8854 - acc: 0.665 - ETA: 4s - loss: 0.8923 - acc: 0.660 - ETA: 4s - loss: 0.8993 - acc: 0.654 - ETA: 4s - loss: 0.8959 - acc: 0.656 - ETA: 4s - loss: 0.8885 - acc: 0.661 - ETA: 3s - loss: 0.8801 - acc: 0.663 - ETA: 3s - loss: 0.8739 - acc: 0.666 - ETA: 3s - loss: 0.8881 - acc: 0.660 - ETA: 3s - loss: 0.8846 - acc: 0.660 - ETA: 3s - loss: 0.8888 - acc: 0.659 - ETA: 3s - loss: 0.8923 - acc: 0.657 - ETA: 3s - loss: 0.8940 - acc: 0.654 - ETA: 2s - loss: 0.8971 - acc: 0.652 - ETA: 2s - loss: 0.9022 - acc: 0.646 - ETA: 2s - loss: 0.9058 - acc: 0.641 - ETA: 2s - loss: 0.9084 - acc: 0.644 - ETA: 2s - loss: 0.9085 - acc: 0.644 - ETA: 2s - loss: 0.9070 - acc: 0.649 - ETA: 2s - loss: 0.9174 - acc: 0.645 - ETA: 2s - loss: 0.9089 - acc: 0.649 - ETA: 1s - loss: 0.9098 - acc: 0.646 - ETA: 1s - loss: 0.9139 - acc: 0.645 - ETA: 1s - loss: 0.9143 - acc: 0.642 - ETA: 1s - loss: 0.9117 - acc: 0.642 - ETA: 1s - loss: 0.9093 - acc: 0.643 - ETA: 1s - loss: 0.9079 - acc: 0.644 - ETA: 1s - loss: 0.9016 - acc: 0.648 - ETA: 0s - loss: 0.8982 - acc: 0.650 - ETA: 0s - loss: 0.8964 - acc: 0.650 - ETA: 0s - loss: 0.8951 - acc: 0.651 - ETA: 0s - loss: 0.8941 - acc: 0.651 - ETA: 0s - loss: 0.8931 - acc: 0.651 - ETA: 0s - loss: 0.8936 - acc: 0.650 - ETA: 0s - loss: 0.8944 - acc: 0.650 - ETA: 0s - loss: 0.8974 - acc: 0.648 - 7s 5ms/step - loss: 0.8973 - acc: 0.6499 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.53316\n",
      "Epoch 82/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9914 - acc: 0.625 - ETA: 6s - loss: 0.9486 - acc: 0.578 - ETA: 5s - loss: 0.9352 - acc: 0.614 - ETA: 5s - loss: 0.9921 - acc: 0.562 - ETA: 5s - loss: 0.9382 - acc: 0.581 - ETA: 5s - loss: 0.9055 - acc: 0.619 - ETA: 5s - loss: 0.9067 - acc: 0.616 - ETA: 5s - loss: 0.8836 - acc: 0.628 - ETA: 5s - loss: 0.8956 - acc: 0.625 - ETA: 5s - loss: 0.8883 - acc: 0.625 - ETA: 4s - loss: 0.8921 - acc: 0.627 - ETA: 4s - loss: 0.8967 - acc: 0.627 - ETA: 4s - loss: 0.8993 - acc: 0.629 - ETA: 4s - loss: 0.8821 - acc: 0.638 - ETA: 4s - loss: 0.8949 - acc: 0.627 - ETA: 4s - loss: 0.9023 - acc: 0.628 - ETA: 4s - loss: 0.8975 - acc: 0.630 - ETA: 3s - loss: 0.8976 - acc: 0.635 - ETA: 3s - loss: 0.9029 - acc: 0.634 - ETA: 3s - loss: 0.8944 - acc: 0.635 - ETA: 3s - loss: 0.8967 - acc: 0.635 - ETA: 3s - loss: 0.8920 - acc: 0.637 - ETA: 3s - loss: 0.9008 - acc: 0.641 - ETA: 3s - loss: 0.9068 - acc: 0.640 - ETA: 2s - loss: 0.8942 - acc: 0.647 - ETA: 2s - loss: 0.8950 - acc: 0.649 - ETA: 2s - loss: 0.8887 - acc: 0.655 - ETA: 2s - loss: 0.8848 - acc: 0.655 - ETA: 2s - loss: 0.8843 - acc: 0.656 - ETA: 2s - loss: 0.8848 - acc: 0.657 - ETA: 2s - loss: 0.8897 - acc: 0.654 - ETA: 2s - loss: 0.8885 - acc: 0.655 - ETA: 1s - loss: 0.8914 - acc: 0.654 - ETA: 1s - loss: 0.8921 - acc: 0.654 - ETA: 1s - loss: 0.8987 - acc: 0.651 - ETA: 1s - loss: 0.8948 - acc: 0.652 - ETA: 1s - loss: 0.9017 - acc: 0.649 - ETA: 1s - loss: 0.9058 - acc: 0.648 - ETA: 1s - loss: 0.9054 - acc: 0.647 - ETA: 0s - loss: 0.9097 - acc: 0.645 - ETA: 0s - loss: 0.9094 - acc: 0.646 - ETA: 0s - loss: 0.9099 - acc: 0.646 - ETA: 0s - loss: 0.9098 - acc: 0.646 - ETA: 0s - loss: 0.9063 - acc: 0.649 - ETA: 0s - loss: 0.9043 - acc: 0.650 - ETA: 0s - loss: 0.9016 - acc: 0.650 - ETA: 0s - loss: 0.9030 - acc: 0.650 - 7s 5ms/step - loss: 0.9040 - acc: 0.6499 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.53316\n",
      "Epoch 83/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.1299 - acc: 0.468 - ETA: 6s - loss: 1.1133 - acc: 0.468 - ETA: 5s - loss: 1.0105 - acc: 0.552 - ETA: 5s - loss: 0.9617 - acc: 0.593 - ETA: 5s - loss: 0.9837 - acc: 0.587 - ETA: 5s - loss: 0.9879 - acc: 0.599 - ETA: 5s - loss: 0.9934 - acc: 0.602 - ETA: 5s - loss: 0.9686 - acc: 0.609 - ETA: 5s - loss: 0.9891 - acc: 0.597 - ETA: 5s - loss: 0.9668 - acc: 0.612 - ETA: 4s - loss: 0.9686 - acc: 0.613 - ETA: 4s - loss: 0.9425 - acc: 0.632 - ETA: 4s - loss: 0.9386 - acc: 0.629 - ETA: 4s - loss: 0.9294 - acc: 0.631 - ETA: 4s - loss: 0.9313 - acc: 0.631 - ETA: 4s - loss: 0.9259 - acc: 0.638 - ETA: 4s - loss: 0.9256 - acc: 0.637 - ETA: 3s - loss: 0.9338 - acc: 0.637 - ETA: 3s - loss: 0.9300 - acc: 0.641 - ETA: 3s - loss: 0.9292 - acc: 0.640 - ETA: 3s - loss: 0.9353 - acc: 0.636 - ETA: 3s - loss: 0.9308 - acc: 0.636 - ETA: 3s - loss: 0.9246 - acc: 0.639 - ETA: 3s - loss: 0.9228 - acc: 0.644 - ETA: 2s - loss: 0.9248 - acc: 0.643 - ETA: 2s - loss: 0.9243 - acc: 0.643 - ETA: 2s - loss: 0.9240 - acc: 0.641 - ETA: 2s - loss: 0.9256 - acc: 0.637 - ETA: 2s - loss: 0.9312 - acc: 0.632 - ETA: 2s - loss: 0.9370 - acc: 0.633 - ETA: 2s - loss: 0.9374 - acc: 0.634 - ETA: 2s - loss: 0.9358 - acc: 0.637 - ETA: 1s - loss: 0.9325 - acc: 0.637 - ETA: 1s - loss: 0.9280 - acc: 0.637 - ETA: 1s - loss: 0.9345 - acc: 0.631 - ETA: 1s - loss: 0.9347 - acc: 0.632 - ETA: 1s - loss: 0.9353 - acc: 0.630 - ETA: 1s - loss: 0.9345 - acc: 0.630 - ETA: 1s - loss: 0.9308 - acc: 0.635 - ETA: 0s - loss: 0.9235 - acc: 0.637 - ETA: 0s - loss: 0.9221 - acc: 0.636 - ETA: 0s - loss: 0.9215 - acc: 0.636 - ETA: 0s - loss: 0.9221 - acc: 0.635 - ETA: 0s - loss: 0.9196 - acc: 0.636 - ETA: 0s - loss: 0.9180 - acc: 0.635 - ETA: 0s - loss: 0.9211 - acc: 0.635 - ETA: 0s - loss: 0.9191 - acc: 0.636 - 7s 5ms/step - loss: 0.9176 - acc: 0.6373 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.53316\n",
      "Epoch 84/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9495 - acc: 0.687 - ETA: 6s - loss: 0.9517 - acc: 0.656 - ETA: 5s - loss: 0.9683 - acc: 0.656 - ETA: 5s - loss: 0.9571 - acc: 0.656 - ETA: 5s - loss: 0.8817 - acc: 0.687 - ETA: 5s - loss: 0.8786 - acc: 0.682 - ETA: 5s - loss: 0.8685 - acc: 0.678 - ETA: 5s - loss: 0.8679 - acc: 0.671 - ETA: 5s - loss: 0.8663 - acc: 0.666 - ETA: 5s - loss: 0.8659 - acc: 0.656 - ETA: 4s - loss: 0.8773 - acc: 0.656 - ETA: 4s - loss: 0.8721 - acc: 0.661 - ETA: 4s - loss: 0.8605 - acc: 0.668 - ETA: 4s - loss: 0.8700 - acc: 0.669 - ETA: 4s - loss: 0.8675 - acc: 0.672 - ETA: 4s - loss: 0.8683 - acc: 0.673 - ETA: 4s - loss: 0.8749 - acc: 0.669 - ETA: 3s - loss: 0.8809 - acc: 0.664 - ETA: 3s - loss: 0.8898 - acc: 0.662 - ETA: 3s - loss: 0.8971 - acc: 0.656 - ETA: 3s - loss: 0.8917 - acc: 0.657 - ETA: 3s - loss: 0.8949 - acc: 0.659 - ETA: 3s - loss: 0.8881 - acc: 0.663 - ETA: 3s - loss: 0.8873 - acc: 0.664 - ETA: 3s - loss: 0.8857 - acc: 0.666 - ETA: 2s - loss: 0.8942 - acc: 0.662 - ETA: 2s - loss: 0.8951 - acc: 0.660 - ETA: 2s - loss: 0.8984 - acc: 0.658 - ETA: 2s - loss: 0.9078 - acc: 0.654 - ETA: 2s - loss: 0.9069 - acc: 0.653 - ETA: 2s - loss: 0.9070 - acc: 0.651 - ETA: 2s - loss: 0.9043 - acc: 0.652 - ETA: 1s - loss: 0.9074 - acc: 0.647 - ETA: 1s - loss: 0.9052 - acc: 0.648 - ETA: 1s - loss: 0.9118 - acc: 0.642 - ETA: 1s - loss: 0.9120 - acc: 0.645 - ETA: 1s - loss: 0.9142 - acc: 0.645 - ETA: 1s - loss: 0.9135 - acc: 0.646 - ETA: 1s - loss: 0.9112 - acc: 0.646 - ETA: 0s - loss: 0.9088 - acc: 0.646 - ETA: 0s - loss: 0.9077 - acc: 0.647 - ETA: 0s - loss: 0.9089 - acc: 0.647 - ETA: 0s - loss: 0.9080 - acc: 0.647 - ETA: 0s - loss: 0.9040 - acc: 0.649 - ETA: 0s - loss: 0.9027 - acc: 0.649 - ETA: 0s - loss: 0.9022 - acc: 0.650 - ETA: 0s - loss: 0.9051 - acc: 0.648 - 7s 5ms/step - loss: 0.9036 - acc: 0.6499 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.53316\n",
      "Epoch 85/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9194 - acc: 0.656 - ETA: 5s - loss: 0.9696 - acc: 0.625 - ETA: 5s - loss: 1.0075 - acc: 0.604 - ETA: 5s - loss: 0.9271 - acc: 0.625 - ETA: 5s - loss: 0.9278 - acc: 0.637 - ETA: 5s - loss: 0.8828 - acc: 0.651 - ETA: 5s - loss: 0.8992 - acc: 0.633 - ETA: 5s - loss: 0.9426 - acc: 0.605 - ETA: 5s - loss: 0.9346 - acc: 0.614 - ETA: 5s - loss: 0.9270 - acc: 0.625 - ETA: 4s - loss: 0.9164 - acc: 0.627 - ETA: 4s - loss: 0.9326 - acc: 0.627 - ETA: 4s - loss: 0.9257 - acc: 0.625 - ETA: 4s - loss: 0.9356 - acc: 0.616 - ETA: 4s - loss: 0.9332 - acc: 0.618 - ETA: 4s - loss: 0.9306 - acc: 0.619 - ETA: 4s - loss: 0.9337 - acc: 0.623 - ETA: 3s - loss: 0.9294 - acc: 0.625 - ETA: 3s - loss: 0.9212 - acc: 0.628 - ETA: 3s - loss: 0.9179 - acc: 0.634 - ETA: 3s - loss: 0.9157 - acc: 0.635 - ETA: 3s - loss: 0.9238 - acc: 0.626 - ETA: 3s - loss: 0.9110 - acc: 0.634 - ETA: 3s - loss: 0.9061 - acc: 0.632 - ETA: 3s - loss: 0.9103 - acc: 0.628 - ETA: 2s - loss: 0.9108 - acc: 0.629 - ETA: 2s - loss: 0.9003 - acc: 0.634 - ETA: 2s - loss: 0.8976 - acc: 0.632 - ETA: 2s - loss: 0.8980 - acc: 0.631 - ETA: 2s - loss: 0.8995 - acc: 0.628 - ETA: 2s - loss: 0.8957 - acc: 0.632 - ETA: 2s - loss: 0.8910 - acc: 0.637 - ETA: 1s - loss: 0.8910 - acc: 0.641 - ETA: 1s - loss: 0.8929 - acc: 0.641 - ETA: 1s - loss: 0.8881 - acc: 0.645 - ETA: 1s - loss: 0.8848 - acc: 0.645 - ETA: 1s - loss: 0.8842 - acc: 0.647 - ETA: 1s - loss: 0.8861 - acc: 0.648 - ETA: 1s - loss: 0.8882 - acc: 0.649 - ETA: 0s - loss: 0.8940 - acc: 0.644 - ETA: 0s - loss: 0.9007 - acc: 0.641 - ETA: 0s - loss: 0.8990 - acc: 0.641 - ETA: 0s - loss: 0.9044 - acc: 0.640 - ETA: 0s - loss: 0.9035 - acc: 0.639 - ETA: 0s - loss: 0.9066 - acc: 0.636 - ETA: 0s - loss: 0.9062 - acc: 0.640 - ETA: 0s - loss: 0.9086 - acc: 0.638 - 7s 5ms/step - loss: 0.9094 - acc: 0.6373 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.53316\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "Epoch 86/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.1002 - acc: 0.500 - ETA: 5s - loss: 1.0139 - acc: 0.562 - ETA: 5s - loss: 1.0061 - acc: 0.583 - ETA: 5s - loss: 0.9844 - acc: 0.593 - ETA: 5s - loss: 0.9589 - acc: 0.606 - ETA: 5s - loss: 0.9237 - acc: 0.625 - ETA: 5s - loss: 0.9155 - acc: 0.629 - ETA: 5s - loss: 0.8875 - acc: 0.636 - ETA: 5s - loss: 0.8911 - acc: 0.642 - ETA: 5s - loss: 0.8916 - acc: 0.640 - ETA: 4s - loss: 0.9125 - acc: 0.636 - ETA: 4s - loss: 0.9123 - acc: 0.635 - ETA: 4s - loss: 0.9162 - acc: 0.627 - ETA: 4s - loss: 0.9123 - acc: 0.633 - ETA: 4s - loss: 0.9035 - acc: 0.641 - ETA: 4s - loss: 0.9030 - acc: 0.644 - ETA: 4s - loss: 0.9043 - acc: 0.641 - ETA: 3s - loss: 0.9042 - acc: 0.638 - ETA: 3s - loss: 0.9047 - acc: 0.639 - ETA: 3s - loss: 0.9084 - acc: 0.635 - ETA: 3s - loss: 0.9035 - acc: 0.641 - ETA: 3s - loss: 0.8940 - acc: 0.639 - ETA: 3s - loss: 0.8997 - acc: 0.638 - ETA: 3s - loss: 0.8938 - acc: 0.636 - ETA: 2s - loss: 0.9019 - acc: 0.631 - ETA: 2s - loss: 0.9034 - acc: 0.633 - ETA: 2s - loss: 0.8991 - acc: 0.633 - ETA: 2s - loss: 0.9084 - acc: 0.630 - ETA: 2s - loss: 0.9050 - acc: 0.631 - ETA: 2s - loss: 0.8978 - acc: 0.636 - ETA: 2s - loss: 0.8944 - acc: 0.639 - ETA: 2s - loss: 0.9000 - acc: 0.637 - ETA: 1s - loss: 0.9014 - acc: 0.634 - ETA: 1s - loss: 0.9026 - acc: 0.636 - ETA: 1s - loss: 0.9014 - acc: 0.634 - ETA: 1s - loss: 0.9004 - acc: 0.635 - ETA: 1s - loss: 0.8967 - acc: 0.641 - ETA: 1s - loss: 0.8954 - acc: 0.642 - ETA: 1s - loss: 0.8970 - acc: 0.641 - ETA: 0s - loss: 0.9039 - acc: 0.639 - ETA: 0s - loss: 0.9052 - acc: 0.638 - ETA: 0s - loss: 0.9041 - acc: 0.638 - ETA: 0s - loss: 0.9050 - acc: 0.638 - ETA: 0s - loss: 0.9037 - acc: 0.639 - ETA: 0s - loss: 0.9053 - acc: 0.637 - ETA: 0s - loss: 0.9056 - acc: 0.637 - ETA: 0s - loss: 0.9072 - acc: 0.636 - 7s 5ms/step - loss: 0.9077 - acc: 0.6353 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.53316\n",
      "Epoch 87/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9141 - acc: 0.687 - ETA: 6s - loss: 0.8898 - acc: 0.671 - ETA: 5s - loss: 0.8487 - acc: 0.687 - ETA: 5s - loss: 0.8692 - acc: 0.671 - ETA: 5s - loss: 0.8724 - acc: 0.656 - ETA: 5s - loss: 0.8785 - acc: 0.677 - ETA: 5s - loss: 0.8631 - acc: 0.692 - ETA: 5s - loss: 0.8545 - acc: 0.683 - ETA: 5s - loss: 0.8619 - acc: 0.687 - ETA: 5s - loss: 0.8716 - acc: 0.671 - ETA: 4s - loss: 0.8811 - acc: 0.667 - ETA: 4s - loss: 0.8791 - acc: 0.669 - ETA: 4s - loss: 0.8670 - acc: 0.677 - ETA: 4s - loss: 0.8781 - acc: 0.671 - ETA: 4s - loss: 0.8740 - acc: 0.672 - ETA: 4s - loss: 0.8710 - acc: 0.669 - ETA: 4s - loss: 0.8767 - acc: 0.667 - ETA: 3s - loss: 0.8782 - acc: 0.659 - ETA: 3s - loss: 0.8765 - acc: 0.654 - ETA: 3s - loss: 0.8650 - acc: 0.659 - ETA: 3s - loss: 0.8651 - acc: 0.660 - ETA: 3s - loss: 0.8551 - acc: 0.666 - ETA: 3s - loss: 0.8580 - acc: 0.668 - ETA: 3s - loss: 0.8640 - acc: 0.666 - ETA: 3s - loss: 0.8598 - acc: 0.671 - ETA: 2s - loss: 0.8656 - acc: 0.669 - ETA: 2s - loss: 0.8677 - acc: 0.667 - ETA: 2s - loss: 0.8752 - acc: 0.664 - ETA: 2s - loss: 0.8796 - acc: 0.661 - ETA: 2s - loss: 0.8791 - acc: 0.661 - ETA: 2s - loss: 0.8874 - acc: 0.661 - ETA: 2s - loss: 0.8833 - acc: 0.663 - ETA: 1s - loss: 0.8778 - acc: 0.665 - ETA: 1s - loss: 0.8808 - acc: 0.666 - ETA: 1s - loss: 0.8780 - acc: 0.666 - ETA: 1s - loss: 0.8746 - acc: 0.668 - ETA: 1s - loss: 0.8734 - acc: 0.669 - ETA: 1s - loss: 0.8749 - acc: 0.669 - ETA: 1s - loss: 0.8733 - acc: 0.667 - ETA: 0s - loss: 0.8691 - acc: 0.668 - ETA: 0s - loss: 0.8698 - acc: 0.670 - ETA: 0s - loss: 0.8748 - acc: 0.666 - ETA: 0s - loss: 0.8805 - acc: 0.662 - ETA: 0s - loss: 0.8803 - acc: 0.663 - ETA: 0s - loss: 0.8797 - acc: 0.663 - ETA: 0s - loss: 0.8852 - acc: 0.657 - ETA: 0s - loss: 0.8860 - acc: 0.656 - 7s 5ms/step - loss: 0.8869 - acc: 0.6572 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.53316\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 1.0230 - acc: 0.625 - ETA: 6s - loss: 1.0367 - acc: 0.640 - ETA: 5s - loss: 0.9413 - acc: 0.666 - ETA: 5s - loss: 0.9100 - acc: 0.671 - ETA: 5s - loss: 0.9451 - acc: 0.662 - ETA: 5s - loss: 0.9445 - acc: 0.666 - ETA: 5s - loss: 0.9973 - acc: 0.633 - ETA: 5s - loss: 0.9852 - acc: 0.628 - ETA: 5s - loss: 0.9633 - acc: 0.635 - ETA: 4s - loss: 0.9647 - acc: 0.631 - ETA: 4s - loss: 0.9456 - acc: 0.642 - ETA: 4s - loss: 0.9609 - acc: 0.635 - ETA: 4s - loss: 0.9689 - acc: 0.622 - ETA: 4s - loss: 0.9755 - acc: 0.627 - ETA: 4s - loss: 0.9600 - acc: 0.635 - ETA: 4s - loss: 0.9609 - acc: 0.628 - ETA: 4s - loss: 0.9627 - acc: 0.621 - ETA: 3s - loss: 0.9423 - acc: 0.631 - ETA: 3s - loss: 0.9277 - acc: 0.638 - ETA: 3s - loss: 0.9280 - acc: 0.631 - ETA: 3s - loss: 0.9293 - acc: 0.629 - ETA: 3s - loss: 0.9255 - acc: 0.632 - ETA: 3s - loss: 0.9182 - acc: 0.637 - ETA: 3s - loss: 0.9125 - acc: 0.640 - ETA: 2s - loss: 0.9090 - acc: 0.643 - ETA: 2s - loss: 0.9095 - acc: 0.643 - ETA: 2s - loss: 0.9100 - acc: 0.643 - ETA: 2s - loss: 0.9083 - acc: 0.645 - ETA: 2s - loss: 0.9053 - acc: 0.643 - ETA: 2s - loss: 0.9061 - acc: 0.640 - ETA: 2s - loss: 0.9015 - acc: 0.641 - ETA: 2s - loss: 0.9012 - acc: 0.639 - ETA: 1s - loss: 0.8948 - acc: 0.640 - ETA: 1s - loss: 0.8970 - acc: 0.634 - ETA: 1s - loss: 0.8970 - acc: 0.633 - ETA: 1s - loss: 0.8960 - acc: 0.634 - ETA: 1s - loss: 0.8961 - acc: 0.636 - ETA: 1s - loss: 0.8980 - acc: 0.636 - ETA: 1s - loss: 0.9031 - acc: 0.634 - ETA: 0s - loss: 0.8997 - acc: 0.635 - ETA: 0s - loss: 0.9017 - acc: 0.635 - ETA: 0s - loss: 0.9019 - acc: 0.637 - ETA: 0s - loss: 0.9052 - acc: 0.633 - ETA: 0s - loss: 0.9100 - acc: 0.630 - ETA: 0s - loss: 0.9094 - acc: 0.631 - ETA: 0s - loss: 0.9129 - acc: 0.630 - ETA: 0s - loss: 0.9089 - acc: 0.632 - 7s 5ms/step - loss: 0.9092 - acc: 0.6320 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.53316\n",
      "Epoch 89/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9132 - acc: 0.593 - ETA: 5s - loss: 1.0433 - acc: 0.531 - ETA: 5s - loss: 0.9709 - acc: 0.635 - ETA: 5s - loss: 0.9518 - acc: 0.632 - ETA: 5s - loss: 0.9308 - acc: 0.643 - ETA: 5s - loss: 0.9067 - acc: 0.645 - ETA: 5s - loss: 0.9371 - acc: 0.625 - ETA: 5s - loss: 0.9182 - acc: 0.640 - ETA: 5s - loss: 0.8992 - acc: 0.649 - ETA: 4s - loss: 0.8942 - acc: 0.659 - ETA: 4s - loss: 0.8936 - acc: 0.656 - ETA: 4s - loss: 0.9092 - acc: 0.648 - ETA: 4s - loss: 0.9122 - acc: 0.644 - ETA: 4s - loss: 0.9213 - acc: 0.636 - ETA: 4s - loss: 0.9139 - acc: 0.639 - ETA: 4s - loss: 0.9069 - acc: 0.642 - ETA: 4s - loss: 0.9032 - acc: 0.643 - ETA: 3s - loss: 0.9090 - acc: 0.645 - ETA: 3s - loss: 0.9072 - acc: 0.649 - ETA: 3s - loss: 0.9034 - acc: 0.650 - ETA: 3s - loss: 0.9031 - acc: 0.654 - ETA: 3s - loss: 0.9025 - acc: 0.653 - ETA: 3s - loss: 0.9105 - acc: 0.649 - ETA: 3s - loss: 0.9034 - acc: 0.652 - ETA: 2s - loss: 0.9060 - acc: 0.651 - ETA: 2s - loss: 0.9060 - acc: 0.653 - ETA: 2s - loss: 0.9081 - acc: 0.651 - ETA: 2s - loss: 0.9071 - acc: 0.657 - ETA: 2s - loss: 0.9054 - acc: 0.657 - ETA: 2s - loss: 0.9192 - acc: 0.651 - ETA: 2s - loss: 0.9121 - acc: 0.653 - ETA: 2s - loss: 0.9097 - acc: 0.653 - ETA: 1s - loss: 0.9107 - acc: 0.653 - ETA: 1s - loss: 0.9125 - acc: 0.651 - ETA: 1s - loss: 0.9125 - acc: 0.650 - ETA: 1s - loss: 0.9058 - acc: 0.656 - ETA: 1s - loss: 0.9086 - acc: 0.657 - ETA: 1s - loss: 0.9102 - acc: 0.655 - ETA: 1s - loss: 0.9123 - acc: 0.653 - ETA: 0s - loss: 0.9097 - acc: 0.655 - ETA: 0s - loss: 0.9016 - acc: 0.660 - ETA: 0s - loss: 0.9008 - acc: 0.660 - ETA: 0s - loss: 0.9029 - acc: 0.660 - ETA: 0s - loss: 0.9090 - acc: 0.657 - ETA: 0s - loss: 0.9049 - acc: 0.659 - ETA: 0s - loss: 0.9057 - acc: 0.658 - ETA: 0s - loss: 0.9036 - acc: 0.658 - 7s 5ms/step - loss: 0.9028 - acc: 0.6585 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.53316\n",
      "Epoch 90/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9646 - acc: 0.593 - ETA: 5s - loss: 0.8896 - acc: 0.671 - ETA: 5s - loss: 0.8821 - acc: 0.677 - ETA: 5s - loss: 0.8557 - acc: 0.687 - ETA: 5s - loss: 0.8563 - acc: 0.687 - ETA: 5s - loss: 0.8739 - acc: 0.677 - ETA: 5s - loss: 0.9234 - acc: 0.638 - ETA: 5s - loss: 0.9370 - acc: 0.636 - ETA: 5s - loss: 0.9280 - acc: 0.638 - ETA: 4s - loss: 0.9251 - acc: 0.637 - ETA: 4s - loss: 0.9194 - acc: 0.642 - ETA: 4s - loss: 0.9275 - acc: 0.638 - ETA: 4s - loss: 0.9188 - acc: 0.644 - ETA: 4s - loss: 0.9146 - acc: 0.642 - ETA: 4s - loss: 0.9143 - acc: 0.652 - ETA: 4s - loss: 0.9152 - acc: 0.654 - ETA: 4s - loss: 0.9171 - acc: 0.654 - ETA: 3s - loss: 0.9208 - acc: 0.652 - ETA: 3s - loss: 0.9207 - acc: 0.649 - ETA: 3s - loss: 0.9167 - acc: 0.648 - ETA: 3s - loss: 0.9228 - acc: 0.642 - ETA: 3s - loss: 0.9324 - acc: 0.639 - ETA: 3s - loss: 0.9242 - acc: 0.639 - ETA: 3s - loss: 0.9245 - acc: 0.639 - ETA: 2s - loss: 0.9221 - acc: 0.641 - ETA: 2s - loss: 0.9278 - acc: 0.638 - ETA: 2s - loss: 0.9245 - acc: 0.638 - ETA: 2s - loss: 0.9224 - acc: 0.642 - ETA: 2s - loss: 0.9225 - acc: 0.641 - ETA: 2s - loss: 0.9221 - acc: 0.641 - ETA: 2s - loss: 0.9178 - acc: 0.642 - ETA: 2s - loss: 0.9199 - acc: 0.640 - ETA: 1s - loss: 0.9177 - acc: 0.641 - ETA: 1s - loss: 0.9150 - acc: 0.641 - ETA: 1s - loss: 0.9161 - acc: 0.642 - ETA: 1s - loss: 0.9132 - acc: 0.647 - ETA: 1s - loss: 0.9055 - acc: 0.651 - ETA: 1s - loss: 0.9022 - acc: 0.653 - ETA: 1s - loss: 0.9066 - acc: 0.653 - ETA: 0s - loss: 0.9063 - acc: 0.653 - ETA: 0s - loss: 0.8980 - acc: 0.658 - ETA: 0s - loss: 0.8951 - acc: 0.660 - ETA: 0s - loss: 0.8959 - acc: 0.658 - ETA: 0s - loss: 0.9016 - acc: 0.653 - ETA: 0s - loss: 0.9055 - acc: 0.647 - ETA: 0s - loss: 0.9106 - acc: 0.644 - ETA: 0s - loss: 0.9148 - acc: 0.642 - 7s 5ms/step - loss: 0.9128 - acc: 0.6432 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.53316\n",
      "Epoch 91/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 1.2021 - acc: 0.531 - ETA: 5s - loss: 1.2422 - acc: 0.500 - ETA: 5s - loss: 1.0734 - acc: 0.572 - ETA: 5s - loss: 1.0360 - acc: 0.570 - ETA: 5s - loss: 0.9932 - acc: 0.600 - ETA: 5s - loss: 1.0005 - acc: 0.583 - ETA: 5s - loss: 0.9887 - acc: 0.584 - ETA: 5s - loss: 0.9592 - acc: 0.609 - ETA: 5s - loss: 0.9544 - acc: 0.604 - ETA: 4s - loss: 0.9473 - acc: 0.606 - ETA: 4s - loss: 0.9316 - acc: 0.622 - ETA: 4s - loss: 0.9146 - acc: 0.632 - ETA: 4s - loss: 0.9247 - acc: 0.629 - ETA: 4s - loss: 0.9147 - acc: 0.638 - ETA: 4s - loss: 0.9054 - acc: 0.639 - ETA: 4s - loss: 0.9132 - acc: 0.632 - ETA: 4s - loss: 0.9039 - acc: 0.636 - ETA: 3s - loss: 0.9178 - acc: 0.630 - ETA: 3s - loss: 0.9182 - acc: 0.633 - ETA: 3s - loss: 0.9180 - acc: 0.635 - ETA: 3s - loss: 0.9062 - acc: 0.644 - ETA: 3s - loss: 0.9043 - acc: 0.646 - ETA: 3s - loss: 0.9028 - acc: 0.645 - ETA: 3s - loss: 0.9036 - acc: 0.644 - ETA: 2s - loss: 0.8988 - acc: 0.646 - ETA: 2s - loss: 0.8950 - acc: 0.645 - ETA: 2s - loss: 0.8969 - acc: 0.642 - ETA: 2s - loss: 0.8947 - acc: 0.646 - ETA: 2s - loss: 0.8951 - acc: 0.644 - ETA: 2s - loss: 0.8961 - acc: 0.643 - ETA: 2s - loss: 0.8926 - acc: 0.643 - ETA: 2s - loss: 0.8916 - acc: 0.642 - ETA: 1s - loss: 0.8897 - acc: 0.644 - ETA: 1s - loss: 0.8949 - acc: 0.643 - ETA: 1s - loss: 0.8907 - acc: 0.645 - ETA: 1s - loss: 0.8941 - acc: 0.645 - ETA: 1s - loss: 0.8945 - acc: 0.647 - ETA: 1s - loss: 0.8936 - acc: 0.648 - ETA: 1s - loss: 0.8917 - acc: 0.649 - ETA: 0s - loss: 0.8962 - acc: 0.643 - ETA: 0s - loss: 0.8938 - acc: 0.644 - ETA: 0s - loss: 0.8939 - acc: 0.644 - ETA: 0s - loss: 0.8952 - acc: 0.643 - ETA: 0s - loss: 0.8996 - acc: 0.641 - ETA: 0s - loss: 0.8987 - acc: 0.640 - ETA: 0s - loss: 0.8995 - acc: 0.639 - ETA: 0s - loss: 0.9037 - acc: 0.638 - 7s 5ms/step - loss: 0.9044 - acc: 0.6379 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.53316\n",
      "Epoch 92/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8536 - acc: 0.593 - ETA: 6s - loss: 0.9666 - acc: 0.515 - ETA: 5s - loss: 0.9199 - acc: 0.593 - ETA: 5s - loss: 0.9282 - acc: 0.601 - ETA: 5s - loss: 0.9531 - acc: 0.600 - ETA: 5s - loss: 0.9129 - acc: 0.599 - ETA: 5s - loss: 0.8939 - acc: 0.607 - ETA: 5s - loss: 0.9089 - acc: 0.605 - ETA: 5s - loss: 0.9090 - acc: 0.600 - ETA: 5s - loss: 0.8991 - acc: 0.612 - ETA: 4s - loss: 0.8973 - acc: 0.627 - ETA: 4s - loss: 0.9041 - acc: 0.622 - ETA: 4s - loss: 0.9009 - acc: 0.629 - ETA: 4s - loss: 0.9089 - acc: 0.627 - ETA: 4s - loss: 0.9129 - acc: 0.627 - ETA: 4s - loss: 0.9106 - acc: 0.623 - ETA: 4s - loss: 0.9081 - acc: 0.626 - ETA: 3s - loss: 0.8880 - acc: 0.637 - ETA: 3s - loss: 0.8850 - acc: 0.636 - ETA: 3s - loss: 0.8813 - acc: 0.635 - ETA: 3s - loss: 0.8840 - acc: 0.638 - ETA: 3s - loss: 0.8773 - acc: 0.642 - ETA: 3s - loss: 0.8836 - acc: 0.638 - ETA: 3s - loss: 0.8816 - acc: 0.640 - ETA: 2s - loss: 0.8768 - acc: 0.641 - ETA: 2s - loss: 0.8864 - acc: 0.638 - ETA: 2s - loss: 0.8841 - acc: 0.638 - ETA: 2s - loss: 0.8907 - acc: 0.633 - ETA: 2s - loss: 0.8830 - acc: 0.639 - ETA: 2s - loss: 0.8909 - acc: 0.637 - ETA: 2s - loss: 0.8860 - acc: 0.640 - ETA: 2s - loss: 0.8852 - acc: 0.643 - ETA: 1s - loss: 0.8854 - acc: 0.642 - ETA: 1s - loss: 0.8851 - acc: 0.646 - ETA: 1s - loss: 0.8860 - acc: 0.645 - ETA: 1s - loss: 0.8918 - acc: 0.645 - ETA: 1s - loss: 0.8910 - acc: 0.646 - ETA: 1s - loss: 0.8992 - acc: 0.641 - ETA: 1s - loss: 0.8989 - acc: 0.640 - ETA: 0s - loss: 0.8938 - acc: 0.639 - ETA: 0s - loss: 0.8922 - acc: 0.641 - ETA: 0s - loss: 0.8982 - acc: 0.640 - ETA: 0s - loss: 0.8964 - acc: 0.641 - ETA: 0s - loss: 0.8987 - acc: 0.639 - ETA: 0s - loss: 0.8961 - acc: 0.641 - ETA: 0s - loss: 0.8966 - acc: 0.642 - ETA: 0s - loss: 0.8966 - acc: 0.642 - 7s 5ms/step - loss: 0.8969 - acc: 0.6426 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.53316\n",
      "\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "Epoch 93/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 0.7275 - acc: 0.750 - ETA: 5s - loss: 0.7415 - acc: 0.718 - ETA: 5s - loss: 0.8013 - acc: 0.708 - ETA: 5s - loss: 0.8129 - acc: 0.718 - ETA: 5s - loss: 0.8495 - acc: 0.700 - ETA: 5s - loss: 0.8693 - acc: 0.692 - ETA: 5s - loss: 0.8459 - acc: 0.705 - ETA: 5s - loss: 0.8445 - acc: 0.703 - ETA: 5s - loss: 0.8655 - acc: 0.684 - ETA: 4s - loss: 0.8834 - acc: 0.659 - ETA: 4s - loss: 0.8727 - acc: 0.659 - ETA: 4s - loss: 0.8957 - acc: 0.648 - ETA: 4s - loss: 0.9187 - acc: 0.639 - ETA: 4s - loss: 0.9280 - acc: 0.633 - ETA: 4s - loss: 0.9108 - acc: 0.641 - ETA: 4s - loss: 0.9111 - acc: 0.640 - ETA: 4s - loss: 0.9045 - acc: 0.639 - ETA: 3s - loss: 0.9060 - acc: 0.638 - ETA: 3s - loss: 0.9164 - acc: 0.634 - ETA: 3s - loss: 0.9356 - acc: 0.628 - ETA: 3s - loss: 0.9319 - acc: 0.626 - ETA: 3s - loss: 0.9242 - acc: 0.633 - ETA: 3s - loss: 0.9248 - acc: 0.631 - ETA: 3s - loss: 0.9252 - acc: 0.627 - ETA: 2s - loss: 0.9249 - acc: 0.627 - ETA: 2s - loss: 0.9229 - acc: 0.627 - ETA: 2s - loss: 0.9176 - acc: 0.631 - ETA: 2s - loss: 0.9160 - acc: 0.633 - ETA: 2s - loss: 0.9094 - acc: 0.640 - ETA: 2s - loss: 0.9120 - acc: 0.638 - ETA: 2s - loss: 0.9126 - acc: 0.639 - ETA: 2s - loss: 0.9112 - acc: 0.636 - ETA: 1s - loss: 0.9147 - acc: 0.639 - ETA: 1s - loss: 0.9122 - acc: 0.640 - ETA: 1s - loss: 0.9096 - acc: 0.640 - ETA: 1s - loss: 0.9096 - acc: 0.638 - ETA: 1s - loss: 0.9058 - acc: 0.641 - ETA: 1s - loss: 0.9044 - acc: 0.642 - ETA: 1s - loss: 0.9047 - acc: 0.641 - ETA: 0s - loss: 0.9040 - acc: 0.642 - ETA: 0s - loss: 0.9101 - acc: 0.640 - ETA: 0s - loss: 0.9103 - acc: 0.639 - ETA: 0s - loss: 0.9094 - acc: 0.640 - ETA: 0s - loss: 0.9077 - acc: 0.641 - ETA: 0s - loss: 0.9042 - acc: 0.643 - ETA: 0s - loss: 0.9085 - acc: 0.640 - ETA: 0s - loss: 0.9039 - acc: 0.645 - 7s 5ms/step - loss: 0.9035 - acc: 0.6459 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.53316\n",
      "Epoch 94/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9802 - acc: 0.562 - ETA: 5s - loss: 0.8470 - acc: 0.656 - ETA: 5s - loss: 0.8301 - acc: 0.666 - ETA: 5s - loss: 0.7986 - acc: 0.687 - ETA: 5s - loss: 0.8585 - acc: 0.662 - ETA: 5s - loss: 0.8617 - acc: 0.666 - ETA: 5s - loss: 0.8706 - acc: 0.660 - ETA: 5s - loss: 0.8887 - acc: 0.656 - ETA: 5s - loss: 0.8629 - acc: 0.663 - ETA: 4s - loss: 0.8680 - acc: 0.656 - ETA: 4s - loss: 0.8560 - acc: 0.664 - ETA: 4s - loss: 0.8553 - acc: 0.669 - ETA: 4s - loss: 0.8611 - acc: 0.665 - ETA: 4s - loss: 0.8611 - acc: 0.658 - ETA: 4s - loss: 0.8685 - acc: 0.652 - ETA: 4s - loss: 0.8606 - acc: 0.654 - ETA: 4s - loss: 0.8734 - acc: 0.645 - ETA: 3s - loss: 0.8950 - acc: 0.638 - ETA: 3s - loss: 0.9028 - acc: 0.634 - ETA: 3s - loss: 0.9106 - acc: 0.634 - ETA: 3s - loss: 0.9057 - acc: 0.635 - ETA: 3s - loss: 0.9155 - acc: 0.627 - ETA: 3s - loss: 0.9163 - acc: 0.626 - ETA: 3s - loss: 0.9170 - acc: 0.625 - ETA: 2s - loss: 0.9111 - acc: 0.628 - ETA: 2s - loss: 0.9049 - acc: 0.633 - ETA: 2s - loss: 0.9092 - acc: 0.629 - ETA: 2s - loss: 0.9079 - acc: 0.628 - ETA: 2s - loss: 0.9106 - acc: 0.628 - ETA: 2s - loss: 0.9054 - acc: 0.631 - ETA: 2s - loss: 0.9029 - acc: 0.637 - ETA: 2s - loss: 0.9054 - acc: 0.634 - ETA: 1s - loss: 0.9117 - acc: 0.631 - ETA: 1s - loss: 0.9088 - acc: 0.634 - ETA: 1s - loss: 0.9056 - acc: 0.635 - ETA: 1s - loss: 0.9047 - acc: 0.635 - ETA: 1s - loss: 0.9082 - acc: 0.632 - ETA: 1s - loss: 0.9053 - acc: 0.632 - ETA: 1s - loss: 0.9037 - acc: 0.631 - ETA: 0s - loss: 0.9064 - acc: 0.631 - ETA: 0s - loss: 0.9011 - acc: 0.635 - ETA: 0s - loss: 0.9011 - acc: 0.638 - ETA: 0s - loss: 0.9009 - acc: 0.637 - ETA: 0s - loss: 0.8972 - acc: 0.639 - ETA: 0s - loss: 0.8979 - acc: 0.638 - ETA: 0s - loss: 0.9006 - acc: 0.635 - ETA: 0s - loss: 0.8993 - acc: 0.637 - 7s 5ms/step - loss: 0.8997 - acc: 0.6366 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.53316\n",
      "Epoch 95/100\n",
      "1508/1508 [==============================] - ETA: 5s - loss: 0.8953 - acc: 0.656 - ETA: 5s - loss: 0.9044 - acc: 0.703 - ETA: 5s - loss: 0.8490 - acc: 0.708 - ETA: 5s - loss: 0.8082 - acc: 0.742 - ETA: 5s - loss: 0.8298 - acc: 0.712 - ETA: 5s - loss: 0.8227 - acc: 0.724 - ETA: 5s - loss: 0.8236 - acc: 0.718 - ETA: 5s - loss: 0.8317 - acc: 0.707 - ETA: 5s - loss: 0.8270 - acc: 0.708 - ETA: 4s - loss: 0.8322 - acc: 0.700 - ETA: 4s - loss: 0.8431 - acc: 0.696 - ETA: 4s - loss: 0.8758 - acc: 0.671 - ETA: 4s - loss: 0.8525 - acc: 0.680 - ETA: 4s - loss: 0.8576 - acc: 0.671 - ETA: 4s - loss: 0.8469 - acc: 0.670 - ETA: 4s - loss: 0.8589 - acc: 0.666 - ETA: 4s - loss: 0.8675 - acc: 0.661 - ETA: 3s - loss: 0.8715 - acc: 0.658 - ETA: 3s - loss: 0.8762 - acc: 0.653 - ETA: 3s - loss: 0.8820 - acc: 0.651 - ETA: 3s - loss: 0.8934 - acc: 0.647 - ETA: 3s - loss: 0.8903 - acc: 0.650 - ETA: 3s - loss: 0.8863 - acc: 0.646 - ETA: 3s - loss: 0.8866 - acc: 0.644 - ETA: 2s - loss: 0.8884 - acc: 0.642 - ETA: 2s - loss: 0.8850 - acc: 0.641 - ETA: 2s - loss: 0.8907 - acc: 0.638 - ETA: 2s - loss: 0.8921 - acc: 0.639 - ETA: 2s - loss: 0.8941 - acc: 0.637 - ETA: 2s - loss: 0.8915 - acc: 0.638 - ETA: 2s - loss: 0.8949 - acc: 0.637 - ETA: 2s - loss: 0.8921 - acc: 0.640 - ETA: 1s - loss: 0.8982 - acc: 0.635 - ETA: 1s - loss: 0.9010 - acc: 0.632 - ETA: 1s - loss: 0.9034 - acc: 0.633 - ETA: 1s - loss: 0.9030 - acc: 0.635 - ETA: 1s - loss: 0.9077 - acc: 0.635 - ETA: 1s - loss: 0.9118 - acc: 0.634 - ETA: 1s - loss: 0.9104 - acc: 0.634 - ETA: 0s - loss: 0.9102 - acc: 0.633 - ETA: 0s - loss: 0.9058 - acc: 0.635 - ETA: 0s - loss: 0.9018 - acc: 0.638 - ETA: 0s - loss: 0.9046 - acc: 0.636 - ETA: 0s - loss: 0.9071 - acc: 0.634 - ETA: 0s - loss: 0.9035 - acc: 0.636 - ETA: 0s - loss: 0.9036 - acc: 0.636 - ETA: 0s - loss: 0.9044 - acc: 0.634 - 7s 5ms/step - loss: 0.9036 - acc: 0.6346 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.53316\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1508/1508 [==============================] - ETA: 6s - loss: 0.6800 - acc: 0.812 - ETA: 5s - loss: 0.8440 - acc: 0.687 - ETA: 5s - loss: 0.8412 - acc: 0.687 - ETA: 5s - loss: 0.9095 - acc: 0.648 - ETA: 5s - loss: 0.9244 - acc: 0.650 - ETA: 5s - loss: 0.8934 - acc: 0.661 - ETA: 5s - loss: 0.8920 - acc: 0.669 - ETA: 5s - loss: 0.8897 - acc: 0.656 - ETA: 5s - loss: 0.9027 - acc: 0.645 - ETA: 4s - loss: 0.8930 - acc: 0.650 - ETA: 4s - loss: 0.8897 - acc: 0.647 - ETA: 4s - loss: 0.8973 - acc: 0.648 - ETA: 4s - loss: 0.9012 - acc: 0.644 - ETA: 4s - loss: 0.9098 - acc: 0.645 - ETA: 4s - loss: 0.9121 - acc: 0.639 - ETA: 4s - loss: 0.9079 - acc: 0.642 - ETA: 4s - loss: 0.8943 - acc: 0.652 - ETA: 3s - loss: 0.8897 - acc: 0.652 - ETA: 3s - loss: 0.8929 - acc: 0.649 - ETA: 3s - loss: 0.8994 - acc: 0.645 - ETA: 3s - loss: 0.9101 - acc: 0.638 - ETA: 3s - loss: 0.9085 - acc: 0.642 - ETA: 3s - loss: 0.9178 - acc: 0.639 - ETA: 3s - loss: 0.9211 - acc: 0.638 - ETA: 2s - loss: 0.9231 - acc: 0.637 - ETA: 2s - loss: 0.9199 - acc: 0.639 - ETA: 2s - loss: 0.9185 - acc: 0.631 - ETA: 2s - loss: 0.9146 - acc: 0.635 - ETA: 2s - loss: 0.9133 - acc: 0.634 - ETA: 2s - loss: 0.9122 - acc: 0.639 - ETA: 2s - loss: 0.9150 - acc: 0.637 - ETA: 2s - loss: 0.9103 - acc: 0.637 - ETA: 1s - loss: 0.9128 - acc: 0.634 - ETA: 1s - loss: 0.9071 - acc: 0.636 - ETA: 1s - loss: 0.9103 - acc: 0.634 - ETA: 1s - loss: 0.9092 - acc: 0.635 - ETA: 1s - loss: 0.9074 - acc: 0.635 - ETA: 1s - loss: 0.9055 - acc: 0.634 - ETA: 1s - loss: 0.8995 - acc: 0.638 - ETA: 0s - loss: 0.8992 - acc: 0.640 - ETA: 0s - loss: 0.9047 - acc: 0.636 - ETA: 0s - loss: 0.9026 - acc: 0.638 - ETA: 0s - loss: 0.9041 - acc: 0.635 - ETA: 0s - loss: 0.9038 - acc: 0.637 - ETA: 0s - loss: 0.9092 - acc: 0.635 - ETA: 0s - loss: 0.9107 - acc: 0.635 - ETA: 0s - loss: 0.9094 - acc: 0.635 - 7s 5ms/step - loss: 0.9100 - acc: 0.6346 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.53316\n",
      "Epoch 97/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.7912 - acc: 0.687 - ETA: 5s - loss: 0.8808 - acc: 0.671 - ETA: 5s - loss: 0.9545 - acc: 0.625 - ETA: 5s - loss: 0.9427 - acc: 0.625 - ETA: 5s - loss: 0.9468 - acc: 0.612 - ETA: 5s - loss: 0.9581 - acc: 0.599 - ETA: 5s - loss: 0.9367 - acc: 0.607 - ETA: 5s - loss: 0.9497 - acc: 0.613 - ETA: 5s - loss: 0.9377 - acc: 0.621 - ETA: 4s - loss: 0.9173 - acc: 0.643 - ETA: 4s - loss: 0.9096 - acc: 0.644 - ETA: 4s - loss: 0.9085 - acc: 0.648 - ETA: 4s - loss: 0.9040 - acc: 0.653 - ETA: 4s - loss: 0.9120 - acc: 0.645 - ETA: 4s - loss: 0.9127 - acc: 0.641 - ETA: 4s - loss: 0.9232 - acc: 0.642 - ETA: 4s - loss: 0.9267 - acc: 0.637 - ETA: 3s - loss: 0.9252 - acc: 0.638 - ETA: 3s - loss: 0.9276 - acc: 0.636 - ETA: 3s - loss: 0.9236 - acc: 0.634 - ETA: 3s - loss: 0.9274 - acc: 0.633 - ETA: 3s - loss: 0.9158 - acc: 0.644 - ETA: 3s - loss: 0.9173 - acc: 0.642 - ETA: 3s - loss: 0.9134 - acc: 0.645 - ETA: 2s - loss: 0.9085 - acc: 0.646 - ETA: 2s - loss: 0.9024 - acc: 0.651 - ETA: 2s - loss: 0.8976 - acc: 0.652 - ETA: 2s - loss: 0.8981 - acc: 0.654 - ETA: 2s - loss: 0.8946 - acc: 0.657 - ETA: 2s - loss: 0.9017 - acc: 0.655 - ETA: 2s - loss: 0.9105 - acc: 0.649 - ETA: 2s - loss: 0.9111 - acc: 0.647 - ETA: 1s - loss: 0.9100 - acc: 0.648 - ETA: 1s - loss: 0.9108 - acc: 0.648 - ETA: 1s - loss: 0.9107 - acc: 0.646 - ETA: 1s - loss: 0.9024 - acc: 0.650 - ETA: 1s - loss: 0.9021 - acc: 0.648 - ETA: 1s - loss: 0.8970 - acc: 0.649 - ETA: 1s - loss: 0.8993 - acc: 0.645 - ETA: 0s - loss: 0.8985 - acc: 0.648 - ETA: 0s - loss: 0.9001 - acc: 0.646 - ETA: 0s - loss: 0.9008 - acc: 0.644 - ETA: 0s - loss: 0.8977 - acc: 0.645 - ETA: 0s - loss: 0.8974 - acc: 0.647 - ETA: 0s - loss: 0.9003 - acc: 0.645 - ETA: 0s - loss: 0.9050 - acc: 0.645 - ETA: 0s - loss: 0.9041 - acc: 0.646 - 7s 5ms/step - loss: 0.9039 - acc: 0.6479 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.53316\n",
      "Epoch 98/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.8785 - acc: 0.687 - ETA: 5s - loss: 0.8161 - acc: 0.703 - ETA: 5s - loss: 0.7900 - acc: 0.708 - ETA: 5s - loss: 0.8138 - acc: 0.695 - ETA: 5s - loss: 0.8133 - acc: 0.668 - ETA: 5s - loss: 0.7784 - acc: 0.687 - ETA: 5s - loss: 0.7949 - acc: 0.692 - ETA: 5s - loss: 0.8051 - acc: 0.687 - ETA: 5s - loss: 0.8194 - acc: 0.684 - ETA: 4s - loss: 0.8224 - acc: 0.684 - ETA: 4s - loss: 0.8147 - acc: 0.690 - ETA: 4s - loss: 0.8241 - acc: 0.682 - ETA: 4s - loss: 0.8446 - acc: 0.663 - ETA: 4s - loss: 0.8467 - acc: 0.669 - ETA: 4s - loss: 0.8413 - acc: 0.675 - ETA: 4s - loss: 0.8483 - acc: 0.675 - ETA: 4s - loss: 0.8401 - acc: 0.676 - ETA: 3s - loss: 0.8533 - acc: 0.664 - ETA: 3s - loss: 0.8658 - acc: 0.661 - ETA: 3s - loss: 0.8671 - acc: 0.657 - ETA: 3s - loss: 0.8718 - acc: 0.656 - ETA: 3s - loss: 0.8721 - acc: 0.650 - ETA: 3s - loss: 0.8794 - acc: 0.645 - ETA: 3s - loss: 0.8901 - acc: 0.638 - ETA: 2s - loss: 0.8889 - acc: 0.640 - ETA: 2s - loss: 0.8879 - acc: 0.645 - ETA: 2s - loss: 0.8888 - acc: 0.643 - ETA: 2s - loss: 0.8872 - acc: 0.638 - ETA: 2s - loss: 0.8929 - acc: 0.634 - ETA: 2s - loss: 0.8907 - acc: 0.635 - ETA: 2s - loss: 0.8871 - acc: 0.637 - ETA: 2s - loss: 0.8816 - acc: 0.641 - ETA: 1s - loss: 0.8852 - acc: 0.642 - ETA: 1s - loss: 0.8754 - acc: 0.648 - ETA: 1s - loss: 0.8831 - acc: 0.644 - ETA: 1s - loss: 0.8828 - acc: 0.644 - ETA: 1s - loss: 0.8914 - acc: 0.642 - ETA: 1s - loss: 0.8960 - acc: 0.641 - ETA: 1s - loss: 0.8925 - acc: 0.643 - ETA: 0s - loss: 0.8901 - acc: 0.646 - ETA: 0s - loss: 0.8922 - acc: 0.644 - ETA: 0s - loss: 0.8941 - acc: 0.644 - ETA: 0s - loss: 0.8992 - acc: 0.641 - ETA: 0s - loss: 0.9018 - acc: 0.638 - ETA: 0s - loss: 0.9038 - acc: 0.639 - ETA: 0s - loss: 0.9028 - acc: 0.639 - ETA: 0s - loss: 0.9047 - acc: 0.638 - 7s 5ms/step - loss: 0.9051 - acc: 0.6379 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.53316\n",
      "Epoch 99/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.9410 - acc: 0.593 - ETA: 5s - loss: 0.8605 - acc: 0.656 - ETA: 5s - loss: 0.8603 - acc: 0.677 - ETA: 5s - loss: 0.8729 - acc: 0.671 - ETA: 5s - loss: 0.9057 - acc: 0.637 - ETA: 5s - loss: 0.9367 - acc: 0.635 - ETA: 5s - loss: 0.9362 - acc: 0.633 - ETA: 5s - loss: 0.9388 - acc: 0.632 - ETA: 5s - loss: 0.9257 - acc: 0.631 - ETA: 4s - loss: 0.9083 - acc: 0.643 - ETA: 4s - loss: 0.8979 - acc: 0.647 - ETA: 4s - loss: 0.8855 - acc: 0.653 - ETA: 4s - loss: 0.9057 - acc: 0.641 - ETA: 4s - loss: 0.9229 - acc: 0.631 - ETA: 4s - loss: 0.9190 - acc: 0.635 - ETA: 4s - loss: 0.9204 - acc: 0.636 - ETA: 4s - loss: 0.9290 - acc: 0.634 - ETA: 3s - loss: 0.9305 - acc: 0.633 - ETA: 3s - loss: 0.9202 - acc: 0.641 - ETA: 3s - loss: 0.9141 - acc: 0.643 - ETA: 3s - loss: 0.9098 - acc: 0.647 - ETA: 3s - loss: 0.9050 - acc: 0.652 - ETA: 3s - loss: 0.9046 - acc: 0.654 - ETA: 3s - loss: 0.9068 - acc: 0.652 - ETA: 2s - loss: 0.9162 - acc: 0.643 - ETA: 2s - loss: 0.9100 - acc: 0.646 - ETA: 2s - loss: 0.9079 - acc: 0.647 - ETA: 2s - loss: 0.9075 - acc: 0.648 - ETA: 2s - loss: 0.9077 - acc: 0.648 - ETA: 2s - loss: 0.9140 - acc: 0.642 - ETA: 2s - loss: 0.9106 - acc: 0.641 - ETA: 2s - loss: 0.9133 - acc: 0.637 - ETA: 1s - loss: 0.9106 - acc: 0.638 - ETA: 1s - loss: 0.9064 - acc: 0.640 - ETA: 1s - loss: 0.9053 - acc: 0.640 - ETA: 1s - loss: 0.9038 - acc: 0.640 - ETA: 1s - loss: 0.9032 - acc: 0.641 - ETA: 1s - loss: 0.9015 - acc: 0.640 - ETA: 1s - loss: 0.9032 - acc: 0.640 - ETA: 0s - loss: 0.9033 - acc: 0.639 - ETA: 0s - loss: 0.9026 - acc: 0.641 - ETA: 0s - loss: 0.9009 - acc: 0.642 - ETA: 0s - loss: 0.9033 - acc: 0.641 - ETA: 0s - loss: 0.9056 - acc: 0.640 - ETA: 0s - loss: 0.9064 - acc: 0.639 - ETA: 0s - loss: 0.9077 - acc: 0.640 - ETA: 0s - loss: 0.9112 - acc: 0.641 - 7s 5ms/step - loss: 0.9106 - acc: 0.6412 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.53316\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "Epoch 100/100\n",
      "1508/1508 [==============================] - ETA: 6s - loss: 0.7364 - acc: 0.625 - ETA: 6s - loss: 0.7927 - acc: 0.671 - ETA: 5s - loss: 0.8133 - acc: 0.687 - ETA: 5s - loss: 0.8443 - acc: 0.656 - ETA: 5s - loss: 0.8552 - acc: 0.656 - ETA: 5s - loss: 0.8678 - acc: 0.661 - ETA: 5s - loss: 0.8805 - acc: 0.656 - ETA: 5s - loss: 0.8555 - acc: 0.675 - ETA: 5s - loss: 0.8662 - acc: 0.673 - ETA: 4s - loss: 0.8723 - acc: 0.668 - ETA: 4s - loss: 0.8624 - acc: 0.667 - ETA: 4s - loss: 0.8781 - acc: 0.656 - ETA: 4s - loss: 0.8798 - acc: 0.658 - ETA: 4s - loss: 0.8725 - acc: 0.665 - ETA: 4s - loss: 0.8698 - acc: 0.662 - ETA: 4s - loss: 0.8654 - acc: 0.658 - ETA: 4s - loss: 0.8705 - acc: 0.659 - ETA: 3s - loss: 0.8731 - acc: 0.661 - ETA: 3s - loss: 0.8836 - acc: 0.656 - ETA: 3s - loss: 0.8815 - acc: 0.653 - ETA: 3s - loss: 0.8894 - acc: 0.650 - ETA: 3s - loss: 0.8941 - acc: 0.647 - ETA: 3s - loss: 0.8883 - acc: 0.648 - ETA: 3s - loss: 0.8879 - acc: 0.647 - ETA: 2s - loss: 0.8882 - acc: 0.646 - ETA: 2s - loss: 0.8861 - acc: 0.647 - ETA: 2s - loss: 0.8852 - acc: 0.649 - ETA: 2s - loss: 0.8793 - acc: 0.654 - ETA: 2s - loss: 0.8805 - acc: 0.651 - ETA: 2s - loss: 0.8860 - acc: 0.650 - ETA: 2s - loss: 0.8887 - acc: 0.646 - ETA: 2s - loss: 0.8885 - acc: 0.646 - ETA: 1s - loss: 0.8922 - acc: 0.644 - ETA: 1s - loss: 0.8934 - acc: 0.643 - ETA: 1s - loss: 0.8986 - acc: 0.640 - ETA: 1s - loss: 0.9008 - acc: 0.638 - ETA: 1s - loss: 0.9091 - acc: 0.636 - ETA: 1s - loss: 0.9056 - acc: 0.636 - ETA: 1s - loss: 0.9071 - acc: 0.637 - ETA: 0s - loss: 0.9098 - acc: 0.637 - ETA: 0s - loss: 0.9127 - acc: 0.634 - ETA: 0s - loss: 0.9122 - acc: 0.636 - ETA: 0s - loss: 0.9077 - acc: 0.639 - ETA: 0s - loss: 0.9103 - acc: 0.639 - ETA: 0s - loss: 0.9198 - acc: 0.637 - ETA: 0s - loss: 0.9181 - acc: 0.637 - ETA: 0s - loss: 0.9127 - acc: 0.640 - 7s 5ms/step - loss: 0.9121 - acc: 0.6406 - val_loss: 1.2180 - val_acc: 0.5040\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.53316\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "#config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "#sess = tf.Session(config=config)\n",
    "#set_session(sess)\n",
    "\n",
    "model_type = 'CNN'\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "sgd = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optimizer = sgd\n",
    "metrics = ['accuracy']\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "\n",
    "X_train = reshape_data(X_train)\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = build_model(loss, optimizer, metrics, input_shape, typ=model_type)\n",
    "history = train_model(model, epochs, typ=model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "\n",
    "# Unify slices into songs\n",
    "def unify_slices(y_pred_aux, y_true_aux, threshold):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    labels = movements + ['undefined']\n",
    "    cont = 0\n",
    "    for i in Z_test:\n",
    "        counter = Counter(y_pred_aux[cont : cont+i[0]])\n",
    "\n",
    "        # Si es mayor a un porcentaje de los casos, lo ponemos como etiqueta\n",
    "        if (counter.most_common()[0][1] > threshold * i[0]):\n",
    "            y_pred.append(counter.most_common()[0][0])\n",
    "        else:\n",
    "            y_pred.append(num_classes + 1)\n",
    "\n",
    "        y_true.append(i[1])\n",
    "        cont += i[0]\n",
    "    \n",
    "    return y_pred, y_true, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Test Data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8899dab43d4778ab701d32b2b29d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7fd61a148f443d4af1afb2816a77818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac700b53f29d47678e4dda0783cb3c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5392fa301f8e481a824455f40dd09dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50b1f8c98c94652ac5fabecb9afeb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "239\n",
      "239\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "\n",
    "print('Loading Test Data...')\n",
    "X_test, Y_test, Z_test = load_data(validate_route, test_samples, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing block\n",
    "\n",
    "model = load_model(os.path.join(model_route, 'gilgamesh.h5'))\n",
    "preds = test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting arrays\n",
    "\n",
    "threshold = 0.4\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_true_aux = [np.argmax(Y_test[i]) for i in range(len(Y_test))] \n",
    "y_pred_aux = [np.argmax(preds[i]) for i in range(len(preds))]\n",
    "\n",
    "y_pred, y_true, labels = unify_slices(y_pred_aux, y_true_aux, threshold)\n",
    "\n",
    "del(y_true_aux)\n",
    "del(y_pred_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names,title='Confusion matrix',cmap=None,normalize=False):\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float32') / cm.sum(axis=1)\n",
    "        cm = np.round(cm,2)\n",
    "        \n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel(\"Predicted label\\naccuracy={:0.4f}\\n misclass={:0.4f}\".format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5188284518828452\n",
      "F1 Score: [0.53703704 0.33333333 0.54166667 0.73684211 0.41584158]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApoAAAJeCAYAAAAUdaVdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nOzdd3wU1frH8c8TQpBOqGl0pEoHkd6LNBFFFOzea7ley8/eC1ds2LtiARULCoh0UMSudAXsIi0JoQlIC2Rzfn/sEtMgAbOb2eT79rUvMzNnzj4zWZaH55yZMeccIiIiIiIFLaKwAxARERGRokmJpoiIiIgEhRJNEREREQkKJZoiIiIiEhRKNEVEREQkKJRoioiIiEhQKNEUEQkiMyttZjPMbJeZvfcP+hltZvMLMrbCYmZdzeznwo5DRILPdB9NEREws1HAdUBj4C9gJTDWOffFP+z3POAqoJNzLu0fB+pxZuaAE51zvxV2LCJS+FTRFJFiz8yuA54A7gdqALWA54DTCqD72sAvxSHJzA8ziyzsGEQkdJRoikixZmYVgTHAlc65qc65vc65Q865Gc65GwNtSpnZE2aWFHg9YWalAtt6mNkmM7vezLaYWbKZXRTYdi9wFzDSzPaY2SVmdo+ZvZnp/euYmTucgJnZhWa21sz+MrM/zGx0pvVfZNqvk5ktCQzJLzGzTpm2LTKz/5nZl4F+5ptZ1SMc/+H4b8oU/zAzG2hmv5jZDjO7LVP7k83sazPbGWj7jJlFBbZ9Fmj2XeB4R2bq/2Yz2wy8dnhdYJ/6gfdoE1iOM7NtZtbjH/1iRcQTlGiKSHHXETgBmHaUNrcDpwCtgJbAycAdmbbHABWBeOAS4Fkzi3bO3Y2/Svquc66cc+6VowViZmWBp4BTnXPlgU74h/Czt6sMzAq0rQI8BswysyqZmo0CLgKqA1HADUd56xj85yAef2I8HjgXaAt0Be4ys3qBtj7g/4Cq+M9db+A/AM65boE2LQPH+26m/ivjr+5emvmNnXO/AzcDk8ysDPAaMME5t+go8YpImFCiKSLFXRVgWx5D26OBMc65Lc65rcC9wHmZth8KbD/knJsN7AEaHWc86cBJZlbaOZfsnFuTS5tBwK/OuTecc2nOubeBn4Ahmdq85pz7xTm3H5iMP0k+kkP456MeAt7Bn0Q+6Zz7K/D+a4AWAM65Zc65bwLvuw54Eeiej2O62zmXGognC+fceOBX4FsgFn9iLyJFgBJNESnutgNV85g7GAesz7S8PrAuo49sieo+oNyxBuKc2wuMBC4Hks1slpk1zkc8h2OKz7S8+Rji2e6c8wV+PpwIpmTavv/w/mbW0MxmmtlmM9uNv2Kb67B8JludcwfyaDMeOAl42jmXmkdbEQkTSjRFpLj7GjgADDtKmyT8w76H1QqsOx57gTKZlmMyb3TOzXPO9cVf2fsJfwKWVzyHY0o8zpiOxfP44zrROVcBuA2wPPY56u1NzKwc/ouxXgHuCUwNEJEiQImmiBRrzrld+OclPhu4CKaMmZU0s1PN7OFAs7eBO8ysWuCimruAN4/UZx5WAt3MrFbgQqRbD28wsxpmNjQwVzMV/xC8L5c+ZgMNzWyUmUWa2UigKTDzOGM6FuWB3cCeQLX1imzbU4B6OfY6uieBZc65f+Gfe/rCP45SRDxBiaaIFHvOucfw30PzDmArsBH4L/BBoMl9wFLge2AVsDyw7njeawHwbqCvZWRNDiOA6/FXLHfgn/v4n1z62A4MDrTdDtwEDHbObTuemI7RDfgvNPoLf7X13Wzb7wEmBq5KPyuvzszsNGAA/ukC4P89tDl8tb2IhDfdsF1EREREgkIVTREREREJCiWaIiIiIpLBzE4ws8Vm9p2ZrQk8fAIzmxB4kMTKwOtot00DQI8CExEREZHMUoFezrk9ZlYS+MLM5gS23eicez+/HSnRFBEREZEMzn8Bz57AYsnA67gu6tHQuYiIiIhkYWYlzGwlsAVY4Jz7NrBprJl9b2aPm1mpPPvRVecSKhEnlHcRZasVdhhFUuOESoUdQpGUrq/HoImKzOse73K80tMLO4Kia9V3y7c550L6F1mJCrWdS8vx5NZ/zO3fugb/wyoOe8k591L2dmZWCZgGXIX/dmqbgSjgJeB359yYo72Phs4lZCLKVqP8wP8VdhhF0uSHjvZQGzleB9P0N3aw1KxcurBDKLL2pKbl3UiOS52qpbM/+jXoXNp+SjXK85a0x+zAymcPOOfa5fn+zu00s0XAAOfcI4HVqWb2Gv776h6Vhs5FREREPMvAIgr+dbR39D8FrVLg59JAH+AnM4sNrDP8j+1dnVf0qmiKiIiISGax+J/wVQJ/UXKyc26mmS00s2qA4X+c7uVH6wSUaIqIiIh4lwEW2jnNzrnvgda5rO91rH1p6FxEREREgkIVTREREREvy2NOpZcp0RQRERHxshAPnRek8E2RRURERMTTVNEUERER8SwL66Hz8I1cRERERDxNFU0RERERL9McTRERERGRrFTRFBEREfEqI6znaCrRFBEREfEs09C5iIiIiEh2qmiKiIiIeFkYD52Hb+QiIiIi4mmqaIqIiIh4WRjP0VSiKSIiIuJZejKQiIiIiEgOqmiKiIiIeJUR1kPnqmiKiIiISFCooikiIiLiZWE8R1OJpoiIiIhn6WIgEREREZEcVNEUERER8bIIXQwkIiIiIpKFKpoiIiIiXmWE9RxNJZoiIiIiXqb7aIqIiIiIZKWKpoiIiIhn6fZGIiIiIiI5qKIpIiIi4mWaoykiIiIikpUqmiIiIiJepjmaIuGjd/NYFj88hGWPDOXawU1zbTPs5Fp8/eBgvnpgEOOv6Jyx/uwudVk6bghLxw3h7C51QxVy2Pj8kwUM6tqaAZ1bMP6ZR3NsX/rNF5zZvzMtalVk3sxpGet/XP09o4b0YmjPdpzepwNzpr8fyrDDwpeLFjCsZxuGdmvJq889lmP7sm+/5JyBXWlXL5oFsz7IWL/kq88YeWrnjFeHhtX4ZN7MUIbueR8vmEeH1s1o37IxTz76cI7tqampXHLBKNq3bEy/np3YsH5dlu2bNm6gdkwlnnky5++lOFv08Xx6dWhB9/bNeO7JcTm2p6amcuUl59K9fTNO69eVjRvWA3Do0CGuu/Jf9O/ajt4dW/HsEzn3LVbMgvMKEVU0pViJMGPcBe05/aGFJO3Yx8IxA5izfBM/J+3OaFOvRnn+b0gzBoyZz659B6laoRQAlcpGcfPpzel511ycg0X/G8Cc5Yns2newsA7HU3w+H2Nvv47xb39Ijdh4Rg7sRs9+A2nQsElGm9j4mox9/EUmvPBkln1Lly7NA0++RO16DdiyOZkRp3ahc48+VKhYKdSH4Uk+n48H77ye5ydNp0ZMPKOH9qB7n4HUb9g4o01sXAL3Pvo8r7/0VJZ923fqxrtzvgRg184dDO3WilO69Qpp/F7m8/m4+fqreX/6HOLiE+jb/RQGDBpMo8Z//yN00uuvUqlSJZZ89xNT33+Xe++6jVcmvpWx/Y5bbqB33wGFEb5n+Xw+7rr5Wt58fxYxcfEM7duFvgMGc2Kjv78PJk+aQMVK0Xy6ZA0fTp3Mg/fezrOvvMns6VM4mJrKvM+Xsn/fPvp0bs3Q4WdRs1btwjsgOW6qaEqx0rZ+Fdam/MX6rXs45Etn6jfrGdi2ZpY2F/RswMsf/ZKRQG7bnQr4K6GLVm9m596D7Np3kEWrN9OnRWzIj8GrVq1YSs069ahZuy5RUVEMPO1MPpk3K0ub+Jq1adT0JCwi61dPnfonUrteAwCqx8RSuUo1/ty+LWSxe93qlf5zm1CrLiWjoug/5AwWLch6buNq1qZhk5OIiDjy1/pHs6fTuUdfSpcuE+yQw8bypYupW68+derWIyoqitPPGMmcmTOytJkzawZnjzoPgKHDzuDzRQtxzgEwe8Z0atepS6MmuY+OFFcrly+hdt361Krj/z4YcvoI5s/JWkmfP2cmZ5w9GoCBQ4fz1eeL/OfVjP379pGWlsaBA/uJKhlF+fLlC+MwvMMiCv4VIko0pViJjS5N4o59GctJO/YRG106S5v6MeVpEFuBuXf2Y/7d/end3J9MxlYuw6ZM+ybu2EdsZf2FfVjK5iRi4xIylmvExpOyOemY+/l+xVLSDh2kZp16BRleWNuyOZkasZnPbRxbj+PczvtwCgNOO7MgQwt7yclJxMX/fW7j4uNJTk7M2iYpifgE/z9IIyMjqVCxIju2b2fv3r089fg4brz1zpDGHA5SkpOIy/R9EBsXT0q285qS6dxHRkZSvkIF/tyxnYFDh1O6TBlOblaXTq0a8u8rr6VSdOWQxi8FR4lmMWNmPcws1wlaZjbbzIr0WKXlMi8lUJjIEBkRQb0a5Rl8/wL+9dwXPPmvU6hQpiS5zWhx2XcuznI5F7md76PZmrKZW6/+N/c99sJRK3PFTy6fs+M4t7/+vIaO3foUUExFQ25/hrN/bo/U5qGx93L5f6+hXLlyQYsvXP2T8/rd8iWUKFGCb1ev5fNlP/Lyc0+yYd0fQYs1LGiOphQFzrmBhR1DsCXt2Ed8pipkXOUybN65P0ebJb9vI83n2LB1L78l76Z+jfIk7dhHlyY1MtrFVy7DFz+mhCx2r6sRG09y0qaM5ZTkRKrXyP/Ugj1/7eaK88/g6pvupGXbk4MRYtiqHhNHSnLmc5tEtWM4twALZk2lV/8hlCxZsqDDC2txcfEkJf59bpMSE4mJicvaJj6exE0biYtPIC0tjd27dhFduTLLly5mxvSp3HvnrezatZOIiAhOOKEU/7rsylAfhufExMWTlOn7IDkpkerZzmtM4NzHxvnP61+7d1MpujLTp0yme+9+lCxZkqrVqtO2Q0e+X7mMWnWK6wWYejKQeICZnW9m35vZd2b2hplNMLMzM23fk6l5BTObZmY/mNkLZv5PsJmtM7OqZlbWzGYF+lptZiMzbb/fzL42s6Vm1sbM5pnZ72Z2eYgP+bgsX7ud+jHlqVWtLCVLRDD8lNrMWb4pS5tZyzbSNZBQVi5XigYx5Vm3dQ8fr0qmZ/NYKpaJomKZKHo2j+XjVcmFcRiedFKrtmz443c2bVjHwYMHmT39fXr2y9+/XQ4ePMjVl5zD0DNH0X/I8CBHGn6atWzLhj/WkrhhHYcOHmTejCn06Hts/y6c++H7DBiqYfPsWrdtz9rff2P9uj84ePAg06a8y4BBg7O0GTBwMO+89QYAH34wha7de2JmzJy/iBVrfmPFmt+47D9Xc+31tyjJDGjZuh3r1v7GxvX+74MZ096j74BBWdr0HTCIKe9MAmD2h1Pp1LU7ZkZcQkLGfM19e/eyYuli6p/YqDAOQwqAKppFgJk1A24HOjvntplZZeBo99k4GWgKrAfmAsOBzPeTGQAkOecGBfqvmGnbRudcRzN7HJgAdAZOANYALxTMEQWPL91x0+tLmXJjL0pEGJM++52fEndx6/AWrPxjO3NWJGYklF8/OJj0dMdd76zgzz3+C4PGfbCKhWP8V5c+PG0VO/fqivPDIiMjuf2+R7l01DDS032cPvI8GjRqytPj/kezlm3o1W8Qq1Yu45pLzmH3rp0sWjCHZx8dy4efLGXejKks+/ZLdv65gw8mvwnA2MdfpMlJLQr5qLwhMjKSm8eM4z/nn066z8dpZ51H/YZNeO7R+2jaog09+g5kzXfLuO7S0ezetZPPPprDC4/fz5SPFgOQtHE9m5MSaXtKl0I+Eu+JjIzkwUeeZMSwQaSn+xh13oU0btKMB+67h1at23LqoCGMPv9i/vPvC2nfsjGVoqMZ/9qkwg7b8yIjIxnz4OOcP2IIvnQfZ426gIaNm/LYA2No3qoNfU8dzFmjL+S6/1xM9/bNqFQpmqfH+5P58y++nBuvvpR+XdrinGPEOefRpFnzQj6iQhbGTwYyzTELf2Z2FRDjnLs907oJwEzn3PuB5T3OuXJm1gMY45zrFlh/MdDCOXetma0D2gGVgXnA5EAfnwfarsOfzCYG9uvonPt3YNuGQD87s8V2KXApgJWt0rbi6VlvayMF48uHhhV2CEXSwbT0wg6hyKpZuXTejeS47ElNK+wQiqw6VUsvc861C+V7RlSs5Up1uaHA+z0w+5qQHIuGzosGI+fVAmkEfr/mn4EdlWlb9rZZlp1zvwBtgVXAA2Z2V6bNqYH/p2f6+fByjgq5c+4l51w751y7iFIV8nc0IiIi4mfo9kZS6D4GzjKzKgCBofN1+JNFgNOAzFcAnGxmdQNzM0cCX2TuzMzigH3OuTeBR4A2wQ1fREREcmdhnWhqjmYR4JxbY2ZjgU/NzAesAG4GppvZYvyJ6N5Mu3wNPAg0Bz4DpmXrsjkwzszSgUPAFUE+BBERESmClGgWEc65icDEbKtPyfTzrYF2i4BFR+ijTuDHeYHXkbbjnJuA/2KgHNtERESkAIXxxUAaOhcRERGRoFBFU0RERMTLdMN2EREREZGsVNEUERER8bIwnqOpRFNERETEq0zPOhcRERERyUEVTREREREvC+Ohc1U0RURERCQoVNEUERER8TBTRVNERERECprhTzQL+nXU9zQ7wcwWm9l3ZrbGzO4NrK9rZt+a2a9m9q6ZReUVvxJNEREREcksFejlnGsJtAIGmNkpwEPA4865E4E/gUvy6kiJpoiIiIhXWZBeR+H89gQWSwZeDugFvB9YPxEYllf4SjRFREREJAszK2FmK4EtwALgd2Cncy4t0GQTEJ9XP7oYSERERMSz8p5TeZyqmtnSTMsvOedeOrzgnPMBrcysEjANaJJLHy6vN1GiKSIiIuJhQUo0tznn2uXVyDm308wWAacAlcwsMlDVTACS8tpfQ+ciIiIiksHMqgUqmZhZaaAP8CPwCXBmoNkFwPS8+lJFU0RERMTDCuE+mrHARDMrgb8oOdk5N9PMfgDeMbP7gBXAK3l1pERTRERERDI4574HWueyfi1w8rH0pURTRERExMPC+clASjRFREREvCof9730Ml0MJCIiIiJBoYqmiIiIiEdZ8O6jGRKqaIqIiIhIUKiiKSIiIuJhqmiKiIiIiGSjiqaIiIiIh4VzRVOJpoiIiIiHhXOiqaFzEREREQkKVTRFREREvEo3bBcRERERyUkVTREREREPC+c5mko0RURERDxKTwYSEREREcmFKpoiIiIiHqaKpoiIiIhINqpoioiIiHhZ+BY0lWiKiIiIeJZp6FxEREREJAdVNCVk6sSU57Gbehd2GEXSt4nbCzuEIimhXOnCDqHIii5bsrBDKLK+2aDvg6JGFU0RERERkWxU0RQRERHxsHCuaCrRFBEREfEoPRlIRERERCQXqmiKiIiIeFn4FjRV0RQRERGR4FBFU0RERMSrdMN2EREREZGcVNEUERER8bBwrmgq0RQRERHxsHBONDV0LiIiIiJBoYqmiIiIiJeFb0FTFU0RERERCQ5VNEVEREQ8LJznaCrRFBEREfEoMz3rXEREREQkB1U0RURERDxMFU0RERERkWxU0RQRERHxsHCuaCrRFBEREfGy8M0zNXQuIiIiIsGhiqaIiIiIh4Xz0LkqmiIiIiISFKpoioiIiHiVqaIpIiIiIpKDKpoiIiIiHmVAGBc0lWiKiIiIeJeedS4iIiIikoMqmiIiIiIeFsYFTVU0RURERCQ4VNEUERER8bBwnqOpRFNERETEq0xD5yIiIiJSRJhZTTP7xMx+NLM1ZnZNYP09ZpZoZisDr4F59aWKpoiIiIhHGRAREfKSZhpwvXNuuZmVB5aZ2YLAtsedc4/ktyNVNKXYWf7FQq4Y0oXLBnXk/VeezrF9+usvcOWwblx9Ri/u/NcItiRtBGBL0kauG9mPa0f04b+nd2fO5ImhDt3zVn29iFvP7Mktw7sxa+JzObbPmzSe20f25q5R/Rn3n3PYlrwpY9v2zYk8etW53H5WL24f2ZttgfMufks+X8jFAztyYf+TeWf8Uzm2vz/hef41uAuXDevOTRedQUpi1vO3d89fnNOjBc/cd0uoQg4biz6eT68OLejevhnPPTkux/bU1FSuvORcurdvxmn9urJxw3oADh06xHVX/ov+XdvRu2Mrnn0i577F2covP+H/Tu/GNUM7M/21Z3Jsn/XmS1x/Rk9uOqsP/7tsJFuTNmXZvm/PX1zRvy2vPnh7qEKWAOdcsnNueeDnv4Afgfjj6UuJphQrPp+PF++/jbufn8QzH3zK53M+YMPvP2dpU7dxcx57ey5PTVlIp76DmfD4fQBEV6vBQ2/M4In3PmLcpNlMffUZtm/ZXBiH4UnpPh9vPnwn//fkRO579yO+nfchiWt/ydKmVqNm3DVxJmPemke7XgN57+kHMra9fM91DDj3MsZOXsidr31I+cpVQ30InuXz+XjmvpsZ++LbjJ/xBYtmT2X9b1k/tw2aNOeZ9+bz4gef0rX/YF5+dEyW7ROfepAW7TuFMuyw4PP5uOvma5nw7nQWfLmCD6e+x68//5ilzeRJE6hYKZpPl6zhksuv4sF7/YnP7OlTOJiayrzPlzLz4694a+LLGUlocZfu8/HqQ3dwy9Nv8OiUT/hy7nQ2Zfs+qNOoGfe/OZuHJ39Ehz6DmPTk2CzbJz8/jiZtTwll2J5lVvCv/L+31QFaA98GVv3XzL43s1fNLDqv/ZVoSrHy6+oVxNSqQ0xCbUqWjKLrgNNY/Mm8LG1anNyZUqXLANCoRRu2pyQDULJkFCWjSgFw6GAq6enpoQ3e49auWUn1hDpUj69FZMkoOvQbwsrPFmRp06RdJ0qdUBqAes1b8+cW/7lNXPsLPl8azTp0BeCEMmUz2gn8vGo5cbXqEluzDiWjouh+6ul8tXBuljatOnThhMDntkmLdmxNScrY9sua7/hz+1baduoRyrDDwsrlS6hdtz616tQlKiqKIaePYP6cmVnazJ8zkzPOHg3AwKHD+erzRTjnwIz9+/aRlpbGgQP7iSoZRfny5QvjMDznt9UriUmoQ42E2kSWjKJT/9NYumh+ljbN2nemVGn/n/MTm7dhR+D7AGDtD9+za/s2WpzSPaRxe5WZFfgLqGpmSzO9Ls3lfcsBU4BrnXO7geeB+kArIBl4NK/YlWhKsbI9ZTNVa/xd/a9SI/aoVckF096mbZeeGctbNydy9Rm9uKRfW4Zf/F+qVI8JarzhZOfWzVSuEZuxHF09lj+3Hvncfv7huzTv2AOAlA1/UKZcBZ656VLuOfdUJj81lnSfL9ghh41tKZupFvP357ZaTCzbM/2lnN3cqZNo37U3AOnp6bz08N38+4a7gx5nOEpJTiIuLiFjOTYunpTkxJxt4v1tIiMjKV+hAn/u2M7AocMpXaYMJzerS6dWDfn3lddSKbpySOP3qh1bk6kS8/f3QeXqMVkSyew++eBtWnX2f9emp6fzxuNjGH3tHUGPs5jb5pxrl+n1UuaNZlYSf5I5yTk3FcA5l+Kc8znn0oHxwMl5vYkSzeNgZl8VdgxHYmaXm9n5x7lvOzPLOfmrSHE51hzp/mSLZr7Pb2u+4/QL/5OxrlpMPE9NWcgLM7/mkw8ns3P71qBFGm5czlOLkfu5/XrOVNb9uIoB510GQLovjV9XLuGsa+7gzgkz2Jq4gS9mvhfMcMNLLif3SOf2ow/f45fV3zHi4isBmPH2a5zcrTfVY49relWR53I7t9m+E47U5rvlSyhRogTfrl7L58t+5OXnnmTDuj+CFmtYye374AjftZ/PmsLaH75nyPmXAzB/8kRad+5F1Zi4YEYYPoIwbJ7X0Ln5f1mvAD865x7LtD42U7PTgdV5ha+rzo+Dc67AJzqZWaRzLu2f9uOce+Ef7LsUWPpPY/CyKjVi2Zbyd7Vie0oylavVyNFu5Tef8d74Jxn76rSM4fIs/VSPoWb9RqxZ9i2d+w0OaszhIrp6DDtS/q5Y/LklmUq5nNs1i79g5mvPcPMLkzPObXT1WGo1akb1+FoAtO7en99XLw9N4GGgakwsWzf//bndujmZyrlU05d/9Slvv/QEj0z8gKjAuf1h5RJWL/uWGW9PYP++vaQdOkjpMmW55Lo7Qxa/l8XExZOU6SKU5KREqmdLcGLi4klK3ERsXAJpaWn8tXs3laIrM33KZLr37kfJkiWpWq06bTt05PuVy6hVp26oD8NzKlePZfvmv78PdmzZTHS1nJ/ZVd9+zrRXnubul9/P+D74ddUyflqxmPnvvU7q/r2kHTrECWXKMurq20IWv9AZOA9YZWYrA+tuA84xs1b4/ymxDrgsr45U0TwOZrYn8P8eZvapmU02s1/M7EEzG21mi81slZnVD7SbYGYvmNnngXaDA+svNLP3zGwGMD+w7kYzWxKYaHtvYF1ZM5tlZt+Z2WozGxlY/6CZ/RBo+0hg3T1mdkPg51Zm9k1g+7TDk3bNbJGZPRSI8xcz65rpeGYGfu6e6T5ZK8ysfH6P18tObNaK5PV/kLJpA4cOHeTzudM5uUf/LG3W/riK58fcxO1PTaRSlb8vSNm2OYnUA/sB2LN7Jz+tXEJ8Hc8fcsjUbdqSlI1/sDVxA2mHDvLt/Bm06to3S5v1P6/m9Qdu5epHXqFCpot96jZtyd7du9j953YAflz6FXF1Twxp/F7W6KTWJK5fS/Km9Rw6eJBP50yjY8+sn9vffljFk/fewJhn3iC6SrWM9beOe4FJC1fwxkfLuPTGe+hz2llKMjNp2bod69b+xsb16zh48CAzpr1H3wGDsrTpO2AQU96ZBMDsD6fSqWt3zIy4hISM+Zr79u5lxdLF1D+xUWEchufUb9aSzRv/YEvg++CredNp2z3r98EfP61m/NhbuPGJV6mY6fvgqrHP8OzsxTwz6xtGX3snXQedUayTTCNoczSPyDn3hXPOnHMtnHOtAq/ZzrnznHPNA+uHOueOPB8iQBXNf64l0ATYAawFXnbOnWz+m5teBVwbaFcH6I5/Eu0nZtYgsL4j0MI5t8PM+gEn4p/zYMCHZtYNqAYkOecGAZhZRTOrjL9s3dg558ysUi6xvQ5c5Zz71MzGAHdniicyEOfAwPo+2fa9AbjSOfdlYDLwgWM8Xk8qERnJpbfdzz1XnEO6z0fvYWdTq0EjJr9rwU4AACAASURBVD37MA2atqRDz/689tj/2L9vLw/f4J8XXTUmnjuensimP37l1UfuxcxwzjHsgsup07BJIR+Rd5SIjOTcG8fw2NXnk57uo8uQs4iv35BpLz5KnSYtaN2tL5Ofup/U/ft47lb/dIQqMXFc/egrRJQowchrbueRK0fhnKNO4+Z0H3ZOIR+Rd5SIjOS/tz/Ibf8eSXq6j/6nj6LOiY2Z+PSDNGzWio69BjD+kXvYv28v//u/SwCoHpfAmGffKOTIvS8yMpIxDz7O+SOG4Ev3cdaoC2jYuCmPPTCG5q3a0PfUwZw1+kKu+8/FdG/fjEqVonl6vP+8nn/x5dx49aX069IW5xwjzjmPJs2aF/IReUOJyEguuvl/3H/laNLT0+k5dCQ16zdi8vPjqNe0Je2692PSE/eRum8vT9zkHzKvGhPPjU+8VsiRe1HeiaGXWW5zT+TozGyPc66cmfUAbnfO9Q2s/wy4NZCc9QKuds4NM7MJwGfOuVcztbsa/1Vb3Z1zFwXWPwKcCewMvFU54AHgc2AeMBmY6Zz73MwigWX4h7pnBdYfNLN7gD34J+mucs7VCvRdH3jPOdfGzBYF4v7SzGoAXzrnGgSO5wbn3GAzuwV/IjsJmOqc25Tf4812ri4FLgWoFhvf9uV5RXpkvtBsP3CwsEMokhLK6cr3YGlYXVdnB8s3G7YXdghF1tltEpY559qF8j3LxDVyJ/47532J/6nvx/QJybFo6PyfS830c3qm5XSyVoyzZ/SHl/dmWmfAA5nK1A2cc684534B2gKrgAfM7K7AfM6T8V8RNgzIeq+T/MftI5fKtnPuQeBfQGngGzNrnG2/w8d4pOM93M9Lh69oqxBd5RhDFBERkcK8j+Y/pUQzdEaYWUSgslgP+DmXNvOAiwND1ZhZvJlVN7M4YJ9z7k3gEaBNoE1F59xs/MPVrTJ35JzbBfx5eP4l/km9n+Y3WDOr75xb5Zx7CH/VtHFe+4iIiIhkpjmaofMz/kSvBnC5c+5A9jkXzrn5ZtYE+DqwbQ9wLtAAGGdm6cAh4AqgPDDdzE7AXwn9v1ze8wLgBTMrg38+5UXHEO+1ZtYTf8XzB2AO/vmkIiIiEkKaoylHFZijOdM5935hx1KYGjRr6R57Z17eDeWYaY5mcGiOZvBojmbwaI5m8BTWHM1Glz1f4P2uvKd3SI5FFU0RERERrwrxnMqCpkQzBJxzFxZ2DCIiIhJ+Dt9HM1zpYiARERERCQpVNEVEREQ8LIwLmqpoioiIiEhwqKIpIiIi4mHhPEdTiaaIiIiIh4VxnqmhcxEREREJDlU0RURERLzKwnvoXBVNEREREQkKVTRFREREPMp/w/bCjuL4KdEUERER8SzT0LmIiIiISHaqaIqIiIh4WBgXNFXRFBEREZHgUEVTRERExMPCeY6mEk0RERERrzINnYuIiIiI5KCKpoiIiIhH+e+jGb4lTVU0RURERCQoVNEUERER8TBVNEVEREREslFFU0RERMTDwrigqURTRERExMs0dC4iIiIiko0qmiIiIiJepRu2i4iIiIjkpIqmiIiIiEcZFtZzNJVoioiIiHhYGOeZGjoXERERkeBQRVNERETEwyLCuKSpiqaIiIiIBIUqmiIiIiIeFsYFTSWaIiIiIl5lpicDiYiIiIjkoIqmiIiIiIdFhG9BUxVNEREREQkOVTRFREREPCyc52gq0ZSQKRsVSftalQs7DJF86zLmo8IOochaMXZAYYdQZNWrWK6wQxDJoERTRERExMPCuKCpRFNERETEqwwwwjfT1MVAIiIiIhIUqmiKiIiIeJhubyQiIiIiko0qmiIiIiJeZabbG4mIiIhIcIRxnqmhcxEREREJDiWaIiIiIh5lQIRZgb+O+p5mNc3sEzP70czWmNk1gfWVzWyBmf0a+H90XvEr0RQRERGRzNKA651zTYBTgCvNrClwC/Cxc+5E4OPA8lEp0RQRERHxMLOCfx2Ncy7ZObc88PNfwI9APHAaMDHQbCIwLK/YdTGQiIiIiIcV5lXnZlYHaA18C9RwziWDPxk1s+p57a9EU0RERKT4qWpmSzMtv+SceylzAzMrB0wBrnXO7T6ehFeJpoiIiIhH5Weo+zhtc861O/L7Wkn8SeYk59zUwOoUM4sNVDNjgS15vYnmaIqIiIhIBvOXLl8BfnTOPZZp04fABYGfLwCm59WXKpoiIiIiHpbX7YiCoDNwHrDKzFYG1t0GPAhMNrNLgA3AiLw6UqIpIiIi4mGhTjOdc18c5W17H0tfGjoXERERkaBQRVNERETEwwrz9kb/1BETTTOrcLQdnXO7Cz4cERERESkqjlbRXAM4so7RH152QK0gxiUiIiJS7PmfdV7YURy/IyaazrmaoQxERERERIqWfM3RNLOzgXrOufvNLAH/I4iWBTc0ERERkWLOLKznaOZ51bmZPQP0xH8/JYB9wAvBDEpERERE/A4/HaggX6GSn4pmJ+dcGzNbAeCc22FmUUGOS0RERETCXH4SzUNmFoH/AiDMrAqQHtSoRERERAQI79sb5eeG7c/if6h6NTO7F/gCeCioUYmIiIhI2Muzoumce93MlgF9AqtGOOdWBzcsERERESmytzfKpgRwCP/wuR5bKSIiIhIiRXro3MxuB94G4oAE4C0zuzXYgYmIiIhIeMtPRfNcoK1zbh+AmY0FlgEPBDMwEREREcn6iMZwk59h8PVkTUgjgbXBCUdEREREioojVjTN7HH8czL3AWvMbF5guR/+K89FREREJIjMICKM52gebej88JXla4BZmdZ/E7xwRERERCSzMM4zj5xoOudeCWUgIiIiIlK05Oeq8/pm9o6ZfW9mvxx+hSI4kWD45KN5dGl3Ep1aN+Hpx8fl2J6amsplF42mU+smDOrdhY3r12Vs+2H1Kob07UaPU1rRq1MbDhw4EMLIvU/nNni6NarKgpu7sfDW7lzWq16ubQa2jGHujV2Zc2NXHh/dCoC46BOYfm1nZlzXhTk3duWcjrVCGXZY+Gj+XNq2aEKrZg15bFzO55GkpqZy4bln06pZQ3p17cj6wOd24ccL6NapPR3btaRbp/Z8umhhiCP3tq8//Ygz+7RjeM/WTHzh8Rzbly/+kvOGdqNjwyp8PGd6lm2bkzZy1QWnc1a/kxnZvwNJm9aHKmxPMrMCf4VKfq46nwDcBzwCnApchB5BKWHK5/Nx2w3X8M4Hs4mNS2Bgz070P3UwDRs3yWjz9huvUalSJb5a8SMfTJnMfffczouvTSItLY2rLr2Qp158jWbNW7Bjx3ZKlixZiEfjLTq3wRNhcM/wZlzw4mI27zrAtGs78/GaLfyWsiejTZ2qZbi8d33OeuZrdu9Po0q5KAC27k5lxNNfc9CXTpmoEsy5sSsfr0lhy+7UwjocT/H5fFx/7VV8MGse8fEJ9OzSgYGDh9C4SdOMNq9PeJVK0dGsXPML709+h7tvv4UJb75DlSpVeff96cTGxfHDmtUMH3IqP63dWIhH4x0+n4+H77mBZyZ+QPWYOC44vSdde59KvRMbZ7SJiUvgroef483xT+fY/54bLuei/9xAhy492bd3DxERuoV3uMrPb66Mc24egHPud+fcHUDP4IYlEhwrli2hTr361K5Tj6ioKE474yzmzZ6Rpc282TMYcc55AAw+bThffPoJzjk+XbiAJic1p1nzFgBUrlyFEiVKhPwYvErnNnha1qrE+u372LhjP4d8jpkrkunTrEaWNiNPqcmbX65n9/40ALbvOQjAIZ/joM9fG4iKjAjriwqCYdmSxdSrX5+6df2f2+EjRjJr5odZ2syeOZ1Ro88HYNjwM/l00UKcc7Rs1ZrYuDgAmjRtxoHUA6SmKoEHWPPdMhJq1yO+Vh1KRkXRb/AZfPbR7Cxt4hJqc2Ljk3IkkWt//Qlfmo8OXfypRpmy5TihdJmQxe5FZgX/CpX8JJqp5q+x/m5ml5vZEKB6kOMSCYrNyUnExdfMWI6Niyc5OTGXNgkAREZGUqFCBXbs2M7a337FMM4ZPoh+3Trw7JOPhDR2r9O5DZ4aFU8geeffUwk279pPjYqlsrSpW60sdauVZfJ/T+H9qzvSrVHVjG2xlU5g1vVd+OLOXrz4yVpVMzNJSkokPuHvz218fDzJiVk/t8lJSRlt/J/biuzYvj1Lm+nTptCiZWtKlcr6eymutqYkUyM2PmO5ekwcW1OS87Xvhj9+o1yFitx0xbmcO6QrTz1wJz6fL1ihep5hRFjBv0IlP4nm/wHlgKuBzsC/gYuDGZQcHzO70MyeOcZ99gT+H2dm7wcnMu9wzuVYZ9luhZtrGzPSfGks/uZLnhk/kQ/mfsLcmR/y+aeak3WYzm3w5OevhBIREdSpWpZRz33LtW+u5P6zmlP+BP/sqOSdBxj06Bf0emARw9vFZwyry5E/k8fS5scf1nD3HbfyxDPPF3yAYSq3c5ZfPp+PlUu+5ppb72PCtE9I3LiOmVMmFWB0Ekp5JprOuW+dc3855zY4585zzg11zn0ZiuAkdJxzSc65Mws7jmCLjYsnKfHvOVTJSYnExMbl0mYTAGlpaezevZvo6MrExiXQsXM3qlSpSpkyZejVdwCrvlsR0vi9TOc2eDbvOkBspRMylmMqliZlV9aq5OadB/hoTQpp6Y5NO/bzx9a91KlWNkubLbtT+XXzHtrXqxySuMNBfHwCiZv+/twmJiYSE5f1cxsXH5/Rxv+53UV0Zf85TNy0idEjz+DFlydQr1790AXucdVj4kjJNKKxZXMS1WrE5nvfRs2aE1+rDpGRkXTvO4if13wfrFC9LwjD5p4YOjezaWY29Uiv0IVYdJlZWTObZWbfmdlqMxtpZtMybe97+Fyb2R4ze8jMlpnZR2Z2spktMrO1ZjY0U7c1zWyumf1sZndn6uu6wHusNrNrc4mljpmtDvzczMwWm9nKwN0GTgxs/8nMXg70McnM+pjZl2b2q5mdHMRTVWBatWnHH7//xoZ1f3Dw4EGmT5lMv1MHZ2nT79TBvPf2GwDMnD6VLt16YGb06N2XH9asYt++faSlpfH1l5/RsFGT3N6mWNK5DZ7vN+6iTtWyJFQuTckSxuDWsXy8JiVLmwWrN3NK/SoARJctSd1qZdm4fR8xFU+gVKT/q75C6Uja1o1m7Za9IT8Gr2rTrj2///Yb6wKf26nvvcvAQUOytBk4aChvTXodgA+mvk+37j0xM3bu3MlZw4dw95ixnNKpc2GE71lNW7Rh47rfSdy4jkMHDzJ/5hS69j413/vu3rWTP7dvA2Dp159Rt0GjYIYrQXS0q86PaQhWjssAIMk5NwjAzCoC95pZNefcVvxX+L8WaFsWWOScuzmQjN4H9AWaAhOBw7PXTwZOwv9EpyVmNgv/E50uAjrgH4X71sw+dc4dqWR0OfCkc26SmUUBJYAaQANgBHApsAQYBXQBhgK3AcMK4JwEVWRkJGPHPcGoMwbj8/k4+9wLadSkKQ+PvZeWrdvQf+AQzjnvIq6+7CI6tW5CpejKPP+qPzGqVCmay668hoG9OmFm9Oo7gD79BxbyEXmHzm3w+NId905dw4RLTybC4P3Fm/g1ZQ/X9j+RVZt28fGaLXz28za6NKrG3Bu7ku7gwRk/sXPfITo3rMhtQxrj8P/hf3nRWn7Z/FdhH5JnREZG8sjjTzF8yKn4fD7OveAimjRtxtgxd9O6TVsGDh7KeRdezKUXn0+rZg2Jjq7Mq2+8BcD4F55l7e+/Me7BsYx7cCwA02bMpVp1XcYQGRnJjXeP4+oLzyA93ceQM8+lfsMmvPj4WJo0b023PgP54fvl3HTFuezetZPPF87lpScf4N2531CiRAmuufU+rjxvKM5B45NaMmzkBYV9SIUqlLcjKmj2T+ZRyD9jZg2BecBkYKZz7nMzux1/kvgasAI40TmXZmapwAnOOWdmY4BU59xYM4sAdjjnKpnZhUAv59z5gf7HADvwJ5pVnHN3Bdb/D9jqnHvKzPY458qZWZ1ADCeZ2SjgduB1YKpz7tfA9gXOuRMDfbwOzAsko/UC7VrlcoyX4k9Mia9Zq+2SVb8W9GkUCZouYz4q7BCKrBVjBxR2CEXWz0n6h0SwnFy/0jLnXLtQvmf1Bie5EePeK/B+nxveNCTHohtTFSLn3C9AW2AV8ICZ3YU/wTwXOAd4zzmXFmh+yP39r4J0IDXQRzpZK9PZ/+VwuJBxLHG9hb9KuR+YZ2a9ApsyTwpLz7ScPYbMfb3knGvnnGtXpUrV3JqIiIjIUUQE4RUq+blhuwSJmcXhr0a+Gbj6+0LnXJKZJQF34B8aP1Z9zawy/iRxGP47BKQDE8zsQfxJ5+nAeUeJqx6wNlDxrAe0ANYeRywiIiLyDxjhPXSe70TTzEo553TztYLVHBhnZunAIeCKwPpJQDXn3A/H0ecXwBv451O+5ZxbCmBmE4DFgTYvH2V+JsBI4FwzOwRsBsYAFY4jFhERESnG8kw0A1cTvwJUBGqZWUvgX865q4IdXFEXeOLSvFw2dQHGZ2tbLtPP9+S2zTk3Af8jQ3N7r8eAx3JZf3jfdfgvIsI59wDwQLamOw5vD7S5MNPP6zJvExERkYITEb4FzXwN0z8FDAa2AzjnvkOPoAwaM1uGf6j6zcKORUREROSfyM/QeYRzbn22+QHF91lQQeaca1vYMYiIiIh3hHNFMz+J5sbA8LkzsxLAVcAvwQ1LRERERPxP8gnfTDM/Q+dXANcBtYAU4BT+vmhFRERERCRXeVY0nXNbgLNDEIuIiIiIZFOkh87NbDw5bwKOc+7SoEQkIiIiIkVCfuZoZn4G2wn4b/a9MTjhiIiIiEhmYTxFM19D5+9mXjazN4AFQYtIRERERAD/k4EiwjjTPJ7HXdYFahd0ICIiIiJStORnjuaf/D1HMwL/E2JuCWZQIiIiIuJ3PFVBrzhqomn+Gze1BBIDq9KdczkuDBIRERERye6oSXIgqZzmnPMFXkoyRURERELIf9P2gn2FSn6uOl9sZm2cc8uDHo2IiIiIZDCzsL4Y6IiJpplFOufSgC7Av83sd2Av/gugnHOuTYhiFBEREZEwdLSK5mKgDTAsRLGIiIiISDZhXNA8aqJpAM6530MUi4iIiIgUIUdLNKuZ2XVH2uiceywI8YiIiIhIJkX1WeclgHIEKpsiIiIiIsfiaIlmsnNuTMgiEREREZEswv0RlHnO0RQRERGRwhPGeeZRb9jeO2RRiIiIiEiRc8SKpnNuRygDEREREZFsLLwvBgrn57SLiIiISBCY2atmtsXMVmdad4+ZJZrZysBrYF79KNEUERER8TALwn/5MAEYkMv6x51zrQKv2Xl1kp9nnYuIiIhIIfBfdR7693XOfWZmdf5pP6poioiIiEh+/dfMvg8MrUfn1ViJpoiIiIiHRVjBv4CqZrY00+vSfITyPFAfaAUkA4/mtYOGzkVERESKn23OuXbHsoNzLuXwz2Y2HpiZ1z5KNEVEREQ8zDxyx3Yzi3XOJQcWTwdWH609KNEUERER8azCuhjIzN4GeuAfYt8E3A30MLNWgAPWAZfl1Y8STRERERHJwjl3Ti6rXznWfpRoioiIiHiVFd1nnYuIiIiIHDdVNEVEREQ8LCKMS5qqaIqIiIhIUKiiKSIiIuJRhXXVeUFRoikiIiLiYWE8cq6hcxEREREJDlU0JWQOpqWzafv+wg6jSGoUV76wQyiS1jw0sLBDKLKi2/+3sEMospbNeqiwQ5ACZUQQviVNVTRFREREJChU0RQRERHxKCO852gq0RQRERHxKgvvq841dC4iIiIiQaGKpoiIiIiH6clAIiIiIiLZqKIpIiIi4lG6GEhEREREgkZD5yIiIiIi2aiiKSIiIuJhYVzQVEVTRERERIJDFU0RERERjzLCuyqoRFNERETEqwwsjMfOwzlJFhEREREPU0VTRERExMPCt56piqaIiIiIBIkqmiIiIiIeZeiG7SIiIiIiOaiiKSIiIuJh4VvPVKIpIiIi4mlhPHKuoXMRERERCQ5VNEVEREQ8y3TDdhERERGR7FTRFBEREfEoPetcRERERIJGQ+ciIiIiItmooikiIiLiYeFbz1RFU0RERESCRBVNEREREa+y8J6jqURTRERExKPC/arzcI5dRERERDxMFU0RERERDwvnoXNVNEVEREQkKFTRFBEREfGw8K1nKtEUERER8bQwHjnX0LkUP19/+hFn9mnH8J6tmfjC4zm2L1/8JecN7UbHhlX4eM70jPVLv/6M0YO7ZLy6NKnBovkzQxm65300fy5tWzShVbOGPDbuoRzbU1NTufDcs2nVrCG9unZk/fp1ACz8eAHdOrWnY7uWdOvUnk8XLQxx5N43f95cWjRrRLPGDRj38IM5tqempnLuqJE0a9yArp06sH7duoxt4x56gGaNG9CiWSMWzJ8XwqjDQ99OTfhu2p2snn43N1zUN8f2h68fzjfv3MI379zC9x/cRfJnD2dsGz2kA6um38Wq6XcxekiHUIbteZ9/soBBXVszoHMLxj/zaI7tS7/5gjP7d6ZFrYrMmzktY/2Pq79n1JBeDO3ZjtP7dGDO9PdDGbYUMFU0pVjx+Xw8fM8NPDPxA6rHxHHB6T3p2vtU6p3YOKNNTFwCdz38HG+OfzrLvu06dmPSzC8A2LXzT87o1ZpTuvYKafxe5vP5uP7aq/hg1jzi4xPo2aUDAwcPoXGTphltXp/wKpWio1m55hfen/wOd99+CxPefIcqVary7vvTiY2L44c1qxk+5FR+WruxEI/GW3w+H9defSWz5iwgPiGBLqe0Z/DgoTRp+ve5nfDqK0RXimbNT78x+d13uP22m3nzrXf58YcfeO/dd1j+3RqSk5IYOKAPq374hRIlShTiEXlHRITxxC1nMeiKZ0hM2ckXk25k5qer+Gnt5ow2Nz06NePnK87uTstGCQBEVyjD7ZeeSufRD+Oc46u3bmbWou/Z+df+kB+H1/h8Psbefh3j3/6QGrHxjBzYjZ79BtKgYZOMNrHxNRn7+ItMeOHJLPuWLl2aB558idr1GrBlczIjTu1C5x59qFCxUqgPwxP8tzcK35KmKppSrKz5bhkJtesRX6sOJaOi6Df4DD77aHaWNnEJtTmx8UlERBz5j8fCOdPp2L0vJ5QuE+yQw8ayJYupV78+devWIyoqiuEjRjJr5odZ2syeOZ1Ro88HYNjwM/l00UKcc7Rs1ZrYuDgAmjRtxoHUA6Smpob8GLxqyeLF1K/fgLr1/Od2xMizmTljepY2M2dMZ/R5FwAw/IwzWbTwY5xzzJwxnREjz6ZUqVLUqVuX+vUbsGTx4sI4DE9qf1Idft+4jXWJ2zmU5uO9ecsZ3KPFEdufNaAtk+cuA/yV0I+/+Yk/d+9j51/7+fibn+jXuekR9y1OVq1YSs069ahZuy5RUVEMPO1MPpk3K0ub+Jq1adT0JCzbd22d+idSu14DAKrHxFK5SjX+3L4tZLFLwVKiKcXK1pRkasTGZyxXj4lja0ryMfczf+YU+g05oyBDC3tJSYnEJ9TMWI6Pjyc5MTFLm+SkpIw2kZGRVKhQkR3bt2dpM33aFFq0bE2pUqWCH3SYSEpKJCHLuU0gMdu5TUpKJKFmpnNbsSLbt28nMTHnvklJWfctzuKqV2RTyp8Zy4kpfxJfrWKubWvFRlM7rgqLlvzs37dapaz7btlJXLXiWXXLLmVzErFxCRnLNWLjSdmcdMz9fL9iKWmHDlKzTr2CDC/smBX8K1Q0dC7FinPuH/exbctmfv/lBzp27V0AERUduZ3b7Pd+y6vNjz+s4e47bmXazLkFH2AY+0fnNh/7FmeWy5Dkkb4lRvRvywcfryQ93d8it9Pojrh3MVMAn7utKZu59ep/c/8TLx51hEm8zdO/OTMbZmbHPQ5hZu3M7Kk82lQys/8cZ//rzKzq8UWXo686Zrb6GNr/o3OTS3+3ZVv+qqD69pLqMXGkJP9dzdmyOYlqNWKPqY+PZk2jR9/BRJYsWdDhhbX4+AQSN/09rzIxMZGYwHD4YXHx8Rlt0tLS2L17F9GVK/vbb9rE6JFn8OLLE6hXr37oAg8D8fEJbMpybjcRl+3cxscnsGljpnO7axeVK1cmPiHnvrGxWfctzhK37CShRnTGcnyNaJK27sq17Zn92zJ57tIj71u9EslH2Le4qREbT3LS/7N333FSV9cbxz+PLCgWBFSkWLBTRBEQFQgWLCj2XmKs0aixRo2/FDV2Y2+xxt7BgiL2gNgLNhQbIjYQG9hQkOX8/rh3cViQJrszu/O8ffnanZnvzJ75sjtz5tx7z/1k+uXx4z6lxTy81n7/3bcc8oedOOL4f7J21+41EWIdohr5r7aUdKIJbA/MdzIVES9FxBFzOKwpME+JppJin7tfPTeS5qdSPUOiGRE95ieoUtdhrS58POZ9Pv14DD9PmcIjg+7id322nKfH8LD5rHXpti7vjxrFmDEfMGXKFO7ufwdb9dtmhmO26rctt95yIwD33j2A3htujCQmTpzIrjtuw0mnnM76PXoWI/yS1m3ddRk16j3GfJDObf87bqff1tvOcEy/rbfllptuAODuuwaw4cabIIl+W29L/ztuZ/LkyYz54ANGjXqPdbuX+xv3L15680NWXWEZVmy9FA0rGrDLFl14YOjrMx232ootaNZkUZ577YPp1z36zFtsukE7mi7RmKZLNGbTDdrx6DNv1Wb4JWvNzl356IP3+eSjMUyZMoXBAwew8eZbzdV9p0yZwhEH7MG2O+/JFtvsWMOR1g11eei8xpKlXKF7W9INkl6XNEDSopL6SHpF0ghJ10paOB9/lqSR+dhzJfUAtgXOkfSqpFUkDZV0oaRnJL0hqXu+b/d83Sv56xr5+o0kDcrfn5x/3lBJoyVVJaBnAavkn3FOPvY4SS/mWP5V8HzekvQf4GVg+YLneqqkIwsun17w+NXPaZsfTAAAIABJREFUy+KSHpf0cj4H2xXcXFH9fM3juTlD0hPAkZK2kfR8PiePSVq24Odfl3/265J2knQW0Dg/1i35uO/zV0k6J5/vEZJ2Kzi3Q3Ocb0u6RXVgPK6iooLjTjqHI/bdiV236M6mW+3AKqu358oLTp++KGjk6y+zdc8OPP7gQM78x1Hs1nf96fcf+8mHjB/3KV3W61Wsp1CyKioqOPeCi9lxmy1Zt3NHtt9pF9p36Mjpp5zE4LwoaO999+frr76ic8fVueziCzn5tDMBuPqKyxj9/ijOOet0eq3XhV7rdeGLzz8v5tMpKRUVFVxw0aVs028LOndqz0677EqHjh055eQTGXR/Orf77n8AX339FR3brcrFF57PaaenFkgdOnZkp112ZZ21OrDt1n258OLLvOK8QGXlNI4++07u/89hvHr3P7jrkVd4a/Rn/POQfvTbsNP043bt243+Dw+f4b4Tvp3EmVc/xFM3H89TNx/PGVc9xIRvJ9X2UyhJFRUV/P208zhoz+3ZdqOu9N1mR1ZdowOXnHMq/3skLQoa8epwNum6Oo8Muod//fVItt24GwAP3383w59/mnvvvJkdN9uAHTfbgLfemDn5t7pBC2LO2iwfWGoLfAD0ioinJV0LjAYOBvpExLuSbiQlbTcCzwLtIiIkNY2IiZKuBwZFxID8mEOB9yLij5J6A/+JiDUlNQEmRcRUSZsCh0TETpI2Ao6NiK0lnQxsDmwMLAG8A7QE2uSfsWb+GZsDO+c4BdwH/Bv4KMffIyKey8eOAboBiwN3R0SXXOl8D+geETOucmB6tXHRiPg2D7s/B6wGrDiL8zUSuHYezs3IiDg0X24GTMz3ORBoHxF/kXQ2sHBEHFV1XERMkPR9RCxeEOf3EbG4pJ2APwF9gaWBF4H1gDWAgUBHYCzwNHBcRDxV7fkeBBwE0LL18l3ve3JE9VNiC8AarZcodgj1UqOKYg9c1F/N1v1zsUOot4Y/MHMPW1swOrZZfHhEdKvNn7l6x85x8Z2PLvDH3XLNFrXyXGr6VfTjiHg6f38z0Af4ICLezdfdAPQGvgV+Aq6RtCMwu4+EtwFExDCgiaSmwJJAf6U5jheQkp9ZeSAiJkfEl8DnwLKzOGbz/P8rpCS4HSkRBPiwKsksFBFjgK8krVN131klmZmAMyS9DjxGSnSr4qh+vnoxb+fmjoLvlwMeljQCOI5fzsmmwGUFsU9g9noBt0VEZUSMB54A1s23vRARn0TENOBVoG31O0fEVRHRLSK6NW2+1Bx+lJmZmdUnNZ1ozlW5NCKmAt2Bu0hzD2e35LT6YwZwKjAkVyW3ARb5lfsWNuarZNar7gWcGRGd8/+rRsR/820/zCaua4B9gf1IVchfsxewDNA1IjoD4wvinem5zeO5KYzvEuDSiOhEqs5W/QzN4ufMzuyGw+fmfJqZmdn8qoH5mXMz0S1PN/xcBQuVJTWX9Kik9/LXZrN7DKj5RHMFSRvk7/cgVfDaSlo1X7c38ISkxYElI2IwcBTQOd/+HWmYu1DVHMFewDcR8Q2polm1lHjfeYyx+s94GNg/x4SkNpJazMXj3EMaXl43P8avWRL4PCJ+lrQxaci8SvXz9dQ8npvqP6fqnOxTcP0jwPQxq4Jfkp8lzWoZ9TBgN0kNJC1DqkC727OZmVktKdJioOtJeU2hE4DHI2I14PF8ebZqOtF8C9gnDxM3Jw1r70ca5h4BTAOuICVMg/JxTwBH5/vfDhyXF7RU9TuZoNR65wrggHzdv4EzJT0NzNMs9zzE/XRe7HJORDwC3Ao8m2McwOwTuqrHmQIMAe6MiMrZHHoL0E3SS6Tq5tsFt1U/X5czb+em0Mmk8/wkULilwmlAs/x8XyPNWQW4Cni9ajFQgXuA14HXgP8Bx0fEZ5iZmVm9lacofl3t6u1I0x7JX7ef0+PU9GKg6YtsFtBjDiUt7nlpTsfWtrwI6GVgl4h4r9jxlKL2ndaJGwcOLXYY9ZIXA9UMLwaqOV4MVHO8GKjmFGUx0Jqd47L+jy3wx928wzJzfC7VczlJEyOiacHtEyJitsPnfhVdAJQap48ilZOdZJqZmVmpW1rSSwX/H1QTP6TGFm/kldgLrJqZH3OjBfl4C0pEjARm2IhVUifgpmqHTo6I9WotMDMzM6vTBCxUM12qv5yP6ux4Sa0iYpykVqQOPrPlVcI1JCJG8MvCHTMzM7P5UptbRs7BfaQFxmflrwPndAcPnZuZmZnZDCTdRtowZg1Jn0g6gJRgbibpPWCzfHm2XNE0MzMzK2HF2OA5Ivb4lZv6zMvjuKJpZmZmZjXCFU0zMzOzElZCczTnmSuaZmZmZlYjXNE0MzMzK1E12N6oVjjRNDMzMytZ8tC5mZmZmVl1rmiamZmZlSoVp73RguKKppmZmZnVCFc0zczMzEpYHS5oOtE0MzMzK1Vp1XndTTU9dG5mZmZmNcIVTTMzM7MSVnfrma5ompmZmVkNcUXTzMzMrJTV4ZKmE00zMzOzEuadgczMzMzMqnFF08zMzKyE1eHuRq5ompmZmVnNcEXTzMzMrITV4YKmE00zMzOzklaHM00PnZuZmZlZjXBF08zMzKxECbc3MjMzMzObiSuaZmZmZqVKbm9kZmZmZjYTVzTNzMzMSlgdLmg60TQzMzMraXU40/TQuZmZmZnVCFc0zczMzEqW3N7IzMzMzKw6VzTNzMzMSlhdbm/kRNPMzMysRIk6vRbIiabVngCmVE4rdhj10vhvfip2CPXSy2MnFDuEemv00POLHUK9deBtrxY7BLPpnGiamZmZlbI6XNL0YiAzMzMzqxGuaJqZmZmVsLrc3siJppmZmVkJq8urzj10bmZmZmY1whVNMzMzsxJWhwuarmiamZmZWc1wRdPMzMysVNXxju1ONM3MzMxKWF1ede6hczMzMzOrEa5ompmZmZUo4fZGZmZmZmYzcUXTzMzMrITV4YKmK5pmZmZmVjNc0TQzMzMrZXW4pOlE08zMzKyEub2RmZmZmVk1rmiamZmZlTC3NzIzMzMzq8YVTTMzM7MSVocLmk40zczMzEpaETJNSWOA74BKYGpEdJufx3GiaWZmZmazsnFEfPlbHsCJppmZmVmJEm5vZGZmZmb1SwCPSBou6aD5fRBXNM3MzMxKlWqsvdHSkl4quHxVRFxVcLlnRIyV1AJ4VNLbETFsXn+IE00zMzOzElZDA+dfzm6BT0SMzV8/l3QP0B2Y50TTQ+dmZmZmNp2kxSQtUfU9sDnwxvw8liuaZmZmZqWs9tcCLQvcozRmXwHcGhEPzc8DOdE0MzMzs+kiYjSw9oJ4LCeaZmZmZiVLdbq9kRNNMzMzsxJWQ6vOa4UXA5mZmZlZjXBF08zMzKxEiaJsdb7AuKJpZmZmZjXCFU0zMzOzUlaHS5quaFrZeW7YY+yxRXd227QrN1154Uy3v/riM+y//UZs2H4Zhjw0cIbb/nPOyezdrwd79+vB4w/cXVsh1xnD/vcIW/TszKbrd+LKS86d6fYXn32K7TfrQfs2TXjo/ntmuO3uO25msw3WYrMN1uLuO26urZDrjFefHsLRO/TmyG17MvC6S2e6/YGbr+IvO23M8btuyqkH78YXYz+ZftstF53Osbv04dhd+vDMw/fVZth1wpDHHqZXtzXpsU57LrngnJlunzx5Mgfvtxc91mlPvz69+PjDMdNvG/nGCLbZrDcbrd+ZTXp04aeffqrFyEvbF28+yxMn7cLQE3fi/Ydv+NXjxr38OIMPWY+JH76V7vfW8zx1xh8YduqePHXGH/jy7Zd+9b5W+pxoWlmprKzk/H8dz7lX38nNg5/lsUF38cGot2c4ZtlWy/G3sy5j0613nuH6Z4Y8wrtvvsZ1A4dxVf9HufW/l/LD99/WZvglrbKykn/93zFcfes9DB42nEH39GfUO2/NcEyrNstz1kVXsvUOu85w/cQJX3PpeWfSf/BQBjz4BJeedybfTJxQm+GXtGmVlVx79j844ZKbOO+uITz90EA+Gf3uDMe0XaMjZ9w8mH/f+RjrbdqPWy46HYCXn3ycMW+/wdm3PcxpN97PoBuvYNL33xXjaZSkyspK/nbskdwy4D6GPv8aAwfcwbtvz/h7e9tN19G0aVOeeeUt/njoEZx28t8BmDp1KocftC9nnX8pQ597lQGDHqVhw4bFeBolJ6ZV8ubt57Duny+k94m3M/bFR/hu3OiZjpv60w+MGXInTdt2nH5do8Wb0u3Q8+j9z1tZe5+TeO36k2sx8tKkGvivtjjRtLLy1uvDWW7FlWizQlsaNmrEpv125KnHHpzhmFbLrcCq7Tqy0EIz/nmMef9tOnfvSUVFBY0XXYxV23XkuWGP12b4Je31V15ixZVWZoUVV6JRo0b0235nHnt40AzHLLfCirTr0Gmmc/vU0MfoueEmNG3WnCWbNqPnhpvw5JBHazP8kjbqjVdpuVxbll1uRSoaNqLHFtvx0tBHZjim47o9WbhxYwBW69SFrz8fB8Cno9+lfdf1aVBRwSKNF2WF1dvz2jNDa/kZlK5Xhr9I25VXYcW2K9OoUSO222lXHh58/wzHPDz4fnbZY28Att5uR556YggRwRP/e5T2a3aiY6e1AGjefCkaNGhQ68+hFE0cM5JFl1mORZdpw0IVDWnVbTPGvzbzNtnv3nclK2++Nws1XHj6dUsuvwaLNF0GgMVbr8y0qZOp/HlKrcVeiqQF/39tcaJpZeWL8eNo0bLN9MvLtGzNF+PHzdV9V223Js8Pe4yffpzExK+/4uXnnuLzcZ/WVKh1zvhxY2nZernpl1u2asP4cXN3bsePG0urme47doHHWFd9/cU4lmrZavrl5i1aTk8kZ2XIvbfRuefGAKywegdefXoIk3/8kW8nfM3Il57lq/E+t1U+GzeW1m2Wn365Ves2jKv2d52OSb+fFRUVNGnShK+//orRo95DiD127Mfmvdfjsotmni5Srn6a+DmLNFt2+uXGzVoweeIXMxzzzcfv8OOE8SzbqdevPs5nr/yPJsutQYOGjWosVqtZXgw0B5K2B96NiJH58inAsIh4rLiRzUxSa+DiiNh5jgfP+v6DgT0jYuKCjax0RMRM12kuP9p177UJb414hT/t1pemzZdizXXWpUGF/4Sq/JZzO6v71ukOxQvaLE/PrM/Pkw/cxeiRr3PSNQMAWHuDDRn95mucuN92NGm2FKut1YWFXHWbbpa/t9WGFX/td3tq5VReeO5pBg95hsaNF2W37fqyVucu/G7DTWos3jpjFr+zhX/TMW0ab/W/kLX2+eevPsR3Y0fzzj2Xse4RF9dAgHVLXX41dEVzzrYHOlRdiIgTayLJlPSbX/kjYuz8Jpn5/lvV5yQToEXL1nz+2S/Vii8+G8vSLVrO9f33OeQvXH/fMC68/h4iguVXXKUmwqyTWrZuw2cFC1A+G/cpLVrO3blt2boN46rdd9mCCl65a96iFV999ksF8+vPP6PZMjOf2xHPP8k9/72E4y68joaNfhmK3OHAIzj79kf4++W3ERG0WmGlWom7LmjVug1jP/14+uVxYz+lZavWszgm/X5OnTqVb7/9lmbNmtOq9XJs0LM3Sy21NIsuuiibbNaXEa+9Uqvxl6pFmrXgpwnjp1/+ccLnLLzk0tMvT508ie/Gvs/z5x/KkL9vz8QP3mD45cdOXxD044TxDL/yeNba9yQWW2a5mR7f6o56nWhKaivpbUk3SHpd0gBJi0rqI+kVSSMkXStp4Xz8WZJG5mPPldQD2BY4R9KrklaRdL2knfPxYySdIelZSS9J6iLpYUnvS/pTPmYjScMk3ZMf+wpJC+Xbvpd0iqTngQ0kdZX0hKTh+XFa5eOOKIjr9nzdhjmmV/NzWSI/3zfy7YtIui4/x1ckbZyv31fS3ZIekvSepH8XnK8xkpaWtJikByS9JukNSbvN7fMtde06deHjMaMZ+/GH/DxlCo89cDc9+/Sdq/tWVlbyzYSvARj19pu8/86brNtr45oMt07p1LkrY0a/z8cfjmHKlCk8cO8A+mzeb67u22ujTXl66ON8M3EC30ycwNNDH6fXRpvWcMR1xyod1+azjz/g808/YurPU3jm4YF03XCzGY754O03uPr0EzjuwmtZsvkvb+jTKiv5Li+s+vDdkXz03tustf6GtRp/KevcpRsfvD+Kj8Z8wJQpUxh4151svuXWMxyz+ZZb0/+2mwAYNPBuevXeCEls1GczRr45gkmTJjF16lSefXoYq6/RvhhPo+QsuWJ7fvj8YyZ9OZZpU39m3EuPsuxavaff3rDx4mx27iNsfPq9bHz6vTRdaU26HnIuTVdsz8+TvuOly45hje0OpfkqaxfxWZSIGpifWZsDRuUw7rcGcEBEPC3pWuAY4GCgT0S8K+lG4JD8dQegXUSEpKYRMVHSfcCgiBgAsxyu+jgiNpB0AXA90BNYBHgTuCIf051UFf0QeAjYERgALAa8EREnSmoIPAFsFxFf5OTudGB/4ARgpYiYLKlpfsxjgcPy81ocqN5T4zCAiOgkqR3wiKTV822dgXWAycA7ki6JiI8L7tsXGBsR/fJzXnIen2/Jqqio4JgT/80xB+zMtMpK+u28Fyuv1p5rLjqDdmuuQ68+W/LW6y/zt8P25rtvv+HpIQ/x34vP4ubBzzJ16s8ctudWACy6+BKceM6VVHjofLqKigpOPOM8DthjOyorK9l5jz+wWrsOXHT2qazZuQt9tujH668M57D9d+fbiRMZ8uiDXHzO6Qwe9hJNmzXn0KP/yk590xvRYcecQNNmzYv8jEpHg4oK9vvrqZxx2F5MmzaNjbfdjeVXWYM7Lz+HlTusTbcNN+eWC09j8qQfuPD49Jlv6ZZtOO7C65g69WdOPmBHABovtjh/Pu1iT/koUFFRwennXMieO21NZWUlu/9+X9Zo34F/n/4v1l6nC1tstQ177L0fRxy8Hz3WaU/TZs25/NqUdDZt2oyDDzuSrTbpgSQ22awvm26xVZGfUWlYqEEFHXc/lhcuOQKmTWO5HtuwROuVeff+K1lyhfYsu3bvX73vh0P7M+mLTxj14LWMevBaALoffjELNynn14S6O3iuWc6NqicktSXNp1whX94E+CfQICJ65+v6kJKyXYHhwEvAA6Tkcoqk65kx0Zx+WdIYoGdEfCppf2CDiPhjPu4jYC1SUndKwc/bH1grIo6SNBVYOCIqJa0JPANU9X9oAIyLiM0lPQR8D9wL3BsR30s6gZQY3wLcHRGf5Oc7KCLWlHQPcElE/C//3Cfz8+ySY66K80Hg9Ih4Kj+fbkBz4GHgzvx4T+Zj5/h8qw+9SzoIOAhg2dbLdb1r6Ovz8k9oc6lFk4XnfJDNs5fHusVSTem98jLFDqHeOvC2V4sdQr01+JD1hkdEt9r8mWut0zUG/+/ZBf64yzdfuFaeS70eOs/mKpOOiKmkyuNdpHmZD83l40/OX6cVfF91uapsUD2Gqss/RURl/l7AmxHROf/fKSI2z7f1Ay4DugLDJVVExFnAgUBj4LlctSw0u48/hXFWUq2yHRHv5p81AjhT0onz+HwLH+uqiOgWEd2aFgznmZmZ2ZyJuj10Xg6J5gqSNsjf7wE8BrSVtGq+bm/giTz8vGREDAaOIlUiAb4DlviNMXSXtFKem7kb8NQsjnkHWKYqVkkNJXXM91k+IoYAxwNNgcUlrRIRIyLibFIVtnqiOQzYKz/W6sAK+WfMkdLq9UkRcTNwLqkKamZmZjZPymGizlvAPpKuBN4DjgSeA/pLqgBeJM0tbA4MlLQI6QPE0fn+twNXSzoCmN8V3c8CZwGdSAngPdUPyMP0OwMX5zmRFcCFwLvAzfk6ARfkuaOn5gU+lcBI4EGgcJnuf4ArJI0ApgL75jmecxNvJ9ICqGnAz8Ah8/OkzczM7LeruzM0yyPRnBYR1VdEP05aDFNoHGnofAYR8TQF7Y2AfQtua1vw/fWkxTEz3JYTu0kRsdssHnvxapdfBWY1Q3qmbrYRcfgsjhsDrJlv/6kw1tnEuXXB923ztw/n/6vft23B99Ufp231483MzOy3q8tthcth6NzMzMzMiqBeVzQjYgy5wlfEGIYCQ4sZg5mZmdVd1Xerqktc0TQzMzOzGlGvK5pmZmZmdV7dLWi6omlmZmZmNcMVTTMzM7MSVocLmk40zczMzEpVbe/ks6B56NzMzMzMaoQrmmZmZmYlzO2NzMzMzMyqcUXTzMzMrJTV3YKmE00zMzOzUlaH80wPnZuZmZlZzXBF08zMzKyEub2RmZmZmVk1rmiamZmZlSzV6fZGTjTNzMzMSpTw0LmZmZmZ2UycaJqZmZlZjXCiaWZmZmY1wnM0zczMzEpYXZ6j6UTTzMzMrITV5VXnHjo3MzMzsxrhiqaZmZlZqVLdHjp3RdPMzMzMaoQrmmZmZmYlSvn/usoVTTMzMzOrEa5ompmZmZWyOlzSdKJpZmZmVsLc3sjMzMzMrBpXNM3MzMxKmNsbmZmZmZlV44qmmZmZWQmrwwVNJ5pmZmZmJa0OZ5oeOjczMzOzGUjqK+kdSaMknTC/j+OKppmZmVkJq+32RpIaAJcBmwGfAC9Kui8iRs7rY7miaWZmZmaFugOjImJ0REwBbge2m58HckXTzMzMrESJorQ3agN8XHD5E2C9+XkgJ5pWa95549Uve63e/MNixzEPlga+LHYQ9ZTPbc3wea05Prc1py6d2xVr+we+/PLwhxs31NI18NCLSHqp4PJVEXFV/n5WqW3Mzw9xomm1JiKWKXYM80LSSxHRrdhx1Ec+tzXD57Xm+NzWHJ/b2YuIvkX4sZ8AyxdcXg4YOz8P5DmaZmZmZlboRWA1SStJagTsDtw3Pw/kiqaZmZmZTRcRUyX9GXgYaABcGxFvzs9jOdE0+3VXzfkQm08+tzXD57Xm+NzWHJ/bEhQRg4HBv/VxFDFfczvNzMzMzGbLczTNzMzMrEY40TQzMzOzGuFE08zMzMxqhBNNs3kkzbxHw6yus9rhc79g+XxafVb1+y2pYfXrrGZ4MZDZfJLUDVg4Ip4udizlTlJfoBswGrgnIn4sckh1giRFREhaE/gZ+DIivpK0UERMK3Z8ZjVB0lbAFsAPEfG3YsdT37miaTaXCj/1SjoauB64RNItRQvKkLQ68G9gUeB3wIWSFi1uVHVDTjL7AE8AxwP9Ja0UEdMk+f2hjiio0nWS1Dk32LZZkNQWOB94EthM0o1FDagM+IXEbC5UVX7y942AxsB6EdEFaC3puqIGWGYK3ljbAl2AU3Jl4kJgMnC+pMWKFmCJKzh/TYAWwDYRcQDwP+B2J5t1R0FVelNgAHA1cJKkrkUOreRI6gy0Ai6NiAHAeqTX7+uLGlg95xcRszmolmQeQdqGa3ugZz5kc2B5SQOKFGJZKXhj3QK4BzgJ2FnSYhHxDnApaSeLS5wozVo+f32BW4BDgFb5vJ5G+v0eJGllD5+Xvvxv2RU4lDQcvDXpg/D2TjZn+FDVi/S7fSpwkKSN8u/3ZsAakm4rYpj1ml+EzeagIMlcH+gFXAE8BWwrqWdE/AxsCSwiqVXxIi0PBW+sBwI7ATsASwH7S2ocEe8C5wLnOlGatYLzdxMwljS/tTNARJwO9CdVfqzESVqElFxuDFRExHjgP6SpJLtL6l7M+Iotv170BPqSXi92B24AdpPUO7++9yCNhlgNcKJp9iuqzcnsQ3rzfSwi7gWuAT4C9pS0cUT8HBFbR8S4IoVbNvLUhR1Ib6w/R8TbwD+AbYDDc7L5TkSMLGacpSp/GLoDGBURdwLHAosDu0haFyAiTvYit9JVUKVbKCJ+Ai4hVafPlLR8RIwifSBeGJhYvEhLxv6kD1ZExJfAIOA94IBc2YyIeL6YAdZnXnVuNgvVhsuXjogv89BKK2DTiJgqaQ1gD6AJ8Hfgp/AfVI0oGC7vArwFNCcNmTcBjo6IcZI2AM4C9omIMcWLtvQUnL8mEfGtpL+SEsx+EfFCTj5PA74GTo2Ib4sasM2RpH6kofJmpAr+NGA7oANwQkR8lD90lV0HhoLf93VJr8sjJN0ALAPsEBGTJbUDtgUGR8QbRQ24nnOiaVZNtSTzaGA1UjIzWdI9QEPSi9XPklYDvo6Ir4oYcr0mqUFEVEraklSl2SUnRysCfwKWB/4aEZ9KWjIivilqwCWm4E13W1JV548RMV7S4cABwMER8byk1kCziHizqAHbHElaD7iRNC9zW1Ll8mFgOHAUsAKwF6niX1bTRwp+3/sBFwAHRMST+babSR9Sd4qIHyUtGhGTihlvOXCiafYrJO1PGnLZMSI+L7j+v8DqwCZ5fqbVgKpKcv5+DWAgsEdEvCJpBaCSNOR7ANAW+D1QGRGVRQq5ZOUVyecCh0TEs3n6wc/AH4ATgb0i4rlixmi/Lv/+t4uIgfnyn4HVI+KIfPmPpNGVbYCmwGJ5rnLZkLRU1Qd+SauQVuDvERFvS1oLaBIRT0m6nZRsbuERqNpRUewAzEqFpA2BthFxQ76qM2neU1NJ+wC9gdci4gBJNwEtgY+LE239lhc4HC3pmoj4APgReBRoJ2kX0krRSaQk6ULSm8iUogVcYnJ18tiIOCZftSZwJfCtpL1JCebDpKrYkqQqvZWuRsDEgmRqJPA7SR0iYmREXC1pG1IyOry4odY+pV1+rpB0bER8CIwDXgAOkzSJ1AKtUtKDEbG7pC5OMmuPFwOZ/WIMMCRXDyAlNruShmsnkxq0t5BUERF7R4STzJrzM3A2MFXSSRHxEfADqWLzMqkx+7NA94gYmxcE2S8+A66X1Da3eHoH2Ii0yrwpKclcHWgUERdXDS1aaYqIEcDTwGhJB0fE/4CvgH6S+krqRPr3/L6YcRZLHlnaG1g4v15MIrU+Wwp4jPQ6fgOpZywR8XKxYi1Hrmha2csrOBURH0paAnhR0nkR8W9JzwOTIuJ7STuR2sDSXTAbAAAgAElEQVQsSXqRtxqSh7+/ldQBWE/SkRFxQtXtktYmtSs5qlgxliKlBvbHR8ShwOt5AVuTiOgn6UWgcUR8nD9M3UlKOj8pWsD2q5Q2HNg4IgblOZk/kH7n75f0BfBP0hzlP5Mq0idE6iNbliLiJ0lB6pH5Q0ScCzwEkNsbHUfqTmG1zHM0rezlFiHTcqVyal7gMwC4MyJOl9QY2AX4G2kSuRdL1JCCifzNgB/zm0c34K/AmxFxcl55fhGpT+bAogZcYvIQ4gfAIxGxf758Hamn4s7593wbUrX4hIi4r4jh2hxIuhVoT5qP/Mc8P7kH8CBwYET0l7QwaRHXZ4ULGctBwevF8gD5Q9RKwP3AHRFxqqT2pG4U10XEveV2jkqBE00rW5I6knoJTs6T6XsCr5JepH4EHgFuiIhz8mKKMZH601kNyqujTyL1uXsqIi7NbUqOIv17naS0a81ov2n8ouCDUkPgFeCViNhbUgVp2kdjYGdSc2oi4mmfv9JU8OG3I6kq92FE9Cq4vScwDDg8Iv5TrDhLQV5dfhppHuuFeb7qiqTFg/fmD6ctIuJz/74XhxNNK0uSFiWtwl2ElFD+kbQwYh1gCdK8zPHAi6R9tC8rUqhlRdKqpGrbVaT5ZleQkv1zlXZm+itwTF4gZFlBZadRREzJyeaLwBsR8fucbN4JEBE7FjVYm62Cf8u1geWAd4Frga8iYvuC49YEWkXEo0UKtejyOToDOILU5uwvwEMRcVmubD4EbBkRo4sYZtlzomllKc/L7AjsA2xCGkZ8VFIbYCvS6s2/5LlsUyPi/SKGWxZyS5Lrgbci4qB83Zqklf93RMQZcp/MmRQkJhuRFkl9S1r08w1p5e2IiNg3J5trRsSrxYvW5oZ+6QH5+4h4IV/3LKnLxTmkD2M7RsTEcq3SSVqaNPLRC+iS/wY2Jo18DI2ICyQtEmnnJCsirzq3shTJG6St2yYA/5S0cER8CjwBdJHUKtJWhk4ya0hO+AHI5/leoIOkrnko+A1SK559c4XCO9ZUU5Bk3gB8DvQhLRRZF1gP2FDSLREx1Ulm6cvzDf9KTjJzdZqI2ABoAJwOXBQRE/P1ZZNkFr5ekHax6g98SHr9bhwRQ0iv6VtIWtFJZmlwRdPKklIvxv0jYkulnoP/Is1hO5LUc+000vZ8XxYxzHqtoBLXE1gLGAsMJk1j2IrUI/O1SLsCLR4RZdm6ZXaqOiaQVtN+HhFXSGpBat69akQcrtScff2IGFbMWG3uSGpKmjJyHKkfJHnu7aIRMUlSs4iYUG6VzILXi81IrxeLkaq+vUh9db8Czo+040/ziPi6iOFaAVc0rSxU+yRMRPQHWku6PSLGkhLNZUiNkPcG9nOSWbPym8aWwGXAysAOwCDSKukHSUOEnfOxTjILSGoA0yvz04AvgK0ktYy0i9VtQHdJa0TElIgYVv1vwEpD1b9Lwb/Pz6Qkat1chZ6a5yefobRX/QQov0pmfr3YELiU1Ce2N3Aeqf/xfaTXkOOV+sZOLFasNjNXNK3eK/zkn+dc/lw1OVzSC8CnEbFDHrI6CLgq3Iy9RkhaBlgqcoN1SVcAgyPivvwGcRqwdEQcJOlU0qrRstvp5NdIWiIivsvfdyc16X6B1A9zC+A7UpK5OGme5q4R4T6ZJaoggepLaio+ktT1oilwNemD1w+kfcvLrh2VpJWBJSLitXz5XOCzSD0ykXQp0CIidpW0BfBxRIwsXsQ2K65oWr1WLcnckjTEuKmk5QAiojvQVdJ9Obk8xUlmzchDuLsBU5R6k0Lq77h61SHAreTtECPin04yfyGpCfAvSbtK2oA0J7MfaQvO3qT5qy1I1Z0bScOITjJLWE4ytyL1eRxA2hDiv8AUUoX/Y9LGKofmD2PlVpVeC1hEqXk9wPvAMnl6ARHxZ2ApSctExMNOMkuTdwayeq0gyfwTqX/gk6R+mVMlPRZpa8OLgYPz4p9xxYu2fsttd24gDQueKOka0tDXXZLGRsStSo3aV5fUChifh4UtqSC1uukNrATskxeLbE76nX4jIi7KLaImR2peXVbz+OoaScuSul/sAqxGGv69lbQhwT8i4vLC48vt3zJSg/XmwAuSDiBtJ3kBsJGk4aTK71LkD6dWmjx0bvWeUgPw04Gt8pvv1qTG1R8AC5Ne3I+MiPFFDLNe0y8NqBcBWpFakHxLmo/ZgtTj8UHSftzHRMQDxYq1lElairTf+wlA/4j4Z77+T8C2wDaRtu+0Epdb8XQjVTJ/Bm4HDgA+Ah4nVfi3A74stw9cBVMKFom0O9ifgf2APUlz6Q8m9TtuCZwVEfcWMVybAw+dWzloDdyWk8yKiBhEaoD8KbAicJqTzJqT3zSm5dWip5Mm719LqtAdSHpj7UoaAt4pIh4owyHCX1WwWGQpYFJEXE8aal1R0j75sOdIrW+WLEqQNk9y1flAYFCkzQcWInVd+BDoAIwgLUj8vIyTzG2A23LHiUtJmzjcCUyIiL1Je7zvnauefr0oYR46t3LwIbBdXoH7Tr6uJfBJROxVxLjKQn7T2ILU3+6gPPz3mqSvgUNJbVxujIhXCu9TnGhLTz5/OwB/B76QdFdEXJPfWw+XtCOpMv8ft3QpTZIaVFWa87SQG0j9e6tWkH+k1C/zdlJ7tcOqFsyVm4LFUacCR0fE97lAcKWkn4D7JO1X2K7LrxelzYmmlYOnSXPY9pH0DNAEOJo0DGM1LC8C2pW0oGFoTpp2Ig2bX0jaNu6HIoZYkgoqOw2BzUm/s5OAByU1jIjL8229SXs8D/eczNKTp4t0lfQ2qXL/A2kO5uFAD0kPRsSPufNFB1JXjPfK/N+yK3AKMEbSbsCBkq4jVTQXAcr1vNRJnqNpZSFXEbYjzWP7BjgzIl4vblT1V0GS1Doixko6jFSRew4YTWrDszPpDaVRuE/mLEnqA2xMqsAfHxFfS+oEPAqcHWmbvTaRdrSyEiRpUdIwb09SX9g9I+JpSXuR5mReADweEZOKGGZRFbxedATeBg4hfbhaCrgLaE5aLHVoRHxVeJ9ixWxzz4mmlZVcXSMiphQ7lvqq4E1jW9Kk/aMj4t08xDsyIt6WtCJpnubOkRtQW1LtTfdq4GXSyuS7gQERMU5SZ2AYae/yj4oYrs1Gwb9lJ9L2qs+Ttpf8NM9b3oO0MO5MYGA5J055keaJpB3b3lDqE/tFRHyQ+2neDuwVEe8VNVCbZ040zWyBk9SLNCfzDxExIg8fNoqIb3PCeTJwckTcXcw4S1Xuk3k58JeIeDwPH64PjALuyVVib8tZwgqSzA1IFblRpO1VpwJ3R8QL+bg/AO9ExPPFi7a4ciLen/TB843cJ/OnvOJ8B9Iw+j8iYmBRA7X54lXnZrbAFKz+XBV4CmiQh80HANflOWjNgf+LiLu9WnRmklYAXiN1S/gjQETcATxDamC9S67M/5iP9zksQTnJ3J60xeqkvBDxPKAxsIOk4yW9CDxYrklmwe9uG+BVUn/jf5B2RBqZOy1UAIdHxED/rtdNTjTN7DcreANokr++Qnp9uZVUwbkceAdYJCKuqWphVM5DhbMiaXHSnL0VgE7AhpLOAYiI/sAQ4NFI+5dX5ut9DkuQ0pa2xwHbRsQQSe2B9qRFXePy92dExBdFDLMoCl4vlgaIiIdIu4T9h9TmaSvS7/qGEdE/Iobm4/y7Xgd51bmZ/Wa5erM1sKekMcBLpO0+lRewrAPsSKpsTr9PUYItbUEaYv1dRFwtqQvwlKTGEfHniLi9yPHZr5jFB6eppKbrB+cPEMsDmwCn5kVcjSLtllV2H7jy68WWwP/lTiDTImJb/dKgfS3SVJErixupLQiuaJrZb5bnoZ1JSi7XJ7Uv+hH4Mc/XvAM4NiJeLl6UpUtSO6X9mn8gLRo5VlLXSFui9gJ2zMf4NbtEVSWLktpLWgX4HjgSWBN4hLTN5M5AM6WdsqYU3q+cSFqXtOnAoaRdkTbMc45/ktQbuAc4oWoeq9VtXgxkZr+ZpD1JIyTvA+cDu0bEh5LaAM1IC4GcZBYoWCyyCrAPsD3wN9I5XI+0A8rAfGwjd0ooTZJaklpN7SNpPVJ/2NGkNmq3Rt5OVWlnrAtIbaoGFy3gIin4fW9EamvWGvgaOBvYLa8u7wS8Reqm8Go5VnvrIyeaZjbfJPUjTeQfAZwLLE7aU/5TSTuREqZ/RsTkIoZZsnILqJNJc9I2AtqShldXJW3V2Se/OS8UZbYVYV2Rh8VvJY0QfkX6OxhP2sf8YFKLquGkqv65EXFfkUItmoIkc1NgHX45H18A3SJiktLe738AjoqIb4oYri1gHoYxs/kiaW3SG+lLpHmF75FalCyde+CdCDzpJHPWci/MU0gNvD8DHgQGk4ZYrwGmAPvlw10RKFG5xdTvgY+BLYHPIuJzUpeA4cBKeQrEThFxXzmunM5JZnfSh6jnIuJ/pKk2E4AOkrYi7ZZ0j5PM+scVTTObZ5KWBf4FdIqInvm6jYAepDeTH4Brq1qSePhrZnkV8l+BZ4FlgD75pvOAh0jJS+uIOKM4EdrsVP+9zkPCDwBTI2LLfN2RpN2ADiDlW2X3dyCpAamoNRKoBNaOiMm5V+bupHmrnwO3uBtF/eRE08zmSuEbQH5T7Uva1eTBiDin4PoGwMIRMdFvGr8uD7nuC+xBSi7fIiWb30TETZKOIr0RbwL86PNYOgqGgvuSFvtERJwnqTFwA9AOOAfYG7ggIh4sYrhFUXCOGkbEz5KWJu2M9EBEHFFwXEPSqvNKv17UT040zWyu5QUNawGTScO7W5H24n4vIi4uZmx1VUGbm26khSRHxS+7AY2MiBFFDtFmIQ/3nk3al/teUuuuw0nVu0GkdkZ7RMQr5ZpA5deL3Uj7lw8i9cgcTtpu89h8TFmem3LiRNPM5kqeY3UDaS7VgaSGytcAq5PmFb4cERcWL8K6KQ8tdiY1qz4jvM1eyZPUBLgJ+Cdp9fTJpHm0n5Aq1I2BNhHxdrFiLLa8Av9c0kKpjsAipDncz5E6K9wYEccUL0KrLU40zWyOcgPlPwMvRMQ1SnuXXw18GxGHKe1f/m5EvFHUQOsoSYsBLXKLF0F59lesSyQ1A5YFbgQ2ICVSE0kfGI4u5y4BklYCbiMlk/+R1ALYENg0Ig7Oc7w75kVBVs951bmZzY01SFWJDSS1iYifgIOAbkr7Ed/rJHP+RcQPEfFB/r4sF43UNRExgV+qmM2AVUhVzgHlnGRm04BPgUMlLZtX4T8OtJO0VkSMj4j/leMK/HLkRNPMZlL1BpB3o1mKNP/sBNJ+xJtKagusROqb2chvrFamJpHaGl1K2s3mxoh4stwSqILXiw6SekTEh6RNCB4FLpW0MrAksBRpJyDAVfty4aFzM5ulPJH/ZlKrnZ+A44HuwDFAI1Kz5Zuqdj4xq68kNYmIb3/ltlVICVSDiHi2diMrHXnv8stJuyI1ArYgfRA9D9gUGAZcFBFPFy1IKwpXNM1suoLKxJKkti07AH8n9bm7HHiB1GR8AvAYqcm4Wb2V5xserqRB9dsj4v2IeKEqyZzVMfVVwetFI9J81Z0iYhNSlXcAqeJ7BHAVMJX0+kG5VXzLnRNNM5su973bHDgL6EdqvP4pcCVppegNpFYlt+TbdyqnN1YrS61JTcXbRERl9Rurfv9zssWsjqlvChes5TZPj5J2seqer98D+JK009VU4L+kD6eXSarwkHl5caJpZkiqyF/XBf6PtH1eE9IbbNOI+IRUlXgLWD4i7iENqz9dDm+sVn5yCyPyUO9A4FhJC1c7pkFuNN4UGCZp+SKEWusKNm5YHdiT9NowBOgiaft8zN6kvpmr5Tmb/wb+ERFTixO1FYvnaJqVMUmrAuMj4rv8JnkbMCgizsoLfi4BXgEujIivJS3svcutPpK0AnBoRJyQ/y4OAyqA44CuwNbAKRHxY67oLZSTzCVJw8SnRsSwYsVfGyS1JDWiHw6sTFpJfnNE/D2fvy2BdYDHI6J/8SK1UuKKpll5WxbolN84PyW9gewjqXNEjAEOBXqRqjkVTjKtHvsa2F7SuaRh3xuBxUhzkzcD/kj6e6hqQVWZe2neQ0pA63WSmW0NfEvaYnYUaVRje0nLRcRHpL3eRwJbSlrWczENXNE0K3uSlgBeA7pGxARJfwe6ASdHxGu5UtEiIl4qaqBmNaRgP+4WpGHy5yPiqHzbhqQ+mSeSkqgjcnV/IeAk4In63nhc0kJVLczyOTqTVMkcIuls0ja0O0TEp3lkRDnxNHOiaWYgaTvSAqD1SRWLvwKbAMdHxKvFjM2sJlXttZ3b8/QmbZs4CHgtIv5UcFx74DLSEPmQfN0iefOCektSY2DliHhTUmfgG9IWtEuRks2nJJ0BbA9sFhGfFjFcK0EeOjcz8v7afwFeIi0COht4EvCKcqvXcpK5AbAt8EhEfAVsDqwl6eKC494iVTR75Gom9T3JzFoBu+Rz8QBQGRF/J0212VdSz4j4W76tbfHCtFLlRNPMAIiIwcDhpPZFTSLi1IgYXuSwzGpM7o0pYHfSTjajACLiO1KyuZGky/OxS5AakN9bTjthRcRo0urxg0k7H32Urz8V+AA4RFLviDjOzdhtVjx0bmYzkNQP+CEihhY7FrOaUDBcvkTuuLAIqUXPckDfiJiSj1sC6BQRz+TLFeXSnqfgHHUgLYpqB/QEXiRVfj/Ow+rHkpLvEUUM10qYE00zm6WqN5pix2FWE/IHqt2Bj0jbI75AWuTSBtilcFi8XP8W8tzto0lztV/Il3cjDZNXAF2Av0XED0UM00qcE00zMysrknqQKpg7knatGU1a4NKQtOBnOWDzckwuq0jqSOqru2NEjJLUnLTLz2rAH0gLB891v0ybk4piB2BmZlbL2gGnAc2BhYETc3ujpsAhwKrlmmQWVG+XBT4HWkjak9RPd13SNpPHkuZxf1Wu1V6be14MZGZm9ZakxpI2z993ztXMUaSFb1cA20fEh5J2Bo4BpkbEG8WLuDgKmqsvlb8OIXWhuIhU8d0VuABYNyJ+zqvzcZJpc+KKppmZ1XdbSDoRWIQ0L3MqMAZ4A1hcUjfgn8Dfy2WxT3V54U9f4BhJn5HOz1kRcQKApPVI+5rvX7worS7yHE0zM6vXJG0K3A4Mj4gt8nVbAusBGwKTgcsjYmC5DgXnOZkDgf2AJUi7g3Ug9dddGrgT+EtEDCpakFYnOdE0M7N6pzBhzAtZ1gCOIg2N75Wvbw2MJ803nFBuSWa1c9QF+GNEHJIb0rcmbbF5O2kYvV1EjCy3c2S/nedomplZvZOHgreQdD6wd0Q8C/wZWETSTXm4/HygaURMqLpPEUOudfkc9ZS0N7A2aQegLSNiWkR8QppisGK+PLLqPsWM2eoeJ5pmZlZvSGqQv64PnAe8CRwm6RzgS2BfUhujK0h7dX9VpFCLpmrhTz5HlwNbAB2BT4B/STpIUi+gB/BW0QK1esFD52ZmVuflYfBJETFR0hrACcALEXG5pKWBAcCLEXFcPr5lRHxWrkPBkrqTGtT/LSKel7QyaWV5T6Ax8CFwf0TcW8QwrR5wRdPMzOo0SRXATqRdfSAtZmlG2qt8lYj4Mt++saRLACLis/y17JLMbElgI6BPvvwRqfr7NqlZ/QERcW9B2yOz+eJE08zM6rTckuhq4CtJNwHvktoVfQ5sL2mlPES+BXBL8SItHRHxKGlnpP0l7ZHP4Tek5HPpqgSzjBNxW0A8dG5mZnVWtZXTKwHHA42AI4H2wF7AF8BtETG6aIGWKEnbkJLvB4FJwF1uYWQLkiuaZmZWJ1UlmZJaSlokIj4gVTK/BS4lLWS5ndSqx1WVWYiI+4Hfk/YwHxERg5QVOTSrJ1zRNDOzOkvSVsApwDCgIiKOkLQU8H/A8sABQIOI+KaIYZa8vE3ntcAREXF3seOx+sMVTTMzq5Mk/Q44G9ib1Hh9d0n983zMs/N1KzvJnLOIeIS0K9CrxY7F6hdXNM3MrM7Ju9f0JvXGbA2cShoCvh74JCJ2k9QwIn4uXpRm5oqmmZnVCQWNxhsCRMRQ4D1gd+CMiHgPeAJYU1JnJ5lmxedE08zM6oS88Gcb4F7gHkl7RsRk4EdgRUnbAu2A7SLCQ8BmJaCi2AGYmZnNjbzjz2GkPcoXB86V9CPw33z9zsCFETGqeFGaWSEnmmZmVpIkLRQR0/L3HYBLgHfzwhUkfQbcDWwIHAwsHBE/lOu2kmalyEPnZmZWciQ1JjVcR9JawE+k+ZhrSGonaeGIeAa4DVg2IqZGxA/g3WzMSolXnZuZWcmRtDLwB6A5aZ/y9SPiY0lXAw2Bx0n7c98E7BYRzxYtWDP7Va5omplZycnbRY4lDYnfCHySr/8j8D3wF6AvsE9EPOudbMxKkxNNMzMrGQUtjDoArwAHAs2A/SWtkA87DngSaAm84zmZZqXLiaaZmZWM3MJoO+A/pOldNwEPAn2A30naBziTtMVkY+BveGGrWcnyHE0zMysZkjqSFvjsGBGjJDUHpgKrkeZsrg9cEBG35+NbRcS4ogVsZrPlT4FmZlZ0BcPfywKfAy0k7Qn0AtYFugPHAk0i4quq7SWdZJqVNg+dm5lZ0RQs4lkqfx0CvARcBIwGdgUuANbNieVXAN5e0qxu8NC5mZkVlaS+wDHAZ8AY4PyImJhvWw+4Adg/9800szrEiaaZmRVNnpM5ENgPWALoBnQgtS9aGrgT+EtEDCpakGY23zxH08zMalW1dkQLA49GxJOSFgJeB04C2pGG0XeIiJFuYWRWNznRNDOzWpVbGPUEVia9D+0i6b6IeBD4RNJUYMW8z/nIqvsUL2Izm19ONM3MrFZUVSUlrQ9cTqpefkba9edfkpYnJZY9SLsBmVkd5zmaZmZWayR1JzVc/1tEPJ/3NN8V6ElqwP4hcH9E3FvEMM1sAXFF08zMatOSwEaknX6eBz4C3iS1N/prHi6vPo/TzOoo99E0M7NaExGPAjuS9i7fIyKmAt+Qks+lq/pqOsk0qx88dG5mZrVO0jbALaR9zCcBd7mFkVn944qmmZnVuoi4H/g9aQ/zERExSFmRQzOzBchzNM3MrCgi4j5JPwHXShoTEXcXOyYzW7A8dG5mZkUlaTPg/YgYXexYzGzBcqJpZmZmZjXCczTNzMzMrEY40TQzMzOzGuFE08zMzMxqhBNNM7P5JKlS0quS3pDUX9Kiv+GxNpI0KH+/raQTZnNsU0mHzsfPOFnSsXN7fbVjrpe08zz8rLaS3pjXGM2sfnGiaWY2/36MiM4RsSYwBfhT4Y25LeQ8v85GxH0RcdZsDmkK/9/evQfrNd1hHP8+EtJI0gRtdQQjdb8HEbeRuESGUmTEqEtblRFMSTGUTjGoKcEwYzCuk1CK0iCliEsQISpyRS6MuATjUqIiKPLrH+t32F7nnPcVXuTk+cycOe9Ze7177X3++s1ae6+Hr1xompl921xompl9MyYA6+RM3ixJlwJTgDUkDZb0mKQpOfPZHUDS7pJmS3qEEstIth8q6eL8vKqkWyVNz5/tgXOAtXM29bzsd6KkJyTNkHRG5Vx/kjRH0n3A+vVuQtLheZ7pkv5RM0s7SNIESXMl7ZX9O0k6rzL2EV/3H2lmHYcLTTOzr0lSZ2APYGY2rQ9cGxFbAO8DpwCDImJLYDJwvKQfAFcCvwB2BH7axukvAh6KiM2BLYGngZMp+072jYgTJQ2mJOz0B/oCW0kaIGkr4JfAFpRCdusGbmdMRGyd480ChlWOrQUMBPYELst7GAa8GxFb5/kPl9SngXHMbBngZCAzsyXXVdK0/DwBuBpYDXgxIiZl+7bARsDETFdcAXgM2ACYFxHPAki6Dhjeyhi7AL8GiIhPgXclrVTTZ3D+TM2/u1MKzx7ArRGxKMcY28A9bSLpLMryfHfgnsqxv0fEYuBZSc/nPQwGNqs8v9kzx57bwFhm1sG50DQzW3IfRETfakMWk+9Xm4B7I+LAmn59gW8qMUPA2RFxec0Yxy7BGKOBfSNiuqRDgZ0qx2rPFTn2MRFRLUiRtNZXHNfMOiAvnZuZNdckYAdJ6wBIWlHSesBsoI+ktbPfgW18/37gqPxuJ0k/BN6jzFa2uAc4rPLsZ29JPwEeBoZI6iqpB2WZvp4ewGuSlgcOrjm2v6Tl8pp/BszJsY/K/khaT1K3BsYxs2WAZzTNzJooIt7MmcEbJHXJ5lMiYq6k4cCdkt4CHgE2aeUUvweukDQM+BQ4KiIekzQxtw+6K5/T3BB4LGdUFwKHRMQUSTcB04AXKcv79ZwKPJ79Z/LFgnYO8BCwKnBkRHwo6SrKs5tTVAZ/E9i3sf+OmXV0zjo3MzMzs6bw0rmZmZmZNYULTTMzMzNrCheaZmbLGEldJN0k6TlJj7f1hrikFyTNzI3hJ1fa95f0tKTFkvpV2peXdE1+Z5akP1aOHZffeUrSDbkHp5l1cC40zcy+B3LT92/LMOCdiFgHuBAY2U7fnXNj+H6VtqcoG8A/XNN3f6BLRGwKbAUckUlJvYERQL+M6+xE2UjezDo4F5pmZnVIuk3SkzkjNzzbds9IyemS7s+27pJG5YzeDEn7ZfvCyrmGShqdn0dLukDSeGCkpP6SHpU0NX+vn/06STq/ct5jJO0q6dbKeXeTNKbBW9oHuCY/3wLsmm+MNyQiZkXEnNYOAd2yaO5KyX//bx7rTNngvjOwIvBqo+OZ2dLL2xuZmdV3WES8Lakr8ISk2ynxkQMiYp6klbPfqZQ4xk0BWknwac16lHjKT3OPzAER8YmkQcBfgP0oiUF9gC3y2MrAO8Alkn4cEW8CvwVG5bg30Xqu+QURcS3QG3gZIM/3LrAK8FZN/wDGSQrg8oi4os693EIpYl+jFJPHRcTbeU3nAy8BHwDjImJcA/8bM1vKudA0M6tvhKQh+XkNSuH3cETMA2gppoBBVJaEI+KdBs59c0ZLQolvvEbSupQib5CoQ54AAAV5SURBVPnKeS+LiE+q40n6K3CIpFHAdnweVXlAnTFbm71sba+7HSLi1dz8/V5JsyOidrm8qj9lr8/VgJWACZLuoxTF+1CK5QXAzZIOiYjr6lynmS3lXGiambVD0k6UQm+7iFgk6UFgOq3PGIrWC7ZqW+1LMNW4yj8D4yNiSL6g82Cd844C/gl8SClYP8lrrjejOZ9SMM/PpeyewNu1nSPi1fz9Ri7T9+fLz2VWHQTcHREfA29Imgj0y2uflzOv5BL/9oALTbMOzs9ompm1ryflxZlFkjYAtgW6AAMl9QGoLJ2PA45u+WJl6fx1SRtKWg4YQtt6Aq/k50Mr7eOAI1teGGoZLwvBV4FTKBnlZPsB+QJP7c+12WUs8Jv8PBR4IGrSOyR1y9hKMlJyMOUloPa8BOyiohvlfzU727dVid8UsCswq865zKwDcKFpZta+u4HOkmZQZhwnUWIWhwNjJE0Hbsq+ZwEr5RY+04Gds/1k4A7gAcrzi205Fzg7ZwI7VdqvohRrM/K8B1WOXQ+8HBHPfIV7uhpYRdJzwPF5fUhaTdK/ss+qwCM53r+BOyPi7uw3RNJ8ynL9nZLuye9cAnSnFKRPAKMiYkZEPE55fnMKJdZyOaDe855m1gE4gtLMbCkm6WJgakRc/V1fi5lZLReaZmZLKUlPUp7x3C0iPvqur8fMrJYLTTMzMzNrCj+jaWZmZmZN4ULTzOwbJKmfpIuW4HujJQ1txjW1MV4flZzzZ1Vyz1dop++akhZKOqHS1mp2uaSjVTLUQ9KPKv0PzlSjGZl6tHlz79DMvg9caJqZfYMiYnJEjPiur6MBI4ELI2Jdyobqw9rpeyFwV8sfdbLLJ1L2HX2x5hzzgIERsRnl7X2/dW62DHChaWbWoJzVG6mSe35fZpM/KOl5SXtnn50k3ZGfB0qalj9TK/tS/iFzy6dLOqeVcU6T9ETOFl7RkkMuaYSkZ3JW8Mb2xqhzHwJ2oWw5BCX3fN82+u4LPA88XXOo1ezyiJgaES/UniciHq0kJU0CVq93nWa29HMykJlZ47oBD0bESZmUcxawG7ARpVgbW9P/BOB3ETFRUnfgQ0l7UIq6bXIT+JX5sosj4kz4LGZyL0oC0MlAn4j4SFKvdsboAUxo4x4OAt4AFrQkCVGSgnrXdsxN10/Ke/xs2TwiXvma2eXDqMyQmlnH5RlNM7PG/Y+ygTuUjccfyrjFmcBarfSfCFwgaQTQKwu7QZSNzBfBF3LSq3bO5ydnUmYeN872GcD1kg4BWorEL40REe+1kQzUNzd2bzTr/AzK8vrCamMmHrVkl68GdMtrqkvSzpRC86RG+pvZ0s0zmmZmjfu4EtW4GPgIICIWt8RDVkXEOZLuBH4OTJI0iLZzywHIl2oupTz/+LKk0/k8H31PYACwN3CqpI3bGOMV2p/RnAX0ktQ5i9/VyaXvGtsAQyWdC/QCFkv6EHidJcgul7QZJeVoj4j4T3t9zaxjcKFpZtYkktaOiJnATEnbARtQcstPk/S3lqXzmlnNlqLyrVwKHwrcopKTvkZEjJf0CKVg7C5pldoxImI20LfOtY3Pc99IyT2/vbZPROxY6X86sDAiLpa0DZldTlk63xWYXGe8NYExwK8iYm57fc2s4/DSuZlZ8xxbyT3/ALgr88LHApMlTaPy7CNARCwArqQsx99GyQyH8mb3dbmcPpWypL2gtTEavLaTgOMz73wVSv45kvaWdGZ7X2wvuzxfWJpPmSWdIemq/NppOc6l+eJSu4WpmXUMTgYyMzMzs6bwjKaZmZmZNYULTTMzMzNrCheaZmZmZtYULjTNzMzMrClcaJqZmZlZU7jQNDMzM7OmcKFpZmZmZk3hQtPMzMzMmuL/4O7KuAKcqgIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print results\n",
    "\n",
    "print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "print('F1 Score: ' + str(f1_score(y_true, y_pred, average=None)))\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(conf_matrix, normalize=True, target_names=movements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
